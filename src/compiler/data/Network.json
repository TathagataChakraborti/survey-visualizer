{
    "nodes": [
        {
            "UID": 5,
            "slug": "paper_5",
            "title": "The effectiveness of real-time graphic simulation in telerobotics",
            "abstract": "An experimental telerobotic system has been developed which uses graphic simulation support. The operator may select from a variety of display methods including the overlay of the simulation aligned with live video images from the remote site. The robot may also be directly controlled from the simulation, using a six degree-of-freedom hand controller. An experiment has been carried out to determine the effect that having perspective control of simulation has upon the performance of teleoperation task. The experiment also compared the effectiveness of different display methods.",
            "link": "https://ieeexplore.ieee.org/abstract/document/169800",
            "authors": "Browse, R. A., & Little, S. A.",
            "venue": "SMC",
            "sessions": "",
            "year": 1991,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 6,
            "slug": "paper_6",
            "title": "Robotic Assembly Operation based on Task-Level Teaching in Virtual Reality",
            "abstract": "The authors propose a robot teaching interface which uses virtual reality. The teaching method provides a user interface with which a novice operator can easily direct a robot. The operator performs the assembly task in a virtual workspace generated by a computer. The operator's movements are recognized as robot task-level operations by using a finite automation. The system interprets the recognized operations into manipulator-level commands using task-dependent interpretation rules and a world model. A robot executes the assembly task in the workplace by replicating the operator's movements in the virtual workspace.",
            "link": "https://www.computer.org/csdl/proceedings-article/robot/1992/00220204/12OmNvTTcbj",
            "authors": "Takahashi, T., & Ogata, H.",
            "venue": "ICRA",
            "sessions": "",
            "year": 1992,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 7,
            "slug": "paper_7",
            "title": "Applications of augmented reality for human-robot communication",
            "abstract": "The director/agent (D/A) metaphor of telerobotic interaction is discussed as a potential means for achieving human-robot synergy. In order for the human operator to communicate spatial information to the robot during D/A operations, the medium of augmented reality through overlaid virtual stereographics is proposed, leading to what is referred to as virtual control. An overview is given of the ARGOS (Augmented Reality through Graphic Overlays on Stereovideo) system. In particular, the uses of overlaid virtual pointers for enhancing absolute depth judgement tasks, virtual tape measures for real-world quantification, virtual tethers for perceptual enhancement in manual teleoperation, virtual landmarks for enhancing depth scaling, and virtual object overlays for on-object edge enhancement and display superposition are all presented and discussed.",
            "link": "https://ieeexplore.ieee.org/abstract/document/583833",
            "authors": "Milgram, P., Zhai, S., Drascic, D., & Grodski, J.",
            "venue": "IROS",
            "sessions": "",
            "year": 1993,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Objects",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Numerical Readings",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 8,
            "slug": "paper_8",
            "title": "An interactive virtual reality simulation system for robot control and operator training",
            "abstract": "Robotic systems are often very complex and difficult to operate, especially as multiple robots are integrated to accomplish difficult tasks. In addition, training the operators of these complex robotic systems is time-consuming and costly. In this paper a virtual reality based robotic control system is presented. The virtual reality system provides a means by which operators can operate, and be trained to operate, complex robotic systems in an intuitive, cost-effective way. Operator interaction with the robotic system is at a high, task-oriented, level. Continuous state monitoring prevents illegal robot actions and provides interactive feedback to the operator and real-time training for novice users.",
            "link": "https://ieeexplore.ieee.org/abstract/document/351289?casa_token=3HGGWT9MkzMAAAAA:PdiS4guZ4XYwFXXehwwmrPhnlrPsxNUJy6zxf3okau8h7XMuC2TLPzoX3Bgb693ENwC1rvoj",
            "authors": "Miner, N. E., & Stansfield, S. A.",
            "venue": "ICRA",
            "sessions": "",
            "year": 1994,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 9,
            "slug": "paper_9",
            "title": "Antarctic Undersea Exploration Using a Robotic Submarine with a Telepresence User Interface (lepresence User)",
            "abstract": "This field experiment used a telepresence-controlled, remotely operated underwater vehicle to study sea-floor ecology in Antarctica. In using environmental data to create the virtual reality model in near real-time, this experiment represents the first combined use of telepresence and virtual reality for scientific purposes.",
            "link": "https://ieeexplore.ieee.org/abstract/document/483008?casa_token=ZVRrT9Ugsz4AAAAA:yiEse7Xk-yO-9NrLly2uynr_eSeTR03iNNbpZlyU1042p65dBK7vuQAQ6yv-bRBZl4K8nkWo",
            "authors": "Stoker, C. R., Burch, D. R., Hine, B. P., & Barry, J.",
            "venue": "IEEE Expert ",
            "sessions": "",
            "year": 1995,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 10,
            "slug": "paper_10",
            "title": "Vevi: A virtual environment teleoperations interface for planetary exploration",
            "abstract": "Remotely operating complex robotic mechanisms in unstructured natural environments is difficult at best. When the communications time delay is large, as for a Mars exploration rover operated from Earth, the difficulties become enormous. Conventional approaches, such as rate control of the rover actuators, are too inefficient and risky. The Intelligent Mechanisms Laboratory at the NASA Ames Research Center has developed over the past four years an architecture for operating science exploration robots in the presence of large communications time delays. The operator interface of this system is called the Virtual Environment Vehicle Interface (VEVI), and draws heavily on Virtual Environment (or Virtual Reality) technology. This paper describes the current operational version of VEVI, which we refer to as version 2.0. In this paper we will describe the VEVI design philosophy and implementation, and will describe some past examples of its use in field science exploration missions.",
            "link": "https://www.jstor.org/stable/44611966?casa_token=WlS0uiTb84AAAAAA:fjJbgBpIDX-IIMXE0Y5Dwy_CXqUFZJ-RGBJMh2JaoQ_bqLBOosrh4MNXnnlET0b3YyRU4IajQGrca2oNcRQJp2WOqF5V0vnpDNmDkr6IIx61Ve6mpas",
            "authors": "Hine, B., Hontalas, P., Fong, T., Piguet, L., Nygren, E., & Kline, A.",
            "venue": "SAE",
            "sessions": "",
            "year": 1995,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 11,
            "slug": "paper_11",
            "title": "Telerobotic Control with Stereoscopic Augmented Reality",
            "abstract": "Teleoperation in unstructured environments is conventionally restricted to direct manual control of the robot. Under such circumstances operator performance can be affected by inadequate visual feedback from the remote site, caused by, for example, limitations in the bandwidth of the communication channel. This paper introduces ARTEMIS (Augmented Reality TEleManipulation Interface System), a new display interface for enabling local teleoperation task simulation. An important feature of the interface is that the display can be generated in the absence of a model of the remote operating site. The display consists of a stereographical model of the robot overlaid on real stereovideo images from the remote site. This stereographical robot is used to simulate manipulation with respect to objects visible in the stereovideo image, following which sequences of robot control instructions can be transmitted to the remote site. In the present system, the update rate of video images can be very low, since continuous feedback is no longer needed for direct manual control of the robot. Several features of the system are presented and its advantages discussed, together with an illustrative example of a pick-and-place task.",
            "link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/2653/0000/Telerobotic-control-with-stereoscopic-augmented-reality/10.1117/12.237424.short",
            "authors": "Rastogi, A., Milgram, P., Drascic, D., & Grodski, J. J. ",
            "venue": "SPIE",
            "sessions": "",
            "year": 1996,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 12,
            "slug": "paper_12",
            "title": "Steering a simulated unmanned aerial vehicle using a head-slaved camera and hmd",
            "abstract": "Military use of unmanned aerial vehicles (UAVs) is gaining importance. Video cameras in these devices are often operated with joysticks and their image is displayed on a CRT. In this experiment, the simulated camera of a simulated UAV was slaved to the operator's head movements and displayed using a helmet mounted display (HMD). The task involved maneuvering a UAV along a winding course marked by tress. The influence of several parameters of the set-up on a set of flight handling characteristics was assessed. To enable variation of FOV and to study the effect of the HMD optics, a simulated HMD consisting of a head slaved window, was projected on a screen. One of the FOVs, generated in this way, corresponded with the FOV of the real HMD, enabling a comparison. The results show that the simulated HMD yields a significantly better performance that the real HMD. Performance with a FOV of 17 degrees is significantly lower than with 34 or 57 degrees. An image lag of 50 ms, typical of pan-and-tilt servo motor systems, has a small but significant influence on steering accuracy. Monocular and stereoscopic presentation did not result in significant performance differences.",
            "link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/3058/0000/Steering-a-simulated-unmanned-aerial-vehicle-using-a-head-slaved/10.1117/12.276655.short?SSO=1",
            "authors": "de Vries, S. C., & Padmos, P.",
            "venue": "Aerosense",
            "sessions": "",
            "year": 1997,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 13,
            "slug": "paper_13",
            "title": "A human-robot interface using an interactive hand pointer that projects a mark in the real work space",
            "abstract": "A human-robot interface system is under development that takes into account the flexibility of the DigitalDesk approach. The prototype consists of a projector subsystem for information display and a real-time tracking vision subsystem to recognize the human's action. Two levels of interaction using a virtual operational panel and interactive image panel have been developed. This paper presents the third subsystem, the interactive hand pointer used for selecting objects or positions in the environment via the operator's hand gestures. The system visually tracks the operator's pointing hand and projects a mark at the indicated position using an LCD projector. Since the mark can be observed directly in the real work space without monitor displays or HMDs, correction of the indicated position by moving the hand is very easy for the operator. The system enables projection of a mark not only at a target plane with a known height but also to a plane with an unknown height. Experimental results of a pick-and-place task demonstrate the usefulness of the proposed system.",
            "link": "https://ieeexplore.ieee.org/abstract/document/844117?casa_token=D0QGN4h2WhwAAAAA:MTZhDzkr3AEMXyAit-nTZMt8tfOiDPw4Q-nX-EW22n4pMg3fEQZRyrLkihwzI1EnO7KVO4Fa",
            "authors": "Sato, S., & Sakane, S.",
            "venue": "ICRA",
            "sessions": "",
            "year": 2000,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 14,
            "slug": "paper_14",
            "title": "A predictive interface based on virtual and augmented reality for task specification in a Web telerobotic system",
            "abstract": "A very interesting robotics area is the high-level tasks' specification, and the way a user-robot interface can facilitate programming complex actions to be executed on real environments. This paper shows a novel contribution in the user-robot interaction domain, and particularly in the web robotics field. The user, who is in the control loop, is able to specify the robot actions over a simulated scenario (running off-line), assisted with 3D virtual and augmented reality. Then, once the task is defined, it can be easily executed over the real scenario (running online). To get its main goal (i.e. task specification) working in real life scenarios, the system incorporates automatic object recognition and visually-guided grasping among others capabilities.",
            "link": "https://ieeexplore.ieee.org/abstract/document/1041729",
            "authors": "Mar\u00edn, R., Sanz, P. J., & del Pobil, A. P.",
            "venue": "IRDS",
            "sessions": "",
            "year": 2002,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 15,
            "slug": "paper_15",
            "title": "Real time visualization of robot state with mobile virtual reality",
            "abstract": "With the deployment of large, distributed networks of cameras and other sensors, it is becoming necessary to also address the issue of how to effectively present the large volume of gathered information to a user. The paper presents a virtual/augmented reality architecture that has been explicitly designed for use with a fully-portable, wearable computing system. A critical component of this system is a network-based mechanism for the representation of virtual objects and the live communication of changes in their state to users located elsewhere on the network. By presenting virtual objects in a uniform manner over the network, it becomes easy to construct new dynamic, virtual environments that reflect the state of robots or humans within the real environment. We demonstrate the utility of the architecture through several robot and human tracking examples.",
            "link": "https://ieeexplore.ieee.org/abstract/document/1013368?casa_token=9C_7-Ox6buwAAAAA:dsFjygsmSOO7eZ4uHZL5WbEYY2J_TRygByDB2YA0DJzuNXe4Abq7SjVYsLjcxmvAC-wL3URx",
            "authors": "Amstutz, P., & Fagg, A. H.",
            "venue": "ICRA",
            "sessions": "",
            "year": 2002,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Appearances",
                    "parent": "Entity"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 16,
            "slug": "paper_16",
            "title": "Cooperative Robot Teleoperation Through Virtual Reality Interfaces",
            "abstract": "Robots are employed to do exacting routines, ranging from the common place to the difficult and from the relatively safe to the highly dangerous. Remote-controlled robots-or teleoperation-is one way to combine the intelligence and maneuverability of human beings with the precision and durability of robots. Teleoperation can be difficult, due to the complexity both of the system and of information management-and more difficult still in a cooperative environment in which multiple teams are working together in different locations. To facilitate teleoperation, information visualization and a clear communication reference must be deployed in conjunction with an enhanced human machine interface (HMI) among all participating teams. The aim of this paper is to present a set of guidelines defining an ideal user interface utilizing virtual reality desktop for collaborative robot teleoperation in unknown environments. Enhancements in information visualization are discussed, and the case of an underwater robot is presented, because of the special challenges they present: a slow response system and six degrees of movement.",
            "link": "https://ieeexplore.ieee.org/abstract/document/1028783?casa_token=THnCaNVtupMAAAAA:0OrkvGG56jbcUhX5shcMP-Sf2J_YLegMf3EvcizVpwT8ehBW46CpVbJi9RsAgYFr6qChDH8N",
            "authors": "Monferrer, A., & Bonyuet, D.",
            "venue": "IV",
            "sessions": "",
            "year": 2002,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 17,
            "slug": "paper_17",
            "title": "World embedded interfaces for human-robot interaction",
            "abstract": "Human interaction with large numbers of robots or distributed sensors presents a number of difficult challenges including supervisory management, monitoring of individual and collective state, and apprehending situation awareness. A rich source of information about the environment can be provided even with robots that have no explicit representations or maps of their locale. To do this, we transform a robot swarm into a distributed interface embedded within the environment. Visually, each robot acts like a pixel within a much larger visual display space so that any robot need only communicate a small amount of information from its current location. Our approach uses augmented reality techniques for communicating information to humans from large numbers of small-scale robots to enable situation awareness, monitoring, and control for surveillance, reconnaissance, hazard detection, and path finding.",
            "link": "https://ieeexplore.ieee.org/abstract/document/1174285?casa_token=2JIp9DE3bNsAAAAA:dNIfO90dqf93flQxcN1l02Hvjd5GOFQNcv-Rs37wqNGMRk7J1RisNUb83LjFkcqcdL_VgSB1",
            "authors": "Daily, M., Cho, Y., Martin, K., & Payton, D.",
            "venue": "HICSS",
            "sessions": "",
            "year": 2003,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 18,
            "slug": "paper_18",
            "title": "Using augmented reality to interact with an autonomous mobile platform",
            "abstract": "To allow users without special knowledge to interact with robots, it is desirable to make interaction methods as intuitive as possible. This goal is in many cases difficult to achieve since data flow from the robot to the human is limited, especially if free locomotion of both the human and the robot are required. Therefore, new communication channels need to be created. We propose the use of an augmented reality display together with a wearable, wirelessly networked computer to achieve this goal. This system makes it possible to overlay planning, world model and sensory data provided by the robot over the wearer's field of view. We discuss the system architecture, interaction methods and experimental results. We demonstrate an example application for rapid prototyping of a warehouse transport system using the augmented reality system and a mobile platform. The user can create a topological map in an unknown environment on-the fly by setting and manipulating map nodes. This is done by pointing at the floor with a special interaction device, and issuing voice commands. The map is shown to the user as an augmentation of the real world view. Additionally, the robot's path planning data is visualized.",
            "link": "https://ieeexplore.ieee.org/abstract/document/1307282?casa_token=NDYOjQD2D7oAAAAA:ZMT8kpnpjxOmwxE15Pha25Ld8Fx96KstPddbRO6CgXwxrVTLhulaEbPQl6iT54V1jZxo0jx_",
            "authors": "Giesler, B., Salb, T., Steinhaus, P., & Dillmann, R.",
            "venue": "ICRA",
            "sessions": "",
            "year": 2004,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 19,
            "slug": "paper_19",
            "title": "Perspectives on augmented reality based human-robot interaction with industrial robots",
            "abstract": "First steps towards reliable augmented reality based human-robot interaction have been explored by the industrial robot manufacturer KUKA within the German cooperative research project MORPHA. Various aspects of augmented reality were analyzed and evaluated with respect to industrial requirements: interaction devices, tracking methods, accuracy, cost etc. As a result of this study, training of, and qualification for, robot operation and programming was selected as the most promising area for AR-based human-robot interaction with state-of-the-art AR techniques and devices. Therefore, research work concentrated on visualizing workflows that help inexperienced users to cope with rather complex robot operation and programming tasks. Several AR-based human-robot interaction prototypes were developed and presented to KUKA College students. Implementation details and results of initial experiments and a user survey are presented.",
            "link": "https://ieeexplore.ieee.org/abstract/document/1389914",
            "authors": "Bischoff, R., & Kazi, A.",
            "venue": "IROS",
            "sessions": "",
            "year": 2004,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 20,
            "slug": "paper_20",
            "title": "Evaluation of Human Sense of Security for Coexisting Robots using Virtual Reality ",
            "abstract": "When robots coexisting with humans are designed, it is important to evaluate the influence of the shape and size of the robots and their motions on human sense of security. For this purpose, an evaluation system of human sense of security for coexisting robots using virtual reality is discussed. Virtual robots are visually presented to a human subject through a head mounted display; the subject and the robots coexist in the virtual world. Some kinds of physiological indices of the subject are measured, he answers the questionnaire about his impression of the robots, and his sense of security is evaluated. Because of using virtual reality technique, the shape and size of the robots and their motions can be easily changed and tested. In the present report, pick and place motion of humanoid robots is evaluated. As a result of analyzing the questionnaire, it is found that turning the body coordinating with the arm gives good impression on humans.",
            "link": "https://ieeexplore.ieee.org/abstract/document/1307480?casa_token=lz0Iu2GVZ9kAAAAA:JcvEe3Fc30m7VfX3MhLiCQK8w6XRiUddOV2IKkCgOcVEKAhkXIHwzadKtXXpEDggofG5Lyjz",
            "authors": "Nonaka, S., Inoue, K., Arai, T., & Mae, Y.",
            "venue": "ICRA",
            "sessions": "",
            "year": 2004,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 21,
            "slug": "paper_21",
            "title": "Augmented reality for robot development and experimentation",
            "abstract": "The successful development of autonomous robotic systems requires careful fusion of complex subsystems for perception, planning, and control. Often these subsystems are designed in a modular fashion and tested individually. However, when ultimately combined with other components to form a complete system, unexpected interactions between subsystems can occur that make it difficult to isolate the source of problems. This paper presents a novel paradigm for robot experimentation that enables unified testing of individual subsystems while acting as part of a complete whole made up of both virtual and real components. We exploit the recent advances in speed and accuracy of optical motion capture to localize the robot, track environment objects, and extract extrinsic parameters for moving cameras in real-time. We construct a world model representation that serves as ground truth for both visual and tactile sensors in the environment. From this data, we build spatial and temporal correspondences between virtual elements, such as motion plans, and real artifacts in the scene. The system enables safe, decoupled testing of component algorithms for vision, motion planning and control that would normally have to be tested simultaneously on actual hardware. We show results of successful online applications in the development of an autonomous humanoid robot.",
            "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.9737&rep=rep1&type=pdf",
            "authors": "Stilman, M., Michel, P., Chestnutt, J., Nishiwaki, K., Kagami, S., & Kuffner, J.",
            "venue": "CMU TR",
            "sessions": "",
            "year": 2005,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Objects",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 22,
            "slug": "paper_22",
            "title": "Comparison of human psychology for real and virtual mobile manipulators",
            "abstract": "When robots coexisting with humans are designed, it is important to evaluate psychological influence of shape, size and motion of the robots on the humans. For this purpose, an evaluation system of human psychology for coexisting robots using virtual reality is discussed. Virtual (CG) robots are visually presented to a human subject using CAVE system. The subject answers questionnaire about his impression on the robots and their motions, and his psychological state is evaluated. In the present paper, whether humans have similar impressions and feelings for real and virtual robots is investigated. Real and virtual mobile manipulators reach for subjects and pass nearby them. In both situations, the same motion patterns by these robots are presented to the subjects and psychologically evaluated by questionnaire. As the comparison results of this experiment, the subjects had similar feelings for the real and virtual robots.",
            "link": "https://ieeexplore.ieee.org/abstract/document/1513759?casa_token=qNG9e8JK33gAAAAA:bj2yUf2Q7GKbd4UUHiVLsL43lTwySkBtZrRIVeGfuSx3_7aPtxYA3Cmp1zru5GjokR712Ncc",
            "authors": "Inoue, K., Nonaka, S., Ujiie, Y., Takubo, T., & Arai, T.",
            "venue": "ROMAN",
            "sessions": "",
            "year": 2005,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 23,
            "slug": "paper_23",
            "title": "Mixing Robotic Realities",
            "abstract": "This paper contests that Mixed Reality (MR) offers a potential solution in achieving transferability between Human Computer Interaction (HCI) and Human Robot Interaction (HRI). Virtual characters (possibly of a robotic genre) can offer highly expressive interfaces that are as convincing as a human, are comparably cheap and can be easily adapted and personalized. We introduce the notion of a mixed reality agent, i.e. an agent consisting of a physical robotic body and a virtual avatar displayed upon it. We realized an augmented reality interface with a Head-Mounted Display (HMD) in order to interact with such systems and conducted a pilot study to demonstrate the usefulness of mixed reality agents in human-robot collaborative tasks.",
            "link": "https://dl.acm.org/doi/abs/10.1145/1111449.1111504?casa_token=rNaAcKBv8YkAAAAA:RRRjbNm_rJkIozyh1zCQPSt0ewnx5XnWCmENs-F14mFoUFeCc9aiy3qjd8SsUi-QNcgmhDTI9iF_",
            "authors": "Dragone, M., Holz, T., & O'Hare, G. M.",
            "venue": "IUI",
            "sessions": "",
            "year": 2006,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Agents",
                    "parent": "Environmental"
                },
                {
                    "name": "Virtual Alterations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Morphological",
                    "parent": "Virtual Alterations"
                },
                {
                    "name": "Body Extensions",
                    "parent": "Morphological"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 24,
            "slug": "paper_24",
            "title": "U-Tsu-Shi-O-Mi: the Virtual Humanoid You Can Reach",
            "abstract": "We propose a tele-existence framework that enable us to feel humanexistence with a humanoid robot and computer graphic avatars. The purpose is to give computer graphic avatars more presence and make them physically interactive. The system consists of a synchronized pair of a whole humanoid robot and a 3D model computer graphic avatar, and an HMD that overlays the avatar onto the robot. As far as we know, there is no such framework. It requires that the robot can contact with human in safe. In order to make it satisfied, we developed a robot which arms are lightweighted and has force detecting sensors on the arms.",
            "link": "https://dl.acm.org/doi/pdf/10.1145/1179133.1179168",
            "authors": "Shoji, M., Miura, K., & Konno, A.",
            "venue": "SIGGRAPH",
            "sessions": "",
            "year": 2006,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Alterations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Superficial",
                    "parent": "Virtual Alterations"
                },
                {
                    "name": "Cosmetic Alterations",
                    "parent": "Superficial"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 25,
            "slug": "paper_25",
            "title": "Augmented reality visualisation for player",
            "abstract": "One of the greatest challenges when debugging a robot application is understanding what is going wrong. Robots are embodied in a complex, changing and unpredictable real world, using sensors and actuators that are different from humans'. As a result humans may find the development of robotic software to be difficult and time consuming. We present an augmented reality visualisation tool for the popular open source Player system, that enhances the developers understanding of the robots world view and thus improves the robot development process.",
            "link": "https://ieeexplore.ieee.org/abstract/document/1642308?casa_token=5VWCNtZ79tsAAAAA:_KgDOKHZv89_WiLUPeG_6qXHhcHBw4jgvZapT3Ci5dhqQliVuNHvr_ISkz_FsM8UaujaGjM_",
            "authors": "Collett, T. H., & MacDonald, B. A.",
            "venue": "ICRA",
            "sessions": "",
            "year": 2006,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 26,
            "slug": "paper_26",
            "title": "Interactive  laser-projection  for  programming  industrial robots",
            "abstract": "A method for intuitive and efficient programming of industrial robots based on Augmented Reality (AR) is presented, in which tool trajectories and target coordinates are interactively visualized and manipulated in the robot's environment by means of laser projection. The virtual information relevant for programming, such as trajectories and target coordinates, is projected into the robot's environment and can be manipulated interactively. For an intuitive and efficient user input to the system, spatial interaction techniques have been developed, which enable the user to virtually draw the desired motion paths for processing a work piece surface, directly onto the respective object. The discussed method has been implemented in an integrated AR-user interface and has been initially evaluated in an experimental programming scenario. The obtained results indicate that it enables significantly faster and easier programming of processing tasks compared to currently available shop-floor programming methods.",
            "link": "https://ieeexplore.ieee.org/abstract/document/4079265?casa_token=xpVjd7BRrFgAAAAA:LKhLdYbEc8uz1adMwPmY22e3eWfmOPGg6dBEVigYLZbxLCMXLgXPFj_Jt86Z_-Fhj6FfKOcX",
            "authors": "Zaeh, M. F., & Vogl, W.",
            "venue": "ISMAR",
            "sessions": "",
            "year": 2006,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Alteration Previews",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 27,
            "slug": "paper_27",
            "title": "Predictive display and interaction of telerobots based on augmented reality",
            "abstract": "A predictive display method and man-virtual robot interaction based on augmented reality are applied to control a telerobot. We first discuss the process of the augmented reality environment development. Then, we present the advantages of predictive display. Simulation of virtual robot's tasks in the augmented environment improves the safety of the telerobot when it executes the planned tasks. In addition, the immediate feedback from the virtual robot avoids the exacerbation of maneuverability caused by time-delay. For a more natural operation process, we apply multi man-virtual robot interactive methods. Lastly, the experiment of pick & place is conducted to validate the system.",
            "link": "https://www.cambridge.org/core/journals/robotica/article/abs/predictive-display-and-interaction-of-telerobots-based-on-augmented-reality/C6D66B1826B76AA2959F8B6C56028B31",
            "authors": "Xiong, Y., Li, S., & Xie, M.",
            "venue": "Robotica",
            "sessions": "",
            "year": 2006,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 28,
            "slug": "paper_28",
            "title": "Design of the Mending Robot Based on Virtual Reality and Intelligent Decision System",
            "abstract": "A semi-autonomous intelligent control system with layered structure for the telerobot is established and described in detail. It is applied in the virtual environment with real scene reappearance and teleoperation functions to plan and control locomotion and trajectory of the virtual robot instead of controlling the real robot directly. The virtual robot autonomously sends every planned motion instruction to the real robot via WLAN (wireless LAN) in order that these two robots can move synchronously. Because modeling error of the virtual environment and slide phenomena of wheels exist inevitably, the real robot always can not arrived at the desired destination precisely. Here, operators should adjust position and pose of the real robot with remote video and landmarks abidance to eliminate these errors. The virtual environment is made in OpenGL and both the virtual robot animation and the environment simulation are running smoothly. Experiments prove that the intelligent decision platform is reliable and efficient. With intervention of the operator, the mending task can accomplished successfully.",
            "link": "https://ieeexplore.ieee.org/abstract/document/4028163?casa_token=pGc43POncv8AAAAA:BpcNYTcimvpd02slHNTYgmLyvtBvwf4Bi8iOgRfNAGtqXRwrdNi5EuKih9ld6wxOwYRvw3Cc",
            "authors": "Cui, F., Zhang, M. L., & Liu, B. Q.",
            "venue": "ICMLC",
            "sessions": "",
            "year": 2006,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Labels",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 29,
            "slug": "paper_29",
            "title": "Overlay what humanoid robot perceives and thinks to the real-world by mixed reality system",
            "abstract": "One of the problems in developing a humanoid robot is caused by the fact that intermediate results, such as what the robot perceives the environment, and how it plans its moving path are hard to be observed online in the physical environment. What developers can see is only the behavior. Therefore, they usually investigate logged data afterwards, to analyze how well each component worked, or which component was wrong in the total system. In this paper, we present a novel environment for robot development, in which intermediate results of the system are overlaid on physical space using mixed reality technology. Real-time observation enables the developers to see intuitively, in what situation the specific intermediate results are generated, and to understand how results of a component affected the total system. This feature makes the development efficient and precise. This environment also gives a human-robot interface that shows the robot internal state intuitively, not only in development, but also in operation.",
            "link": "https://ieeexplore.ieee.org/abstract/document/4538864?casa_token=UIhV2B1olpsAAAAA:1LF4yfN-FuiJVxcs_3aRyvYE_LVDubcZgZLxwpMyCIQWqpo9rQWTtUtr92WTGXbR7wsy-ihg",
            "authors": "Kobayashi, K., Nishiwaki, K., Uchiyama, S., Yamamoto, H., Kagami, S., & Kanade, T.",
            "venue": "ISMAR",
            "sessions": "",
            "year": 2007,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 30,
            "slug": "paper_30",
            "title": "Using Mixed Reality Agents as Social Interfaces for Robots",
            "abstract": "Endowing robots with a social interface is often costly and difficult. Virtual characters on the other hand are comparatively cheap and well equipped but suffer from other difficulties, most notably their inability to interact with the physical world. This paper details our wearable solution to combining physical robots and virtual characters into a mixed reality agent (MiRA) through mixed reality visualisation. It describes a pilot study demonstrating our system, and showing how such a technique can offer a viable alternative cost effective approach to enabling a rich social interface for human-robot interaction.",
            "link": "https://ieeexplore.ieee.org/abstract/document/4415255?casa_token=LwaL1y8YqzoAAAAA:-5LvD5yaFED-QK3x3mOJq9ROBnrIE54h0_q-1Cx_rHJTwz7oD34rB-R1ibEKYPbkhbxep24V",
            "authors": "Dragone, M., Holz, T., & O'Hare, G. M.",
            "venue": "ROMAN",
            "sessions": "",
            "year": 2007,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Agents",
                    "parent": "Environmental"
                },
                {
                    "name": "Virtual Alterations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Morphological",
                    "parent": "Virtual Alterations"
                },
                {
                    "name": "Body Extensions",
                    "parent": "Morphological"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 31,
            "slug": "paper_31",
            "title": "Augmented reality user interface for reconnaissance robotic missions",
            "abstract": "The problem of visual telepresence and augmented reality control of reconnaissance mobile robots is described. ARGOS (advanced robotic graphical operation system) for teleoperation of various mobile robots through sensory supported visual telepresence is presented. Two robots - Orpheus and Hermes, and one embedded system Orpheus EB - made on Department of Control and Instrumentation (DCI) are described as examples of systems with different features and capabilities that may be controlled through ARGOS. Data fusion of CCD color camera data, thermovision data and 3D proximity data through extended 3D evidence grids is described.",
            "link": "https://ieeexplore.ieee.org/abstract/document/4415224?casa_token=FeGLT7Ffa1MAAAAA:I-plpeInZgTFuNaGubwal07VbVU6QGr8R3o6fCv89P0-1oG9X4et9fKP-iIf-xWNTmN8GNbr",
            "authors": "Zalud, L.",
            "venue": "ROMAN",
            "sessions": "",
            "year": 2007,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 32,
            "slug": "paper_32",
            "title": "Joystick mapped augmented reality cues for end-effector controlled tele-operated robots",
            "abstract": "End-effector control of robots using just remote camera views is difficult due to lack of perceived correspondence between the joysticks and the end-effector coordinate frame. This paper reports the positive effects of augmented reality visual cues on operator performance during end-effector controlled tele-operation using only camera views. Our solution is to overlay a color-coded coordinate system on the end-effector of the robot using AR techniques. This mapped and color-coded coordinate system is then directly mapped to similarly color-coded joysticks used for control of both position and orientation. The AR view along with mapped markings on the joystick give the user a clear notion of the effect of their joystick movements on the end-effector of the robot. All camera views display this registered dynamic overlay information on-demand. An insertion task was used to compare performance with and without the coordinate mapping using fifteen subjects. Preliminary results indicate a significant reduction in distance and reversal errors.",
            "link": "https://ieeexplore.ieee.org/abstract/document/4161038?casa_token=JqbhAyC2PZcAAAAA:98TxTnCvWpSIfq_vD82Rb90ULvhoCBvoyNPEN9ntNnyQwvtuPth8GnH4ffDsTo5ODS7ofOhQ",
            "authors": "Nawab, A., Chintamani, K., Ellis, D., Auner, G., & Pandya, A.",
            "venue": "VR",
            "sessions": "",
            "year": 2007,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 33,
            "slug": "paper_33",
            "title": "Collaborating with a Mobile Robot: An Augmented Reality Multimodal Interface",
            "abstract": "We have created an infrastructure that allows a human to collaborate in a natural manner with a robotic system. In this paper we describe our system and its implementation with a mobile robot. In our prototype the human communicates with the mobile robot using natural speech and gestures, for example, by selecting a point in 3D space and saying \u201cgo here\u201d or \u201cgo behind that\u201d. The robot responds using speech so the human is able to understand its intentions and beliefs. Augmented Reality (AR) technology is used to facilitate natural use of gestures and provide a common 3D spatial reference for both the robot and human, thus providing a means for grounding of communication and maintaining spatial awareness. This paper first discusses related work then gives a brief overview of AR and its capabilities. The architectural design we have developed is outlined and then a case study is discussed.",
            "link": "https://www.sciencedirect.com/science/article/pii/S1474667016415028",
            "authors": "Green, S. A., Chen, X. Q., Billinghurst, M., & Chase, J. G.",
            "venue": "IFAC",
            "sessions": "",
            "year": 2008,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor Numerical Readings",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 34,
            "slug": "paper_34",
            "title": "Mixed reality environment for autonomous robot development",
            "abstract": "This video demonstrates a mixed reality (MR) environment which is constructed for development of autonomous behaviors of robots. Many kinds of functions are required to be integrated for realizing an autonomous behavior. For example, autonomous navigation of humanoid robots needs functions, such as, recognition of environment, localization and mapping, path planning, gait planning, dynamically stable biped walking pattern generation, and sensor feedback stabilization of walking. Technologies to realize each function are well investigated by many research works. However, another effort is required for constructing an autonomous behavior by integrating those functions. We demonstrate a MR environment in which internal status of a robot, such as, sensor status, recognition results, planning results, and motion control parameters, can be projected to the environment and its body. We can understand intuitively how each function works as a part of total system in the real environment by using the proposed system, and it helps solving the integration problems. The overview of the system, projection of each internal status, and the application to an autonomous locomotion experiment are presented in the video clip.",
            "link": "https://ieeexplore.ieee.org/abstract/document/4543538?casa_token=rE9jI_ig6mAAAAAA:XwlqCBB04KIM4uR_nWCoLYLefdiFgq7Y1CSwDq-cnvWPtrkeAWtht7iehZNVjNjhc3nb1WmU",
            "authors": "Nishiwaki, K., Kobayashi, K., Uchiyama, S., Yamamoto, H., & Kagami, S.",
            "venue": "ICRA",
            "sessions": "",
            "year": 2008,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 35,
            "slug": "paper_35",
            "title": "Evaluating the augmented reality human-robot collaboration system",
            "abstract": "This article discusses an experimental comparison of three user interface techniques for interaction with a remotely located robot. A typical interface for such a situation is to teleoperate the robot using a camera that displays the robot's view of its work environment. However, the operator often has a difficult time maintaining situation awareness due to this single egocentric view. Hence, a multimodal system was developed enabling the human operator to view the robot in its remote work environment through an augmented reality interface, the augmented reality human-robot collaboration (AR-HRC) system. The operator uses spoken dialogue, reaches into the 3D representation of the remote work environment and discusses intended actions of the robot. The result of the comparison was that the AR-HRC interface was found to be most effective, increasing accuracy by 30%, while reducing the number of close calls in operating the robot by factors of \u223c3x. It thus provides the means to maintain spatial awareness and give the users the feeling of working in a true collaborative environment.",
            "link": "https://www.inderscienceonline.com/doi/abs/10.1504/IJISTA.2010.030195",
            "authors": "Green, S. A., Chase, J. G., Chen, X., & Billinghurst, M.",
            "venue": "IJISTA",
            "sessions": "",
            "year": 2009,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor Numerical Readings",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 36,
            "slug": "paper_36",
            "title": "Designing laser gesture interface for robot control",
            "abstract": "A laser pointer can be a powerful tool for robot control. However, in the past, their use in the field of robotics has been limited to simple target designation, without exploring their potential as versatile input devices. This paper proposes to create a laser pointer-based user interface for giving various instructions to a robot by applying stroke gesture recognition to the laser\u2019s trajectory. Through this interface, the user can draw stroke gestures using a laser pointer to specify target objects and commands for the robot to execute accordingly. This system, which includes lasso and dwelling gestures for object selection, stroke gestures for robot operation, and push-button commands for movement cancellation, has been refined from its prototype form through several user-study evaluations. Our results suggest that laser pointers can be effective not only for target designation but also for specifying command and target location for a robot to perform.",
            "link": "https://link.springer.com/chapter/10.1007/978-3-642-03658-3_52",
            "authors": "Ishii, K., Zhao, S., Inami, M., Igarashi, T., & Imai, M.",
            "venue": "INTERACT",
            "sessions": "",
            "year": 2009,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "User-Defined Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 37,
            "slug": "paper_37",
            "title": "Mixed reality simulation for mobile robots",
            "abstract": "Mobile robots are increasingly entering the real and complex world of humans in ways that necessitate a high degree of interaction and cooperation between human and robot. Complex simulation models, expensive hardware setup, and a highly controlled environment are often required during various stages of robot development. There is a need for robot developers to have a more flexible approach for conducting experiments and to obtain a better understanding of how robots perceive the world. Mixed Reality (MR) presents a world where real and virtual elements co-exist. By merging the real and the virtual in the creation of an MR simulation environment, more insight into the robot behaviour can be gained, e.g. internal robot information can be visualised, and cheaper and safer testing scenarios can be created by making interactions between physical and virtual objects possible. Robot developers are free to introduce virtual objects in an MR simulation environment for evaluating their systems and obtain a coherent display of visual feedback and realistic simulation results. We illustrate our ideas using an MR simulation tool constructed based on the 3D robot simulator Gazebo.",
            "link": "https://ieeexplore.ieee.org/abstract/document/5152325?casa_token=9L9bXCDk60MAAAAA:sRc2xFbFJ4joDWQXblPeALx_0r1fLrxCqFxZ5rKpn5-uKq-RqiUgMWo2zUbNpjNQeZ0tQw3j",
            "authors": "Chen, I. Y. H., MacDonald, B., & Wunsche, B.",
            "venue": "ICRA",
            "sessions": "",
            "year": 2009,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Objects",
                    "parent": "Environmental"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 38,
            "slug": "paper_38",
            "title": "An Augmented Reality Debugging System for Mobile Robot Software Engineers.",
            "abstract": "Robotics presents a unique set of challenges, which change the way that we must approach the debugging of robotic software. Augmented reality (AR) provides many opportunities for enhancing debugging, allowing the developer to see the real world as the robot does, superimposed in situ on the real world view of the human, intuitively displaying the limitations and discontinuities in the robot's real world view. This paper contributes a systematic analysis of the challenges faced by robotic software engineers, and identifies the recurrent concepts for AR based visualisation of robotic data. This in turns leads to a conceptual design for an AR enhanced intelligent debugging space. Both an open source reference implementation of the conceptual design and an initial evaluation of the implementation's efficacy are described. The AR system provides an opportunity to understand the types of errors that are encountered during debugging. The AR system analysis and design provide a reusable conceptual framework for future designers of robotic debugging systems, and guidelines for designing visualisations. In concert with common, standard robotics interfaces provided by Player/Stage, the AR system design supplies a set of common visualisations, so that many data visualisations can be provided to developers with no additional effort.",
            "link": "https://aisberg.unibg.it/handle/10446/86156",
            "authors": "Collett, T. H. J., & Macdonald, B. A.",
            "venue": "JOSER",
            "sessions": "",
            "year": 2010,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 39,
            "slug": "paper_39",
            "title": "Design and Implementation of a GPS Guidance System for Agricultural Tractors Using Augmented Rreality Technology",
            "abstract": "Current commercial tractor guidance systems present to the driver information to perform agricultural tasks in the best way. This information generally includes a treated zones map referenced to the tractor\u2019s position. Unlike actual guidance systems where the tractor driver must mentally associate treated zone maps and the plot layout, this paper presents a guidance system that using Augmented Reality (AR) technology, allows the tractor driver to see the real plot though eye monitor glasses with the treated zones in a different color. The paper includes a description of the system hardware and software, a real test done with image captures seen by the tractor driver, and a discussion predicting that the historical evolution of guidance systems could involve the use of AR technology in the agricultural guidance and monitoring systems.",
            "link": "https://www.mdpi.com/1424-8220/10/11/10435",
            "authors": "Santana-Fern\u00e1ndez, J., G\u00f3mez-Gil, J., & del-Pozo-San-Cirilo, L.",
            "venue": "Sensors",
            "sessions": "",
            "year": 2010,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 40,
            "slug": "paper_40",
            "title": "Blinkbot: look at, blink and move",
            "abstract": "In this paper we present BlinkBot - a hands free input interface to control and command a robot. BlinkBot explores the natural modality of gaze and blink to direct a robot to move an object from a location to another. The paper also explains detailed hardware and software implementation of the prototype system.",
            "link": "https://dl.acm.org/doi/abs/10.1145/1866218.1866238?casa_token=jcYMTwl1FpcAAAAA:6cQOA8mOW9dtRYGgHxE19lYT83wZh2HhsGMr1a3wmZnZezZD-uBLoO9hmU5UwoorGPAdISztQ8Xo",
            "authors": "Mistry, P., Ishii, K., Inami, M., & Igarashi, T.",
            "venue": "UIST",
            "sessions": "",
            "year": 2010,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 41,
            "slug": "paper_41",
            "title": "Improving the authentic learning experience by integrating robots into the mixedreality environment",
            "abstract": "The main aim of the modern popular teaching method of authentic learning has been to provide students with everyday-life challenges that develop knowledge and skills through problem solving in different situations. Many emerging information technologies have been used to present authentic environment in pedagogical purpose. However, there are few studies that have been discussed the sense of authenticity and characters in scene and how students interact with the characters involved in the task. We designed a system, RoboStage, with authentic scenes by using mixed-reality technology and robot to investigate the difference in learning with either physical or virtual characters and learning behaviors and performance through the system. Robots were designed to play real interactive characters in the task. The experiment of the study conducted with 36 junior high students. The results indicated that RoboStage significantly improved the sense of authenticity of the task and also positively affected learning motivation. Learning performance was conditionally affected by RoboStage.",
            "link": "https://www.sciencedirect.com/science/article/pii/S0360131510001831?casa_token=Hr8TGKaj3vsAAAAA:Qj9BiUR95A2-yq6RtP8EW96jC2FCb9lXwuP8CBiDPWqSbr1EW_HfQefgiKVHPwnbrTNncXtI-A",
            "authors": "Chang, C. W., Lee, J. H., Wang, C. Y., & Chen, G. D.",
            "venue": "Computers & Education",
            "sessions": "",
            "year": 2010,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Objects",
                    "parent": "Environmental"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 42,
            "slug": "paper_42",
            "title": "Mixed reality educational environment for robotics",
            "abstract": "This work presents an operational setup for carrying out experiments in robotics using a mixed-reality approach. The objective of this setup is twofold, on one hand it aims to be a useful educational tool for teaching robotics and on the other it is a tool to help in the development and study of embodied evolution. The design represents an intermediate system between a real and a simulated scenario so as to work with real robots and to make easier and simpler the configuration and modification of the rest of the elements of the experiments. The present operational setup consists, mainly, of a video projector to represent the virtual elements projecting them over the arena of the experiment, a zenithal camera to capture the state of real elements moving on the arena, and a computer which monitors and controls the scenario. As a result, both real and virtual elements interact on the arena and their state is updated based on world rules of the experiment and on the state and actions of other elements. The scheme implemented in the main computer to control and structure this environment is based on the one proposed in a previous work for the definition of scenarios in a simulation environment named Waspbed, which was designed for the study of simulated coevolution processes in multiagent systems. Apart from the advantages provided in robotics research when large testing period times are required, this setup is used for getting students from engineering degrees used to deal with robotics and all its associated fields, such as artificial vision, evolutionary robotics, communication protocols, etc. in a very simple, quick and cheap way, which otherwise wouldn't be feasible taking into account the academic constraints of time and resources.",
            "link": "https://ieeexplore.ieee.org/abstract/document/6053845?casa_token=50DJNC0kt94AAAAA:LI7WjpwQPgKG3q3SwkAKaRXLHjQZycwD12kzhBzSA0TGjzB5nl8id-y-XmNMv16AvEvAALPa",
            "authors": "Garcia, A. P., Fernandez, G. V., Torres, B. M. P., & L\u00f3pez-Pe\u00f1a, F.",
            "venue": "VECIMS",
            "sessions": "",
            "year": 2011,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 43,
            "slug": "paper_43",
            "title": "Augmented Reality to Improve Teleoperation of Mobile Robots",
            "abstract": "In this article we want to discuss the skills required for human tele-operators of mobile robots. We show the classical problems related with teleoperation but focus our discussion in not specialized operators. We want to show how a proposal based on augmented reality interfaces can improve the performance of operators controlling the robot. We present the validation of our solution in two experiments: one in a controlled circuit and another one in a real environment. To carry out our experiments we use a low cost robotic surveillance platform.",
            "link": "https://buleria.unileon.es/handle/10612/2230",
            "authors": "Rodr\u00edguez Lera, F. J., Garc\u00eda Sierra, J. F., Fern\u00e1ndez Llamas, C., & Matell\u00e1n Olivera, V.",
            "venue": "WAF",
            "sessions": "",
            "year": 2011,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Objects",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 44,
            "slug": "paper_44",
            "title": "Touchme: An augmented reality based remote robot manipulation",
            "abstract": "A general remote controlled robot is manipulated by a joystick and a gamepad. However, these methods are difficult for inexperienced users because the mapping between the user input and resulting robot motion is not always intuitive (e.g. tilt a joystick to the right to rotate the robot to the left). To solve this problem, we propose a touch-based interface for remotely controlling a robot from a third-person view, which is called \u201cTouchMe\u201d. This system allows the user to manipulate each part of the robot by directly touching it on a view of the world as seen by a camera looking at the robot from a third-person view. Our system provides intuitive operation, and the user can use our system with minimal user training. In this paper we describe the TouchMe interaction and its prototype implementation. We also introduce three scheduling methods for controlling the robot in response to user interaction and report on the results of empirical comparisons of these methods.",
            "link": "https://star.rcast.u-tokyo.ac.jp/old/content/files/pdf/conference/workshop/13.%20TouchMe.pdf",
            "authors": "Hashimoto, S., Ishida, A., Inami, M., & Igarashi, T.",
            "venue": "ICAT",
            "sessions": "",
            "year": 2011,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "3D Controllers",
                    "parent": "Control Objects"
                },
                {
                    "name": "Virtual Alterations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Morphological",
                    "parent": "Virtual Alterations"
                },
                {
                    "name": "Body Extensions",
                    "parent": "Morphological"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 45,
            "slug": "paper_45",
            "title": "3-d integration of robot vision and laser data with semiautomatic calibration in augmented reality stereoscopic visual interface",
            "abstract": "This paper proposes an augmented reality visualization interface to simultaneously present visual and laser sensors information further enhanced by stereoscopic viewing and 3-D graphics. The use of graphic elements is proposed to represent laser measurements that are aligned to video information in 3-D space. This methodology enables an operator to intuitively comprehend scene layout and proximity information and so to respond in an accurate and timely manner. The use of graphic elements to assist teleoperation, sometime discussed in the literature, is here proposed following an innovative approach that aligns virtual and real objects in 3-D space and color them suitably to facilitate comprehension of objects proximity during navigation. This paper is developed based on authors' previous experience on stereoscopic teleoperation. The approach is experimented on a real telerobotic system, where a user operates a mobile robot located several kilometers apart. The result showed simplicity and effectiveness of the proposed approach.",
            "link": "https://ieeexplore.ieee.org/abstract/document/6062673?casa_token=mQIHKr7IZpUAAAAA:BsL0t3ZFUKePkknnmmQ7epKQvb9omjBnheQE2LgNo4DKfx9FLaOJG-1aa-R4evZ8AQbEu5HK",
            "authors": "Livatino, S., Banno, F., & Muscato, G.",
            "venue": "IEEE Transactions on Industrial Informatics ",
            "sessions": "",
            "year": 2011,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 46,
            "slug": "paper_46",
            "title": "Development of a Tractor Navigation System Using Augmented Reality",
            "abstract": "We developed an intuitive tractor navigation system using augmented reality (AR) by superimposing a computer-generated virtual three-dimensional (3D) image on a camera image. The 3D image was generated using the tractor position and direction determined by two real-time kinematic global positioning systems and an inertial measurement unit. The positioning accuracy of the AR navigation system was examined experimentally by changing the roll, pitch, and yaw angles of a tractor at rest on an actual field. The positioning errors in the world coordinate system were less than 3 cm within 3 m from the front of the tractor, and less than 3 pixels in the image coordinate system. The refresh frequency of the AR image was 30 Hz and the time taken from image capturing to displaying was within 10 ms.",
            "link": "https://www.sciencedirect.com/science/article/abs/pii/S1881836612800218",
            "authors": "Kaizu, Y., & Choi, J.",
            "venue": "EiAEF",
            "sessions": "",
            "year": 2012,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 47,
            "slug": "paper_47",
            "title": "A pilot study of the effectiveness of augmented reality to enhance the use of remote labs in electrical engineering education",
            "abstract": "Lab practices are an essential part of teaching in Engineering. However, traditional laboratory lessons developed in classroom labs (CL) must be adapted to teaching and learning strategies that go far beyond the common concept of e-learning, in the sense that completely virtualized distance education disconnects teachers and students from the real world, which can generate specific problems in laboratory classes. Current proposals of virtual labs (VL) and remote labs (RL) do not either cover new needs properly or contribute remarkable improvement to traditional labs\u2014except that they favor distance training. Therefore, online teaching and learning in lab practices demand a further step beyond current VL and RL. This paper poses a new reality and new teaching/learning concepts in the field of lab practices in engineering. The developed augmented reality-based lab system (augmented remote lab, ARL) enables teachers and students to work remotely (Internet/intranet) in current CL, including virtual elements which interact with real ones. An educational experience was conducted to assess the developed ARL with the participation of a group of 10 teachers and another group of 20 students. Both groups have completed lab practices of the contents in the subjects Digital Systems and Robotics and Industrial Automation, which belong to the second year of the new degree in Electronic Engineering (adapted to the European Space for Higher Education). The labs were carried out by means of three different possibilities: CL, VL and ARL. After completion, both groups were asked to fill in some questionnaires aimed at measuring the improvement contributed by ARL relative to CL and VL. Except in some specific questions, the opinion of teachers and students was rather similar and positive regarding the use and possibilities of ARL. Although the results are still preliminary and need further study, seems to conclude that ARL remarkably improves the possibilities of current VL and RL. Furthermore, ARL can be concluded to allow further possibilities when used online than traditional laboratory lessons completed in CL.",
            "link": "https://link.springer.com/article/10.1007/s10956-011-9345-9",
            "authors": "Mej\u00edas Borrero, A., & And\u00fajar M\u00e1rquez, J. M.",
            "venue": "Journal of Science Education and Technology",
            "sessions": "",
            "year": 2012,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Objects",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 48,
            "slug": "paper_48",
            "title": "A spatial augmented reality system for intuitive display of robotic data",
            "abstract": "In the emerging field of close human-robot-collaboration the human worker needs to be able to quickly and easily understand data of the robotic system. To achieve this even for untrained personnel, we propose the use of a Spatial Augmented Reality system to project the necessary information directly into the users' workspace. The projection system consists of a fixed as well as a mobile projector mounted directly on the manipulator, allowing for visualizing data anywhere in the surroundings of the robot. By enabling the user to simply see the necessary complex information he can better understand the data and behavior of the robotic assistant and has the opportunity to analyze and potentially optimize the working process. Together with an input device, arbitrary interfaces can be realized with the projection system.",
            "link": "https://ieeexplore.ieee.org/abstract/document/6483560?casa_token=SEJjxscrK6IAAAAA:w_K4farRpkzjVxHy-muMq-i-XFflLRrDSUDQnu_8BEJJ7EmEWFyBKxlOBz2CYZKS13juNb3d",
            "authors": "Leutert, F., Herrmann, C., & Schilling, K.",
            "venue": "HRI",
            "sessions": "",
            "year": 2013,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Alteration Previews",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 49,
            "slug": "paper_49",
            "title": "Using Augmented Reality to Improve Usability of the User Interface for Driving a Telepresence Robot",
            "abstract": "Mobile Robotic Telepresence (MRP) helps people to communicate in natural ways despite being physically located in different parts of the world. User interfaces of such systems are as critical as the design and functionality of the robot itself for creating conditions for natural interaction. This article presents an exploratory study analysing different robot teleoperation interfaces. The goals of this paper are to investigate the possible effect of using augmented reality as the means to drive a robot, to identify key factors of the user interface in order to improve the user experience through a driving interface, and to minimize interface familiarization time for non-experienced users. The study involved 23 participants whose robot driving attempts via different user interfaces were analysed. The results show that a user interface with an augmented reality interface resulted in better driving experience.",
            "link": "https://www.degruyter.com/document/doi/10.2478/pjbr-2013-0018/html",
            "authors": "Mosiello, G., Kiselev, A., & Loutfi, A.",
            "venue": "JoBR",
            "sessions": "",
            "year": 2013,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Command Options & Validity",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 50,
            "slug": "paper_50",
            "title": "Flying head: a head motion synchronization mechanism for unmanned aerial vehicle control",
            "abstract": "We propose an unmanned aerial vehicle (UAV) control mechanism, called a \"Flying Head\" which synchronizes a human head and the UAV motions. The accurate manipulation of UAVs is difficult as their control typically involves hand-operated devices. We can incorporate the UAV control using human motions such as walking, looking around and crouching. The system synchronizes the operator and UAV positions in terms of the horizontal and vertical positions and the yaw orientation. The operator can use the UAV more intuitively as such manipulations are more in accord with kinesthetic. Finally, we discuss flying telepresence applications.",
            "link": "https://dl.acm.org/doi/abs/10.1145/2468356.2468721?casa_token=B5gqGq7ZcRsAAAAA:P3iZoYot6enFwFaGYGICka8rXNK0spaCLNG-6JX3CTTsMbDrr64DUuSKxIB80UjWe5w9sL1KcX8W",
            "authors": "Higuchi, K., & Rekimoto, J.",
            "venue": "CHI Extended Abstracts",
            "sessions": "",
            "year": 2013,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 51,
            "slug": "paper_51",
            "title": "Intuitive robot tasks with augmented reality and virtual obstacles",
            "abstract": "Today's industrial robots require expert knowledge and are not profitable for small and medium sized enterprises with their small lot sizes. It is our strong belief that more intuitive robot programming in an augmented reality robot work cell can dramatically simplify re-programming and leverage robotics technology in short production cycles. In this paper, we present a novel augmented reality system for defining virtual obstacles, specifying tool positions, and specifying robot tasks. We evaluate the system in a user study and, more specifically, investigate the input of robot end-effector orientations in general.",
            "link": "https://ieeexplore.ieee.org/abstract/document/6907747?casa_token=-IbrBGgKEw0AAAAA:XAIzGo_fvssflcZiABx9REgeGF0gEEX1F68d3pxzXSeQOALAiQdMSaz9102gqHRJmtOG9D43",
            "authors": "Gaschler, A., Springer, M., Rickert, M., & Knoll, A.",
            "venue": "ICRA",
            "sessions": "",
            "year": 2014,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Command Options & Validity",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 52,
            "slug": "paper_52",
            "title": "Spatial augmented reality as a method for a mobile robot to communicate intended movement",
            "abstract": "Our work evaluates a mobile robot\u2019s ability to communicate intended movements to humans via projection of visual arrows and a simplified map. Humans utilize a variety of techniques to signal intended movement in a co-occupied space. We evaluated an augmented reality projection provided by the robot. The projection is on the floor and consists of arrows and a simplified map. Two pilots and one quasi-experiment were conducted to examine the effectiveness of visual projection of arrows by a robot for signaling intended movement. The pilot work demonstrates the effectiveness of utilizing arrows as a communication medium. The experiment examined the effectiveness of a simplified map and arrows for signaling the short-, mid-range, and long-term intended movement. Two pilot experiments confirm that arrows are an effective symbol for a robot to use to signal intent. A field experiment demonstrates that a robot can use a projected arrow and simplified map to signal its intended movement and people understand the projection for upcoming short-, medium-, and long-term movement. Augmented reality, such as projected arrows and simplified map, are an effective tool for robots to use when signaling their upcoming movement to humans. Telepresence robots in organizations, museum docents, information kiosks, hospital assistants, factories, and as members of search and rescue teams are typical applications where mobile robots reside and interact with people.",
            "link": "https://www.sciencedirect.com/science/article/pii/S0747563214000612?casa_token=FQErRFqJt94AAAAA:0E-iH_ot7dmsX1HthsGiz5A_B0Lrj_hF-WSWjm43OShzq1ESmzRPsIVlrVyRcaA2xArxDoV9Dw",
            "authors": "Coovert, M. D., Lee, T., Shindev, I., & Sun, Y.",
            "venue": "Comp Hum Bvr",
            "sessions": "",
            "year": 2014,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 53,
            "slug": "paper_53",
            "title": "Interactive augmented reality for understanding and analyzing multi-robot systems",
            "abstract": "Once a multi-robot system is implemented on real hardware and tested in the real world, analyzing its evolution and debugging unexpected behaviors is often a very difficult task. We present a tool for aiding this activity, by visualizing an Augmented Reality overlay on a live video feed acquired by a fixed camera overlooking the robot environment. Such overlay displays live information exposed by each robot, which may be textual (state messages), symbolic (graphs, charts), or, most importantly, spatially-situated; spatially-situated information is related to the environment surrounding the robot itself, such as for example the perceived position of neighboring robots, the perceived extent of obstacles, the path the robot plans to follow. We show that, by directly representing such information on the environment it refers to, our proposal removes a layer of indirection and significantly eases the process of understanding complex multi-robot systems. We describe how the system is implemented, discuss application examples in different scenarios, and provide supplementary material including demonstration videos and a functional implementation.",
            "link": "https://ieeexplore.ieee.org/abstract/document/6942709?casa_token=XqLiYXQhL-UAAAAA:Flqn625E85h4BJy7DfMaQGzlq2IMLY4acdteDGcozpr4LI9PBNiQUx7AV1E0Ezeyvq1PfMNS",
            "authors": "Ghiringhelli, F., Guzzi, J., Di Caro, G. A., Caglioti, V., Gambardella, L. M., & Giusti, A.",
            "venue": "IROS",
            "sessions": "",
            "year": 2014,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Labels",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Attributes",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 54,
            "slug": "paper_54",
            "title": "FlyAR: Augmented Reality Supported Micro Aerial Vehicle Navigation",
            "abstract": "Micro aerial vehicles equipped with high-resolution cameras can be used to create aerial reconstructions of an area of interest. In that context automatic flight path planning and autonomous flying is often applied but so far cannot fully replace the human in the loop, supervising the flight on-site to assure that there are no collisions with obstacles. Unfortunately, this workflow yields several issues, such as the need to mentally transfer the aerial vehicle's position between 2D map positions and the physical environment, and the complicated depth perception of objects flying in the distance. Augmented Reality can address these issues by bringing the flight planning process on-site and visualizing the spatial relationship between the planned or current positions of the vehicle and the physical environment. In this paper, we present Augmented Reality supported navigation and flight planning of micro aerial vehicles by augmenting the user's view with relevant information for flight planning and live feedback for flight supervision. Furthermore, we introduce additional depth hints supporting the user in understanding the spatial relationship of virtual waypoints in the physical world and investigate the effect of these visualization techniques on the spatial understanding.",
            "link": "https://ieeexplore.ieee.org/abstract/document/6777462?casa_token=4Q5qUYNQGPkAAAAA:lLFaNYnsFpCsT3774mKFPKHfsyw3zqPx3npFablnhedk3EK2U8MqMlLWwaRkH05okkD7NHeO",
            "authors": "Zollmann, S., Hoppe, C., Langlotz, T., & Reitmayr, G.",
            "venue": "T-VCG",
            "sessions": "",
            "year": 2014,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Command Options & Validity",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 55,
            "slug": "paper_55",
            "title": "Utilization of the Oculus Rift HMD in Mobile Robot Teleoperation",
            "abstract": "This paper mentions some problems related to utilization of a head-mounted display (HMD) for remote control of mobile robots by a human operator and also presents a possible solution. Considered is specifically the new HMD device called Oculus Rift, which is a very interesting device because of its great parameters and low price. The device is described in the beginning, together with some of the specific principles of the Oculus 3D display. Then follows the design of a new graphical user interface for teleoperation, with main focus on visualization of stereoscopic images from robot cameras. Demonstrated is also a way how to display additional data and information to the operator. The overall aim is to create a comfortable and highly effective interface suitable both for exploration and manipulation tasks in mobile robotics.",
            "link": "https://www.scientific.net/AMM.555.199",
            "authors": "Kot, T., & Nov\u00e1k, P.",
            "venue": "Applied Mechanics and Materials",
            "sessions": "",
            "year": 2014,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 56,
            "slug": "paper_56",
            "title": "Vibi: Assistive vision-based interface for robot manipulation.",
            "abstract": "Upper-body disabled people can benefit from the use of robot-arms to perform every day tasks. However, the adoption of this kind of technology has been limited by the complexity of robot manipulation tasks and the difficulty in controlling a multiple-DOF arm using a joystick or a similar device. Motivated by this need, we present an assistive vision-based interface for robot manipulation. Our proposal is to replace the direct joystick motor control interface present in a commercial wheelchair mounted assistive robotic manipulator with a human-robot interface based on visual selection. The scene in front of the robot is shown on a screen, and the user can then select an object with our novel grasping interface. We develop computer vision and motion control methods that drive the robot to that object. Our aim is not to replace user control, but instead augment user capabilities through our system with different levels of semi-autonomy, while leaving the user with a sense that he/she is in control of the task. Two disabled pilot users, were involved at different stages of our research. The first pilot user during the interface design along with rehab experts. The second performed user studies along with an 8 subject control group to evaluate our interface. Our system reduces robot instruction from a 6-DOF task in continuous space to either a 2-DOF pointing task or a discrete selection task among objects detected by computer vision.",
            "link": "https://ieeexplore.ieee.org/abstract/document/7139816?casa_token=ZfdsID4fQRQAAAAA:OVUd1b5DJ2QdMnWVW0Ps3DkcTAUVRulxTMk5NtKyfS5QzpxIMoBQZZ0Ir0B_qb0AFuL32IMs",
            "authors": "Quintero, C. P., Ramirez, O., & J\u00e4gersand, M.",
            "venue": "ICRA",
            "sessions": "",
            "year": 2015,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Command Options & Validity",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 57,
            "slug": "paper_57",
            "title": "That\u2019s on my mind! robot to human intention communication through on-board projection on shared floor space",
            "abstract": "The upcoming new generation of autonomous vehicles for transporting materials in industrial environments will be more versatile, flexible and efficient than traditional AGVs, which simply follow pre-defined paths. However, freely navigating vehicles can appear unpredictable to human workers and thus cause stress and render joint use of the available space inefficient. Here we address this issue and propose on-board intention projection on the shared floor space for communication from robot to human. We present a research prototype of a robotic fork-lift equipped with a LED projector to visualize internal state information and intents. We describe the projector system and discuss calibration issues. The robot's ability to communicate its intentions is evaluated in realistic situations where test subjects meet the robotic forklift. The results show that already adding simple information, such as the trajectory and the space to be occupied by the robot in the near future, is able to effectively improve human response to the robot.",
            "link": "https://ieeexplore.ieee.org/abstract/document/7403771?casa_token=wjrt6OIHNT0AAAAA:xaaGiJXbygcuGJSZM9Foz8_2bHpJQNbNf-qWnHuHa4GUfE87UxD8swt_lSZv0EAxQd-zNIuR",
            "authors": "Chadalavada, R. T., Andreasson, H., Krug, R., & Lilienthal, A. J.",
            "venue": "ECMR",
            "sessions": "",
            "year": 2015,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 58,
            "slug": "paper_58",
            "title": "Communicating robotic navigational intentions",
            "abstract": "This paper presents a study on intention communication in a navigational context using a robotic wheelchair. The robotic wheelchair uses light projection to communicate its motion intentions. The novelty of the work is threefold: the communication of robot intentions to the passenger, the consideration of passenger and robot as a group (\u201cin-group\u201d) [1] who share motion intentions and the communication of the in-group intentions to other pedestrians (the \u201cout-group\u201d). A comparison in an autonomous navigation task where the robotic wheelchair autonomously navigates the environment with and without intention communication was performed showing that passengers and walking people found intention communication intuitive and helpful for passing by actions. Evaluation results significantly show human participant preference for having navigational intention communication for the wheelchair passenger and the person passing by it. Quantitative results show the motion of the person passing by the wheelchair with intention communication was significantly smoother compared to without intention communication.",
            "link": "https://ieeexplore.ieee.org/abstract/document/7354195",
            "authors": "Watanabe, A., Ikeda, T., Morales, Y., Shinozawa, K., Miyashita, T., & Hagita, N.",
            "venue": "IROS",
            "sessions": "",
            "year": 2015,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 59,
            "slug": "paper_59",
            "title": "Augmented reality-aided tele-presence system for robot manipulation in industrial manufacturing",
            "abstract": "This work investigates the use of a highly immersive telepresence system for industrial robotics. A Robot Operating System integrated framework is presented where a remote robot is controlled through operator's movements and muscle contractions captured with a wearable device. An augmented 3D visual feedback is sent to the user providing the remote environment scenario from the robot's point of view and additional information pertaining to the task execution. The system proposed, using robot mounted RGB-D camera, identifies known objects and relates their pose to robot arm pose and to targets relevant to the task execution. The system is preliminary validated during a pick-and-place task using a Baxter robot. The experiment shows the practicability and the effectiveness of the proposed approach.",
            "link": "https://dl.acm.org/doi/abs/10.1145/2821592.2821620?casa_token=jF_vGxwVS1AAAAAA:sSYU2uE7UPP1YiYpB4XAVqBF7Mz71Nto2Xd4M9E6rxP-FEivMn5scRr-wEVAH2b0Pamat4tRCyUP",
            "authors": "Peppoloni, L., Brizzi, F., Ruffaldi, E., & Avizzano, C. A.",
            "venue": "VRST",
            "sessions": "",
            "year": 2015,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 60,
            "slug": "paper_60",
            "title": "Design and Evaluation of a Natural Interface for Remote Operation of Underwater Robots (A natural interface for remote operation of underwater robots)",
            "abstract": "Human-machine interfaces play a crucial role in intervention robotic systems operated in hazardous environments, such as deep sea conditions. This article introduces a user interface abstraction layer to enhance reconfigurability. It also describes a VR-based interface that utilizes immersive technologies to reduce user faults and mental fatigue. The goal is to show the user only the most relevant information about the current mission.",
            "link": "https://ieeexplore.ieee.org/abstract/document/7325202?casa_token=LhAv4mPSTfgAAAAA:6sI2Z6VzBAeny31vMPySunCuaiSmmv3xg7d0TGx-vpGaxOOUvvK5Pnl__XCf21aQLWk6uMk-",
            "authors": "Garc\u00eda, J. C., Patr\u00e3o, B., Almeida, L., P\u00e9rez, J., Menezes, P., Dias, J., & Sanz, P. J.",
            "venue": "CG&A",
            "sessions": "",
            "year": 2015,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 61,
            "slug": "paper_61",
            "title": "Third Point of View Augmented Reality for Robot Intentions Visualization (System: a cyber-physical human system)",
            "abstract": "Lightweight, head-up displays integrated in industrial helmets allow to provide contextual information for industrial scenarios such as in maintenance. Moving from single display and single camera solutions to stereo perception and display opens new interaction possibilities. In particular this paper addresses the case of information sharing by a Baxter robot displayed to the user overlooking at the real scene. System design and interaction ideas are being presented.",
            "link": "https://link.springer.com/chapter/10.1007/978-3-319-40621-3_35",
            "authors": "Ruffaldi, E., Brizzi, F., Tecchia, F., & Bacinelli, S.",
            "venue": "ARVRCG",
            "sessions": "",
            "year": 2016,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 62,
            "slug": "paper_62",
            "title": "Projecting robot intentions into human environments",
            "abstract": "Trained human co-workers can often easily predict each other's intentions based on prior experience. When collaborating with a robot coworker, however, intentions are hard or impossible to infer. This difficulty of mental introspection makes human-robot collaboration challenging and can lead to dangerous misunderstandings. In this paper, we present a novel, object-aware projection technique that allows robots to visualize task information and intentions on physical objects in the environment. The approach uses modern object tracking methods in order to display information at specific spatial locations taking into account the pose and shape of surrounding objects. As a result, a human co-worker can be informed in a timely manner about the safety of the workspace, the site of next robot manipulation tasks, and next subtasks to perform. A preliminary usability study compares the approach to collaboration approaches based on monitors and printed text. The study indicates that, on average, the user effectiveness and satisfaction is higher with the projection based approach.",
            "link": "https://ieeexplore.ieee.org/abstract/document/7745145?casa_token=dl801qwR2tsAAAAA:l920iq5hH_OfnByN8D3oFgX2vqUdAiT5x3xgPvRfYaBt_IBDu2LYrc0mI8mnr2gDjDUdI5c8",
            "authors": "Andersen, R. S., Madsen, O., Moeslund, T. B., & Amor, H. B.",
            "venue": "ROMAN",
            "sessions": "",
            "year": 2016,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Labels",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 63,
            "slug": "paper_63",
            "title": "Realizing mixed-reality environments with tablets for intuitive human-robot collaboration for object manipulation tasks",
            "abstract": "Although gesture-based input and augmented reality (AR) facilitate intuitive human-robot interactions (HRI), prior implementations have relied on research-grade hardware and software. This paper explores using tablets to render mixed-reality visual environments that support human-robot collaboration for object manipulation. A mobile interface is created on a tablet by integrating real-time vision, 3D graphics, touchscreen interaction, and wireless communication. This mobile interface augments a live video of physical objects in a robot's workspace with corresponding virtual objects that can be manipulated by a user to intuitively command the robot to manipulate the physical objects. By generating the mixed-reality environment on an exocentric view provided by the tablet camera, the interface establishes a common frame of reference for the user and the robot to effectively communicate spatial information for object manipulation. After addressing challenges due to limitations in mobile sensing and computation, the interface is evaluated with participants to examine the performance and user experience with the suggested approach.",
            "link": "https://ieeexplore.ieee.org/abstract/document/7745146?casa_token=VQB8u75rFYkAAAAA:GboyzqLaJnS8nH92VylEtG7Sc3ECtt1xwF-ALL4Q8LIE5lnMwssRWgWGvsvrP0K3GT_rea_a",
            "authors": "Frank, J. A., Moorhead, M., & Kapila, V.",
            "venue": "ROMAN",
            "sessions": "",
            "year": 2016,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Labels",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 64,
            "slug": "paper_64",
            "title": "Fourbythree: Imagine humans and robots working hand in hand",
            "abstract": "Since December 2014, FourByThree Project (\u201cHighly customizable robotic solutions for effective and safe human robot collaboration in manufacturing applications\u201d) is developing a new generation of modular industrial robotic solutions that are suitable for efficient task execution in collaboration with humans in a safe way and are easy to use and program by factory workers. This paper summarizes the key technologies that are used to achieve this goal.",
            "link": "https://ieeexplore.ieee.org/abstract/document/7733583?casa_token=IxkpUaXFQGYAAAAA:m8qdjiO2gJvhOHbW8jdiA2Rs2hP02zHetdmc0cRxD0WIoqr1QaEwMh7NYyFQHnvivkZzH938",
            "authors": "Maurtua, I., Pedrocchi, N., Orlandini, A., de Gea Fern\u00e1ndez, J., Vogel, C., Geenen, A., Althoefer, K. and Shafti, A.",
            "venue": "ETFA",
            "sessions": "",
            "year": 2016,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Robot Inherent Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Labels",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 65,
            "slug": "paper_65",
            "title": "Augmented reality for industrial robot programmers: Workload analysis for taskbased, augmented reality-supported robot control",
            "abstract": "Augmented reality (AR) can serve as a tool to provide helpful information in a direct way to industrial robot programmers throughout the teaching process. It seems obvious that AR support eases the programming process and increases the programmer's productivity and programming accuracy. However, additional information can also potentially increase the programmer's perceived workload. To explore the impact of augmented reality on robot teaching, as a first step we have chosen a Sphero robot control scenario and conducted a within-subject user study with 19 professional industrial robot programmers, including novices and experts. We focused on the perceived workload of industrial robot programmers and their task completion time when using a tablet-based AR approach with visualization of task-based information for controlling a robot. Each participant had to execute three typical robot programming tasks: tool center point teaching, trajectory teaching, and overlap teaching. We measured the programmers' workload in the dimensions of mental demand, physical demand, temporal demand, frustration, effort, and performance. The study results show that the presentation of task-based information in the tablet-based AR interface decreases the mental demand of the industrial robot programmers during the robot control process. At the same time, however, the programmers' task completion time increases.",
            "link": "https://ieeexplore.ieee.org/abstract/document/7745108?casa_token=czrPXUF86-4AAAAA:xAmpWg2BcWcRSBWEXNlovrYaBNFXBceTpYsP7AW9oXC1qKQ_11SaT0yBMVgqFyNslx1jylEN",
            "authors": "Stadler, S., Kain, K., Giuliani, M., Mirnig, N., Stollnberger, G., & Tscheligi, M.",
            "venue": "ROMAN",
            "sessions": "",
            "year": 2016,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 66,
            "slug": "paper_66",
            "title": "Measurable augmented reality for prototyping cyberphysical systems: A robotics platform to aid the hardware prototyping and performance testing of algorithms",
            "abstract": "Planning, control, perception, and learning are current research challenges in multirobot systems. The transition dynamics of the robots may be unknown or stochastic, making it difficult to select the best action each robot must take at a given time. The observation model, a function of the robots' sensor systems, may be noisy or partial, meaning that deterministic knowledge of the team's state is often impossible to attain. Moreover, the actions each robot can take may have an associated success rate and/or a probabilistic completion time. Robots designed for real-world applications require careful consideration of such sources of uncertainty, regardless of the control scheme or planning or learning algorithms used for a specific problem. Understanding the underlying mechanisms of planning algorithms can be challenging due to the latent variables they often operate on. When performance testing such algorithms on hardware, the simultaneous use of the debugging and visualization tools available on a workstation can be difficult. This transition from experimentation to implementation becomes especially challenging when the experiments need to replicate some feature of the software tool set in hardware, such as simulation of visually complex environments. This article details a robotics prototyping platform, called measurable augmented reality for prototyping cyberphysical systems (MAR-CPS), that directly addresses this problem, allowing for the real-time visualization of latent state information to aid hardware prototyping and performance testing of algorithms.",
            "link": "https://ieeexplore.ieee.org/abstract/document/7740990?casa_token=AMaqHoI3l2wAAAAA:wCOYLxh7ZEdI3ZOYNqct5bz2XMEYNwra44TNFV7ZTe_x-0SOj9ZC6SLF_MxX6tqCO-AupPq9",
            "authors": "Omidshafiei, S., Agha-Mohammadi, A.A., Chen, Y.F., Ure, N.K., Liu, S.Y., Lopez, B.T., Surati, R., How, J.P. and Vian, J.",
            "venue": "IEEE Control Systems Magazine",
            "sessions": "",
            "year": 2016,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "Internal Readiness",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 67,
            "slug": "paper_67",
            "title": "Visual Programming for Mobile Robot Navigation Using High-level Landmarks",
            "abstract": "We propose a visual programming system that allows users to specify navigation tasks for mobile robots using high-level landmarks in a virtual reality (VR) environment constructed from the output of visual simultaneous localization and mapping (vSLAM). The VR environment provides a Google Street View-like interface for users to familiarize themselves with the robot's working environment, specify high-level landmarks, and determine task-level motion commands related to each landmark. Our system builds a roadmap by using the pose graph from the vSLAM outputs. Based on the roadmap, the high-level landmarks, and task-level motion commands, our system generates an output path for the robot to accomplish the navigation task. We present data structures, architecture, interface, and algorithms for our system and show that, given n s search-type motion commands, our system generates a path in O(n s (n r logn r +m r )) time, where n r and m r are the number of roadmap nodes and edges, respectively. We have implemented our system and tested it on real world data.",
            "link": "https://ieeexplore.ieee.org/abstract/document/7759449?casa_token=i82qv1-EDE8AAAAA:El_2RRUPeGlnyCVSvRERdgaI_GQACK_rCCtRTFp01iNuZ1g57W6Syu-YkYU-xqcmOSsRd0dz",
            "authors": "Lee, J., Lu, Y., Xu, Y., & Song, D.",
            "venue": "IROS",
            "sessions": "",
            "year": 2016,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 68,
            "slug": "paper_68",
            "title": "Immersive remote grasping: realtime gripper control by a heterogenous robot control system",
            "abstract": "Current developments in the field of user interface (UI) technologies as well as robotic systems provide enormous potential to reshape the future of human-robot interaction (HRI) and collaboration. However, the design of reliable, intuitive and comfortable user interfaces is a challenging task. In this paper, we focus on one important aspect of such interfaces, i.e., teleoperation. We explain how to setup a heterogeneous, extendible and immersive system for controlling a distant robotic system via the network. Therefore, we exploit current technologies from the area of virtual reality (VR) and the Unity3D game engine in order to provide natural user interfaces for teleoperation. Regarding robot control, we use the well-known robot operating system (ROS) and apply its freely available modular components. The contribution of this work lies in the implementation of a flexible immersive grasping control system using a network layer (ROSbridge) between Unity3D and ROS for arbitary robotic hardware.",
            "link": "https://dl.acm.org/doi/abs/10.1145/2993369.2996345?casa_token=1hKhL70BhbEAAAAA:szwFqyXUptxzu1kXyp20z1Gj0pcI2qCbEvVJrRHlAVjBrabO0IhSK_wkMG4aN7-_3aKWfWat5sk6",
            "authors": "Krupke, D., Einig, L., Langbehn, E., Zhang, J., & Steinicke, F.",
            "venue": "VRST",
            "sessions": "",
            "year": 2016,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "3D Controllers",
                    "parent": "Control Objects"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 69,
            "slug": "paper_69",
            "title": "Towards ominidirectional immersion for ROV teleoperation",
            "abstract": "The use of omnidirectional cameras underwater is enabling many new and exciting applications in multiple fields. Among them, it will allow Remotely Operated Underwater Vehicles (ROVs) to be piloted directly by means of the images captured by omnidirectional cameras through virtual reality (VR) headsets. This immersive experience will extend the pilot\u2019s spatial awareness and reduce the usual orientation problems during missions. This paper presents this concept and illustrates it with the first experiments for achieving this purpose.",
            "link": "https://ruc.udc.es/dspace/handle/2183/29592",
            "authors": "Bosch, J., Ridao Rodriguez, P., Garcia, R., & Gracias, N.",
            "venue": "Automatica",
            "sessions": "",
            "year": 2016,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 70,
            "slug": "paper_70",
            "title": "Mobile mixed-reality interfaces that enhance human\u2013robot interaction in shared spaces",
            "abstract": "Although user interfaces with gesture-based input and augmented graphics have promoted intuitive human\u2013robot interactions (HRI), they are often implemented in remote applications on research-grade platforms requiring significant training and limiting operator mobility. This paper proposes a mobile mixed-reality interface approach to enhance HRI in shared spaces. As a user points a mobile device at the robot\u2019s workspace, a mixed-reality environment is rendered providing a common frame of reference for the user and robot to effectively communicate spatial information for performing object manipulation tasks, improving the user\u2019s situational awareness while interacting with augmented graphics to intuitively command the robot. An evaluation with participants is conducted to examine task performance and user experience associated with the proposed interface strategy in comparison to conventional approaches that utilize egocentric or exocentric views from cameras mounted on the robot or in the environment, respectively. Results indicate that, despite the suitability of the conventional approaches in remote applications, the proposed interface approach provides comparable task performance and user experiences in shared spaces without the need to install operator stations or vision systems on or around the robot. Moreover, the proposed interface approach provides users the flexibility to direct robots from their own visual perspective (at the expense of some physical workload) and leverages the sensing capabilities of the tablet to expand the robot\u2019s perceptual range.",
            "link": "https://www.frontiersin.org/articles/10.3389/frobt.2017.00020/full",
            "authors": "Frank, J. A., Moorhead, M., & Kapila, V.",
            "venue": "Frontiers R&AI",
            "sessions": "",
            "year": 2017,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Robot Inherent Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Labels",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 71,
            "slug": "paper_71",
            "title": "Communicating robot arm motion intent through mixed reality head-mounted displays",
            "abstract": "Efficient motion intent communication is necessary for safe and collaborative work environments with co-located humans and robots. Humans efficiently communicate their motion intent to other humans through gestures, gaze, and other non-verbal cues, and can replan their motions in response. However, robots often have difficulty using these methods. Many existing methods for robot motion intent communication rely on 2D displays, which require the human to continually pause their work to check a visualization. We propose a mixed-reality head-mounted display (HMD) visualization of the intended robot motion over the wearer\u2019s real-world view of the robot and its environment. In addition, our interface allows users to adjust the intended goal pose of the end effector using hand gestures. We describe its implementation, which connects a ROS-enabled robot to the HoloLens using ROS Reality, using MoveIt for motion planning, and using Unity to render the visualization. To evaluate the effectiveness of this system against a 2D display visualization and against no visualization, we asked 32 participants to label various arm trajectories as either colliding or non-colliding with blocks arranged on a table. We found a 15% increase in accuracy with a 38% decrease in the time it took to complete the task compared with the next best system. These results demonstrate that a mixed-reality HMD allows a human to determine where the robot is going to move more quickly and accurately than existing baselines.",
            "link": "https://journals.sagepub.com/doi/full/10.1177/0278364919842925",
            "authors": "Rosen, E., Whitney, D., Phillips, E., Chien, G., Tompkin, J., Konidaris, G., & Tellex, S.",
            "venue": "IJRR",
            "sessions": "",
            "year": 2017,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 72,
            "slug": "paper_72",
            "title": "Differences in interaction patterns and perception for teleoperated and autonomous humanoid robots",
            "abstract": "As the linguistic capabilities of interactive robots advance, it becomes increasingly important to understand how humans will instruct robots through natural language. What is more, with the increased use of teleoperated humanoid robots, it is important to recognize whether any differences between instructions given to humans and to robots are due to the physical embodiment or to the perceived autonomy of the instructee. In this paper, we present the results of a human-subject experiment in which participants interacted in a collaborative, task-based setting with both a human and a suit-based, teleoperated humanoid robot said to be either autonomous or teleoperated. Our results suggest that humans will use politeness strategies equally with human, autonomous robotic, and teleoperated robotic teammates, reinforcing recent findings that autonomous robots must comprehend and appropriately respond to human utterances that follow such strategies. Our results also suggest variations in how different teammates were perceived. Specifically, our results suggest that human-teleoperated robots were perceived as less intelligent than human teammates; a finding with serious implications for human-robot team dynamics.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8206571?casa_token=6D_zzlOibKYAAAAA:VIMlo2EBPaecen5IvdrUv_NQpkLpuD3tcrvSC22PzS6FV0Fx4Ds7r2mmlBeh4XB6ORHSy1iB",
            "authors": "Bennett, M., Williams, T., Thames, D., & Scheutz, M.",
            "venue": "IROS",
            "sessions": "",
            "year": 2017,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 73,
            "slug": "paper_73",
            "title": "A Comparison of Remote Robot Teleoperation Interfaces for General Object Manipulation",
            "abstract": "Robust remote teleoperation of high-DOF manipulators is of critical importance across a wide range of robotics applications. Contemporary robot manipulation interfaces primarily utilize a free-positioning pose specification approach to independently control each axis of translation and orientation in free space. In this work, we present two novel interfaces, constrained positioning and point-and-click, which incorporate scene information, including points-of-interest and local surface geometry, into the grasp specification process. We also present results of a user study evaluation comparing the effects of increased use of scene information in grasp pose specification algorithms for general object manipulation. The results of our study show that constrained positioning and point-and-click significantly outperform the widely used free positioning approach by significantly reducing the number of grasping errors and the number of user interactions required to specify poses. Furthermore, the point-and-click interface significantly increased the number of tasks users were able to complete.",
            "link": "https://dl.acm.org/doi/abs/10.1145/2909824.3020249",
            "authors": "Kent, D., Saldanha, C., & Chernova, S.",
            "venue": "HRI",
            "sessions": "",
            "year": 2017,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "3D Controllers",
                    "parent": "Control Objects"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Command Options & Validity",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 74,
            "slug": "paper_74",
            "title": "Real-time human-in-the-loop remote control for a life-size traffic police robot with multiple augmented reality aided display terminals",
            "abstract": "Policing of road traffic is listed in the most hazardous tasks since many traffic police personnel injured in a series of accidents at intersections. A life-size traffic cop robot called \u201cIWI\u201d is invented as an alternative to human traffic police in the street for directing traffic on point duty, and this paper proposes a human-robot cooperation scheme with wearable augmented glasses as an HMI to obtain a more immersing teleoperation experience with the aid of wearable augmented reality glasses. First, a humanoid robot is tailored for mobile surveillance with raspberry pi camera based eyes and omnidirectional Mecanum wheels. Second, the real-time video stream acquired by the on-site robot is distributed to multiple terminals, such as the central sever, the wearable augmented reality glasses and the mobile control tablet. Third, with a depth of focus estimation the augmented indications are displayed to aid to understand the remote scenario. Finally, the human policing results, such as STOP, PULL OVER, TURN-LEFT, are compiled and programmed as simplified patterns to control the robot body/hand pantomime. Experimental results show that the proposed methodology and control scheme is feasible in real-time application with high real-time performance of less than 0.5s latency, and open possibilities of easing the traffic jam via simultaneously scheduling multiple traffic cop robots.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8273199?casa_token=v0ncMLVsQHkAAAAA:2vbLuFhot0dZJvbP7Fwct9wwl5Ig4AMIEZNtEgEjsyFwUACMuAPrllmfXE1FmV1vRgEwHh--",
            "authors": "Gong, L., Gong, C., Ma, Z., Zhao, L., Wang, Z., Li, X., Jing, X., Yang, H. and Liu, C.",
            "venue": "ICARM",
            "sessions": "",
            "year": 2017,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 75,
            "slug": "paper_75",
            "title": "Effects of Augmented Reality on the Performance of Teleoperated Industrial Assembly Tasks in a Robotic Embodiment",
            "abstract": "Teleoperation in robotic embodiments allows operators to perform and program manipulation tasks with better accuracy, dexterity, and visualization than what is possible with traditional human-robot interaction paradigms. However, the perception of cues (e.g., egocentric distances) relevant to task execution, is known to be distorted in virtual environments due to many factors, which can be grouped into technical, human, and methodological categories. This phenomenon becomes more pronounced in a low-cost/encumbrance setup, where the dynamic environment is captured with color and depth (RGB-D) cameras and presented in a virtual environment. In this paper, the effects of augmented reality (AR) are evaluated as a tool to deliver additional information, which helps in overcoming the differences in perception between telepresence and actual presence. The AR feedback is used to improve the embodiment illusion and to guide the operator during task execution. The AR setup, comprising an RGB-D camera and a head-mounted display, is integrated with the Baxter robot and evaluated by involving 22 participants in an experiment while they execute a pick-and-place task, taking into account their expertise in AR/virtual reality (VR) and gaming. The use of AR results in enhancing the accuracy and efficiency of the task performance, besides significantly reducing the effect of the differences in skillfulness between the participants. Furthermore, it is found that the sense of presence and embodiment for the participant is positively affected by different types of AR.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8241709?casa_token=nG9Yln-kJfEAAAAA:6j-WkKF8QRqB8dTan_-1GZtiZdHB2Ax0zXmbE_KWk1CLsEyYT-7osQEyhg10W4R3JrCpeaaP",
            "authors": "Brizzi, F., Peppoloni, L., Graziano, A., Di Stefano, E., Avizzano, C. A., & Ruffaldi, E.",
            "venue": "HMS",
            "sessions": "",
            "year": 2017,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Numerical Readings",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 76,
            "slug": "paper_76",
            "title": "Augmented Reality Dialog Interface for Multimodal Teleoperation",
            "abstract": "We designed an augmented reality interface for dialog that enables the control of multimodal behaviors in telepresence robot applications. This interface, when paired with a telepresence robot, enables a single operator to accurately control and coordinate the robot's verbal and nonverbal behaviors. Depending on the complexity of the desired interaction, however, some applications might benefit from having multiple operators control different interaction modalities. As such, our interface can be used by either a single operator or pair of operators. In the paired-operator system, one operator controls verbal behaviors while the other controls nonverbal behaviors. A within-subjects user study was conducted to assess the usefulness and validity of our interface in both single and paired-operator setups. When faced with hard tasks, coordination between verbal and nonverbal behavior improves in the single-operator condition. Despite single operators being slower to produce verbal responses, verbal error rates were unaffected by our conditions. Finally, significantly improved presence measures such as mental immersion, sensory engagement, ability to view and understand the dialog partner, and degree of emotion occur for single operators that control both the verbal and nonverbal behaviors of the robot.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8172389?casa_token=n_Q9t2OvZ5MAAAAA:0E6YVdqVOHjVW7MfrP6PqRJMaTWe-spHtxsswAs-HMyrvnviogEJ_JrZ6zfRLvUC-Nt1YX7v",
            "authors": "Pereira, A., Carter, E. J., Leite, I., Mars, J., & Lehman, J. F.",
            "venue": "ROMAN",
            "sessions": "",
            "year": 2017,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Command Options & Validity",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 77,
            "slug": "paper_77",
            "title": "Comparing robot grasping teleoperation across desktop and virtual reality with ros reality (Comparing Desktop and VR)",
            "abstract": "Teleoperation allows a human to remotely operate a robot to perform complex and potentially dangerous tasks such as defusing a bomb, repairing a nuclear reactor, or maintaining the exterior of a space station. Existing teleoperation approaches generally rely on computer monitors to display sensor data and joysticks or keyboards to actuate the robot. These approaches use 2D interfaces to view and interact with a 3D world, which can make using them difficult for complex or delicate tasks. To address this problem, we introduce a virtual reality interface that allows users to remotely teleoperate a physical robot in real-time. Our interface allows users to control their point of view in the scene using virtual reality, increasing situational awareness (especially of object contact), and to directly move the robot\u2019s end effector by moving a hand controller in 3D space, enabling fine-grained dexterous control. We evaluated our interface on a cup-stacking manipulation task with 18 participants, comparing the relative effectiveness of a keyboard and mouse interface, virtual reality camera control, and positional hand tracking. Our system reduces task completion time from 153 s (\u00b144) to 53 s (\u00b137), a reduction of 66%, while improving subjective assessments of system usability and workload. Additionally, we have shown the effectiveness of our system over long distances, successfully completing a cup stacking task from over 40 miles away. Our paper contributes a quantitative assessment of robot grasping teleoperation across desktop and virtual reality interfaces.",
            "link": "https://link.springer.com/chapter/10.1007/978-3-030-28619-4_28",
            "authors": "Whitney, D., Rosen, E., Phillips, E., Konidaris, G., & Tellex, S.",
            "venue": "ISRR",
            "sessions": "",
            "year": 2017,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 78,
            "slug": "paper_78",
            "title": "Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation",
            "abstract": "Imitation learning is a powerful paradigm for robot skill acquisition. However, obtaining demonstrations suitable for learning a policy that maps from raw pixels to actions can be challenging. In this paper we describe how consumer-grade Virtual Reality headsets and hand tracking hardware can be used to naturally teleoperate robots to perform complex tasks. We also describe how imitation learning can learn deep neural network policies (mapping from pixels to actions) that can acquire the demonstrated skills. Our experiments showcase the effectiveness of our approach for learning visuomotor skills.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8461249?casa_token=WeAQiojfUE4AAAAA:et66IQfb-BTa0UwrRQG1EOwMyMZ-ZKTYFnd2Uk9OA5IvMhvy9_drPomcZX1Rf5BhkY7X65-B",
            "authors": "Zhang, T., McCarthy, Z., Jow, O., Lee, D., Chen, X., Goldberg, K., & Abbeel, P.",
            "venue": "ICRA",
            "sessions": "",
            "year": 2017,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 79,
            "slug": "paper_79",
            "title": "Adaptive Tutoring on a Virtual Reality Driving Simulator",
            "abstract": "We propose a system for a Virtual Reality (VR) driving simulator including an Intelligent Tutoring System (ITS) to train the user's driving skills. The VR driving simulator comprises a detailed model of a city, Artificial Intelligence (AI) traffic, and a physical driving engine, interacting with the driver. In a physical mockup of a car cockpit, the driver operates the vehicle through the virtual environment by controlling a steering wheel, pedals, and a gear lever. Using a Head-Mounted Display (HMD), the driver observes the scene from within the car. The realism of the simulation is enhanced by a 6 Degrees of Freedom (DOF) motion platform, capable of simulating forces experienced when accelerating, braking, or turning the car. Based on a pre-defined list of driving-related activities, the ITS permanently assesses the quality of driving during the simulation and suggests an optimal path through the city to the driver in order to improve the driving skills. A user study revealed that most drivers experience presence in the virtual world and are proficient in operating the car.",
            "link": "https://www.research-collection.ethz.ch/handle/20.500.11850/195951",
            "authors": "Ropelato, S., Z\u00fcnd, F., Magnenat, S., Menozzi, M., & Sumner, R.",
            "venue": "AIVRAR",
            "sessions": "",
            "year": 2017,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Agents",
                    "parent": "Environmental"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 80,
            "slug": "paper_80",
            "title": "Understanding Human-Robot Interaction in Virtual Reality",
            "abstract": "Interactions with simulated robots are typically presented on screens. Virtual reality (VR) offers an attractive alternative as it provides visual cues that are more similar to the real world. In this paper, we explore how virtual reality mediates human-robot interactions through two user studies. The first study shows that in situations where perception of the robot is challenging, a VR display provides significantly improved performance on a collaborative task. The second study shows that this improved performance is primarily due to stereo cues. Together, the findings of these studies suggest that VR displays can offer users unique perceptual benefits in simulated robotics applications.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8172387?casa_token=RCSnSBPUfcIAAAAA:CNe8T8etz6stOCSfu1-u09B3SwSBbwKu-9dGEaV19csfjmt8smQBnMpNcJ0GGLE7H2xWlwNA",
            "authors": "Liu, O., Rakita, D., Mutlu, B., & Gleicher, M.",
            "venue": "ROMAN",
            "sessions": "",
            "year": 2017,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Objects",
                    "parent": "Environmental"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 81,
            "slug": "paper_81",
            "title": "Development of a Driving Simulator with Analyzing Driver's Characteristics Based on a Virtual Reality Head Mounted Display",
            "abstract": "Driving a vehicle is one of the most common daily yet hazardous tasks. One of the great interests in recent research is to characterize a driver\u2019s behaviors through the use of a driving simulation. Virtual reality technology is now a promising alternative to the conventional driving simulations since it provides a more simple, secure and user-friendly environment for data collection. The driving simulator was used to assist novice drivers in learning how to drive in a very calm environment since the driving is not taking place on an actual road. This paper provides new insights regarding a driver\u2019s behavior, techniques and adaptability within a driving simulation using virtual reality technology. The theoretical framework of this driving simulation has been designed using the Unity3D game engine (5.4.0f3 version) and programmed by the C# programming language. To make the driving simulation environment more realistic, the HTC Vive Virtual reality headset, powered by Steamvr, was used. 10 volunteers ranging from ages 19 - 37 participated in the virtual reality driving experiment. Matlab R2016b was used to analyze the data obtained from experiment. This research results are crucial for training drivers and obtaining insight on a driver\u2019s behavior and characteristics. We have gathered diverse results for 10 drivers with different characteristics to be discussed in this study. Driving simulations are not easy to use for some users due to motion sickness, difficulties in adopting to a virtual environment. Furthermore, results of this study clearly show the performance of drivers is closely associated with individual\u2019s behavior and adaptability to the driving simulator. Based on our findings, it can be said that with a VR-HMD (Virtual Reality-Head Mounted Display) Driving Simulator enables us to evaluate a driver\u2019s \u201cperformance error\u201d, \u201crecognition errors\u201d and \u201cdecision error\u201d. All of which will allow researchers and further studies to potentially establish a method to increase driver safety or alleviate \u201cdriving errors\u201d.",
            "link": "https://www.scirp.org/html/8-3500378_77941.htm",
            "authors": "Taheri, S. M., Matsushita, K., & Sasaki, M.",
            "venue": "JTTs",
            "sessions": "",
            "year": 2017,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Agents",
                    "parent": "Environmental"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 82,
            "slug": "paper_82",
            "title": "Towards an Augmented Reality Framework for K-12 Robotics Education",
            "abstract": "In this paper, we investigate how augmented reality (AR) can help students \u201csee the unseen\u201d when learning to operate and program robots.We describe our prototype AR system for robotics education, along with a qualitative pilot study and its preliminary results. The objectives of the pilot study were 1) demonstrate that AR can be successfully deployed in a middle school robotics education setting; and, 2) identify and document how AR might (or might not) catalyze students\u2019 ability to understand their robot\u2019s behavior and adapt their code accordingly. Overall, the pilot study indicated that AR can help students debug their robot more easily, catalyzing discussions around sensor readings that led to code fixes and a reduction in the \u201cbarrier to entry\u201d for some students. At the same time, we also gained some insight into usability issues and current challenges of using AR in the classroom.",
            "link": "https://www.eecs.tufts.edu/~jsinapov/papers/Cheli_VAMHRI2018.pdf",
            "authors": "Cheli, M., Sinapov, J., Danahy, E. E., & Rogers, C.",
            "venue": "VAM-HRI",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor Numerical Readings",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 83,
            "slug": "paper_83",
            "title": "v2v Communication for Augmenting Reality Enabled Smart HUDs to Increase Situational Awareness of Drivers",
            "abstract": "Inter-car communication has emerged in recent times as a viable solution towards reducing traffic hazards, with the recent US government mandate in favor of vehicle-to-vehicle communication highlighting the movement towards this direction in the automobile industry. However, questions remain as to how information from other cars can be effectively relayed to a driver, especially so as to not overload the driver with too much information. Meanwhile, a parallel thread of development in the space of Smart HUDs has shown the applicability of augmented reality to increase the situational awareness of drivers on the road. In this paper, we build on these threads of work and show how Smart HUDs can be an effective platform for projecting relevant information from surrounding vehicles in real time, and how an onboard AI component can avoid increased cognitive burden on the driver by determining when and what information to project based on its models of the driver and the surrounding environment.",
            "link": "https://yochan-lab.github.io/papers/files/papers/vam-hri-cars.pdf",
            "authors": "Dudley, A., Chakraborti, T., & Kambhampati, S.",
            "venue": "VAM-HRI",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 84,
            "slug": "paper_84",
            "title": "Towards Augmented Reality Interfaces for Human-Robot Interaction in Manufacturing Environments",
            "abstract": "As part of the U.S. Department of Commerce, the National Institute of Standards and Technology (NIST) is examining new approaches to increase the competitiveness of smaller U.S.-based manufacturers. Collaborative robotics is one way to increase automation, leveraging the specialized skills of human workers along with the increased repeatability and throughput of robots. However, in order for humans to efficiently work alongside robots, new types of interfaces are needed to streamline interactions. Augmented reality, especially wearable devices, are a new approach that can enable task training, hands-free interactions, and increased safety through situational awareness. This paper will explore some preliminary approaches to integrating augmented reality for a collaborative manufacturing environment.",
            "link": "https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=925355",
            "authors": "Bagchi, S., & Marvel, J. A.",
            "venue": "VAM-HRI",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 85,
            "slug": "paper_85",
            "title": "Designing an AR interface to improve trust in Human-Robots collaboration",
            "abstract": "In a global, e-commerce marketplace, product customisation is driven towards manufacturing flexibility. Conventional caged robots are designed for high volume and low mix production cannot always comply with the increasing low volume and high customisation requirements. In this scenario, the interest in collaborative robots is growing. A critical aspect of Human-Robot Collaboration (HRC) is human trust in robots. This research focuses on increasing the human confidence and trust in robots by designing an Augmented Reality (AR) interface for HRC. The variable affecting the trust involved in HRC have been estimated. These have been utilised for designing the AR-HRC. The proposed design aims to provide situational awareness and spatial dialog. The AR-HRC developed has been tested on 15 participants which have performed a \u201cpick-and-place\u201d task. The results show that the utilisation of AR in the proposed scenario positively affects the human trust in robot. The human-robot collaboration enhanced by AR are more natural and effective. The trust has been measured through an empirical psychometric method also presented in this paper.",
            "link": "https://www.sciencedirect.com/science/article/pii/S2212827118300155",
            "authors": "Palmarini, R., del Amo, I. F., Bertolino, G., Dini, G., Erkoyuncu, J. A., Roy, R., & Farnsworth, M.",
            "venue": "Procedia CIRP",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 86,
            "slug": "paper_86",
            "title": "Enabling Human-Robot-interaction via Virtual and Augmented Reality in Distributed Control System",
            "abstract": "Production and assembly lines are nowadays transforming into flexible and interconnected cells due to rapidly changing production demands. Changes are, for example, varying locations and poses for the processed work pieces and tools as well as the involved machinery like industrial robots. Even a variation in the combination or sequence of different production steps is possible. In case of older involved machines the task of reconfiguration and calibration can be time consuming. This may lead, in addition with the expected upcoming shortage of highly skilled workers, to future challenges, especially for small and medium sized enterprises. One possibility to address these challenges is to use distributed or cloud-based control for the participating machines. These approaches allow the use of more sophisticated and therefore in most cases computationally heavier algorithms than offered by classic monolithic controls. Those algorithms range from simple visual servoing applications to more complex scenarios, like sampling-based path planning in a previously 3D-reconstructed robot cell. Moving the computation of the machine\u2019s reactions physically and logically away from the machine control complicates the supervision and verification of the computed robot paths and trajectories. This poses a potential threat to the machine\u2019s operator since he/she is hardly capable of predicting or controlling the robot\u2019s movements. To overcome this drawback, this paper presents a system which allows the user to interact with industrial robot and other cyber physical systems via augmented and virtual reality. Captured topics in this paper are the architecture and concept for the distributed system, first implementation details and promising results obtained by using a Microsoft HoloLens and other visualization devices.",
            "link": "https://www.sciencedirect.com/science/article/pii/S221282711830043X",
            "authors": "Guhl, J., H\u00fcgle, J., & Kr\u00fcger, J.",
            "venue": "Procedia CIRP",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 87,
            "slug": "paper_87",
            "title": "Development of a 3D VR-Based Interface for Industrial Robot Manipulators",
            "abstract": "Along with the progress in automation and Industry 4.0, the use of industrial robots becomes much more popular. Meanwhile, the conventional manipulative devices, such as teaching pendant and joystick, are not intuitive enough and very time-consuming for task teaching. In facing more complicated tasks these days, it then induces high cost and demands experts with proper training. This paper thus aims to develop an intuitive manipulatory interface for assistance in simplifying the teaching process. We propose a wireless 3D manipulation system based on augmented reality (AR) with the tablet PC as the platform. The system allows the user to govern the movement of the real robot in a 3D space via a virtual one generated through the AR. We also develop several intelligent tools to assist the manipulation, so that accurate and collision-free path can be generated for the robot to follow for task execution. In addition, the system can also be used for offline training, which yields salient training effect for the operator. Experiments are executed to demonstrate the effectiveness of the proposed system, and questionnaires conducted to investigate user's response.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8616309?casa_token=-tIqjdo71k4AAAAA:a1yAUxY32IYnMFVUvNOqwCdqjEThO-RocwX8SbFQ6cfhBcSl8OkDeh9IJl0B_-2WmfVDwVXJ",
            "authors": "Su, Y. H., Chen, C. Y., Cheng, S. L., Ko, C. H., & Young, K. Y.",
            "venue": "SMC",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 88,
            "slug": "paper_88",
            "title": "Come See This! Augmented Reality to Enable human-Robot Cooperative search",
            "abstract": "Robots operating alongside humans in field environments have the potential to greatly increase the situational awareness of their human teammates. A significant challenge, however, is the efficient conveyance of what the robot perceives to the human in order to achieve improved situational awareness. We believe augmented reality (AR), which allows a human to simultaneously perceive the real world and digital information situated virtually in the real world, has the potential to address this issue. Motivated by the emerging prevalence of practical human-wearable AR devices, we present a system that enables a robot to perform cooperative search with a human teammate, where the robot can both share search results and assist the human teammate in navigation to the search target. We demonstrate this ability in a search task in an uninstrumented environment where the robot identifies and localizes targets and provides navigation direction via AR to bring the human to the correct target.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8468622?casa_token=aWlgm9t5TKkAAAAA:wI3KXTqafqDgTrIFEusMPR2eT5TxNXdbH3dQDkGjPMrn5GRuCuEbHkYdVWBrwnz-sfGuPRqS",
            "authors": "Reardon, C., Lee, K., & Fink, J.",
            "venue": "SSRR",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 89,
            "slug": "paper_89",
            "title": "Comparison Multimodal heading and Pointing gestures for Co-Located Mixed Reality Human-Robot Interaction",
            "abstract": "Mixed reality (MR)opens up new vistas for human-robot interaction (HRI)scenarios in which a human operator can control and collaborate with co-located robots. For instance, when using a see-through head-mounted-display (HMD)such as the Microsoft HoloLens, the operator can see the real robots and additional virtual information can be superimposed over the real-world view to improve security, acceptability and predictability in HRI situations. In particular, previewing potential robot actions in-situ before they are executed has enormous potential to reduce the risks of damaging the system or injuring the human operator. In this paper, we introduce the concept and implementation of such an MR human-robot collaboration system in which a human can intuitively and naturally control a co-located industrial robot arm for pick-and-place tasks. In addition, we compared two different, multimodal HRI techniques to select the pick location on a target object using (i)head orientation (aka heading)or (ii)pointing, both in combination with speech. The results show that heading-based interaction techniques are more precise, require less time and are perceived as less physically, temporally and mentally demanding for MR-based pick-and-place scenarios. We confirmed these results in an additional usability study in a delivery-service task with a multi-robot system. The developed MR interface shows a preview of the current robot programming to the operator, e. g., pick selection or trajectory. The findings provide important implications for the design of future MR setups.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8594043?casa_token=DYf7b8s_HKgAAAAA:t7uxu_5QzOiGWB8m1fz1hXmujCftU5AUnMrHTJGdP37Rl0WK5Ge8r6EV2FTnVi-5p1AodwPX",
            "authors": "Krupke, D., Steinicke, F., Lubos, P., Jonetzko, Y., G\u00f6rner, M., & Zhang, J.",
            "venue": "IROS",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 90,
            "slug": "paper_90",
            "title": "Communicating directional Intent in Robot Navigation using Projection Indicators",
            "abstract": "Smooth and efficient robot navigation among humans is a crucial requirement for successful integration of robots in human society. Towards this end, an indispensable characteristic of robot action is legibility while communicating its intention. However, unlike humans, present robots cannot convey its intention through human-like non-verbal communication. This paper explores the use of projection indicators for communicating directional intent of a robot across three different `crossing scenarios' as a means of overcoming the shortcomings of the robot's non-verbal communication abilities. The results of the study show statistically significant improvement in perceived feelings of the measured attributes when using the auxiliary communication method. The studied method also improves cooperation from the participants. Nevertheless, the improvement in perceived feeling does not necessarily replicate in terms of smoothness across all the scenarios.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8525528?casa_token=qVvJyv4h6_0AAAAA:7fQvHKWs01Llkt-3zUhfpD0YVGpoc1BcjMH8aax8PUUc_PqpIx9GjgsxEZH8za-3-WPoCXk6",
            "authors": "Shrestha, M. C., Onishi, T., Kobayashi, A., Kamezaki, M., & Sugano, S.",
            "venue": "ROMAN",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 91,
            "slug": "paper_91",
            "title": "An Education Application for Teaching Robot Arm Manipulator Concepts Using Augmented Reality",
            "abstract": "Teaching robotics is a challenge in many universities due to the mathematics concepts used in this area. In recent years, augmented reality has improved learning in several engineering areas. In this paper, a platform for teaching robotic arm manipulation concepts is presented. The system includes a homemade robotic arm, a control system, and the RAR@pp. The RAR@pp is focused on learning robotic arm manipulation algorithms by the detection of markers in the robotic arm and displaying in real time the values based on the data obtained by the control system. Details on the design of the platform are presented, and the related results are discussed. Experimental data about the usability of the application are also shown.",
            "link": "https://www.hindawi.com/journals/misy/2018/6047034/",
            "authors": "Hern\u00e1ndez-Ordo\u00f1ez, M., Nu\u00f1o-Maganda, M. A., Calles-Arriaga, C. A., Monta\u00f1o-Rivas, O., & Bautista Hern\u00e1ndez, K. E.",
            "venue": "Mobile Information System",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 92,
            "slug": "paper_92",
            "title": "Better Teaming Through Visual Cues: How Projecting Imagery in a Workspace Can Improve Human-Robot Collaboration",
            "abstract": "In this article, we present a communication paradigm using a context-aware mixed-reality approach for instructing human workers when collaborating with robots. The main objective is to utilize the physical work environment as a canvas to communicate task-related instructions and robot intentions in the form of visual cues. A vision-based object-tracking algorithm is used to precisely determine the pose and state of physical objects in and around the workspace. A projection-mapping technique is employed to overlay visual cues on the tracked objects and the workspace. Simultaneous tracking and projection onto objects enable the system to provide just-in-time instructions for carrying out a procedural task.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8359206?casa_token=G5q6Yv5N2ZYAAAAA:fu3Rrx2EyiHO6Hf9k8jInyXlXoI6Aogrc_HeYJ-vc_ZqHsZLy6GUyHQk9OvPQKkP5w4N0rq-",
            "authors": "Ganesan, R. K., Rathore, Y. K., Ross, H. M., & Amor, H. B.",
            "venue": "IEEE Robotics and Automation Magazine",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Numerical Readings",
                    "parent": "Environment"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Robot Inherent Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Attributes",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Appearances",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Alteration Previews",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 93,
            "slug": "paper_93",
            "title": "Facilitating HRI by Mixed Reality Techniques",
            "abstract": "Mobile robots start to appear in our everyday life, e.g., in shopping malls, airports, nursing homes or warehouses. Often, these robots are operated by non-technical staff with no prior experience/education in robotics. Additionally, as with all new technology, there is certain reservedness when it comes to accepting robots in our personal space. In this work, we propose making use of state-of-the-art Mixed Reality (MR) technology to facilitate acceptance and interaction with mobile robots. By integrating a Microsoft HoloLens into the robot\u00bbs operating space, the MR device can be used to a) visualize the robot\u00bbs behavior-state and sensor data, b) visually notify the user about planned/future behavior and possible problems/obstacles of the robot, and c) to actively use the device as an additional external sensor source. Moreover, by using the HoloLens, users can operate and interact with the robot without being close to it, as the robot is able to sense with the users\u00bb eyes.",
            "link": "https://dl.acm.org/doi/abs/10.1145/3173386.3177032?casa_token=7p_o8J3sxiwAAAAA:tj2iTefiPjck3QX1bjoOmGez6ZybRYCQtSRCvhCsKYViuBv5fl8y4hjqtsoYVas9s12Rcf2KelD0",
            "authors": "Renner, P., Lier, F., Friese, F., Pfeiffer, T., & Wachsmuth, S.",
            "venue": "HRI",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Robot Inherent Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 94,
            "slug": "paper_94",
            "title": "Ani-Bot: A Modular Robotics System Supporting Creation, Tweaking, and Usage with Mixed-Reality Interactions",
            "abstract": "Ani-Bot is a modular robotics system that allows users to control their DIY robots using Mixed-Reality Interaction (MRI). This system takes advantage of MRI to enable users to visually program the robot through the augmented view of a Head-Mounted Display (HMD). In this paper, we first explain the design of the Mixed-Reality (MR) ready modular robotics system, which allows users to instantly perform MRI once they finish assembling the robot. Then, we elaborate the augmentations provided by the MR system in the three primary phases of a construction kit's lifecycle: Creation, Tweaking, and Usage. Finally, we demonstrate Ani-Bot with four application examples and evaluate the system with a two-session user study. The results of our evaluation indicate that Ani-Bot does successfully embed MRI into the lifecycle (Creation, Tweaking, Usage) of DIY robotics and that it does show strong potential for delivering an enhanced user experience.",
            "link": "https://dl.acm.org/doi/abs/10.1145/3173225.3173226",
            "authors": "Cao, Y., Xu, Z., Glenn, T., Huo, K., & Ramani, K.",
            "venue": "TEI",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "3D Controllers",
                    "parent": "Control Objects"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Objects",
                    "parent": "Environmental"
                },
                {
                    "name": "Virtual Alterations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Morphological",
                    "parent": "Virtual Alterations"
                },
                {
                    "name": "Body Extensions",
                    "parent": "Morphological"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor Numerical Readings",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Command Options & Validity",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 95,
            "slug": "paper_95",
            "title": "A Comparison of Visualisation Methods for Disambiguating Verbal Requests in Human-Robot Interaction",
            "abstract": "Picking up objects requested by a human user is a common task in human-robot interaction. When multiple objects match the user's verbal description, the robot needs to clarify which object the user is referring to before executing the action. Previous research has focused on perceiving user's multimodal behaviour to complement verbal commands or minimising the number of follow up questions to reduce task time. In this paper, we propose a system for reference disambiguation based on visualisation and compare three methods to disambiguate natural language instructions. In a controlled experiment with a YuMi robot, we investigated realtime augmentations of the workspace in three conditions - head-mounted display, projector, and a monitor as the baseline - using objective measures such as time and accuracy, and subjective measures like engagement, immersion, and display interference. Significant differences were found in accuracy and engagement between the conditions, but no differences were found in task time. Despite the higher error rates in the head-mounted display condition, participants found that modality more engaging than the other two, but overall showed preference for the projector condition over the monitor and head-mounted display conditions.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8525554?casa_token=G0XKW92LyjwAAAAA:VU6aISU2cRwHkLLb61YqenzD-ur68e-ej6IdvupGOvfqWZ7gEA4do46l9ioE38C1CxaQrWRW",
            "authors": "Sibirtseva, E., Kontogiorgos, D., Nykvist, O., Karaoguz, H., Leite, I., Gustafson, J., & Kragic, D.",
            "venue": "ROMAN",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Labels",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 96,
            "slug": "paper_96",
            "title": "Interactive Spatial Augmented Reality in Collaborative Robot Programming: User Experience Evaluation",
            "abstract": "This paper presents a novel approach to interaction between human workers and industrial collaborative robots. The proposed approach addresses problems introduced by existing solutions for robot programming. It aims to reduce the mental demands and attention switches by centering all interaction in a shared workspace, combining various modalities and enabling interaction with the system without any external devices. The concept allows simple programming in the form of setting program parameters using spatial augmented reality for visualization and a touch-enabled table and robotic arms as input devices. We evaluated the concept utilizing a user experience study with six participants (shop-floor workers). All participants were able to program the robot and to collaborate with it using the program they parametrized. The final goal is to create a distraction-free, usable and low-effort interface for effective human-robot collaboration, enabling any ordinary skilled worker to customize the robot's program to changes in production or to personal (e.g. ergonomic) needs.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8525662?casa_token=6RMwkav07voAAAAA:iGKnM6fsYC1nGpE512K7XuU622CaWl1GWDRsn6PVkgl3HQ4zGJz6zUVk1EIM1iJRvktDVZ0-",
            "authors": "Materna, Z., Kapinus, M., Beran, V., Smr\u017e, P., & Zem\u010d\u00edk, P.",
            "venue": "ROMAN",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "User-Defined Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Labels",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Attributes",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Command Options & Validity",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 97,
            "slug": "paper_97",
            "title": "Exploring Augmented Reality Interaction for Everyday Multipurpose Wearable Robots",
            "abstract": "Multipurpose wearable robots are an emerging class of devices, which offer intriguing interaction potential. The interoperability of multipurpose wearable robots and other types of wearable devices remains largely uninvestigated. We take the first steps to bridge this gap by presenting a framework for integrating augmented reality (AR) and multipurpose wearable robots. Our framework uses the publisher-subscriber model to expose different robot functionalities as services on a network. These can be invoked by the AR system. This model is advantageous in coping with different robot morphologies and various interaction methods. We implemented a prototype system using our framework by integrating an AR head-mounted display (HMD) and a wrist-worn robot, and demonstrate four experiences utilizing our prototype solution: 1) Robot status display, 2) shape-changing menus, 3) a media player and 4) a robot pose controller. To evaluate our approach, we performed a user study, which gauged user impressions and usability of developed experiences. Results indicate that our approach was well received, though participants highlighted a number of challenges in AR tracking when interacting within some of the experiences. Lastly, we discuss limitations and future research direction for our project.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8607251?casa_token=acXYuhWWgwgAAAAA:A_PLS7CoUI4UwuEAYXr3caGIsb8EKTRqygF4rBCapDnUwb_k0icy_BCcf_r0SfAxVx7lkYWv",
            "authors": "Urbani, J., Al-Sada, M., Nakajima, T., & H\u00f6glund, T.",
            "venue": "RTCSA",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 98,
            "slug": "paper_98",
            "title": "Design and Evaluation of a Natural user Interface for Piloting and Unmanned Aerial Vehicle",
            "abstract": "Controlling an unmanned aerial vehicle is challenging and requires an intensive training. One cause is the teleoperation with the conventional input device, the remote control, whose functions are complicate. This paper presents an alternative concept for the teleoperation. Its realization includes a Thalmic Myo gesture control wristlet and a Microsoft HoloLens head-mounted display. These devices are used to implement an augmented reality interface, a tactile feedback and a gesture and speech input. Finally, this implementation has been evaluated with 30 participants and compared with a conventional remote control. The results show that the proposed interface is a good solution but does not reach the performance of the remote control.",
            "link": "https://www.degruyter.com/document/doi/10.1515/icom-2018-0001/html",
            "authors": "Herrmann, R., & Schmidt, L.",
            "venue": "i-com",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 99,
            "slug": "paper_99",
            "title": "Interactive Robot Programming Using Mixed Reality",
            "abstract": "The number of robots in industry is increasing every year, they are used to automate various processes. It is necessary to create a system for intuitive and effective interaction between robot and human. This paper represents a system for interactive programming of industrial robots based on Mixed Reality (MR). Microsoft HoloLens glasses were used for immersion in MR. By the developed system, a user without programming skills can assign a task to the manipulator. The main system parts are intuitive geometric path planning, time-optimal trajectory planning, and simulator based on MR. Via MR users are able to interact with robots and the spatial environment through the highly intuitive interface. The system comprises two KUKA robot with different kinematic models, with and without redundancy. The works show features and advantages of the MR-based system comparing to Augmented and Virtual Reality. Experiments on three different path cases show the system performance.",
            "link": "https://www.sciencedirect.com/science/article/pii/S2405896318332233",
            "authors": "Ostanin, M., & Klimchik, A.",
            "venue": "IFAC",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 100,
            "slug": "paper_100",
            "title": "An Augmented Interface to Display Industrial Robot Faults",
            "abstract": "Technology advancement is changing the way industrial factories have to face an increasingly complex and competitive market. The fourth industrial revolution (known as industry 4.0) is also changing how human workers have to carry out tasks and actions. In fact, it is no longer impossible to think of a scenario in which human operators and industrial robots work side-by-side, sharing the same environment and tools. To realize a safe work environment, workers should trust robots as well as they trust human operators. Such goal is indeed complex to achieve, especially when workers are under stress conditions, such as when a fault occurs and the human operators are no longer able to understand what is happening in the industrial manipulator. Indeed, Augmented Reality (AR) can help workers to visualize in real-time robots\u2019 faults. This paper proposes an augmented system that assists human workers to recognize and visualize errors, improving their awareness of the system. The system has been tested using both an AR see-through device and a smartphone.",
            "link": "https://link.springer.com/chapter/10.1007/978-3-319-95282-6_30",
            "authors": "Pace, F. D., Manuri, F., Sanna, A., & Zappia, D.",
            "venue": "AVR",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Virtual Alterations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Superficial",
                    "parent": "Virtual Alterations"
                },
                {
                    "name": "Cosmetic Alterations",
                    "parent": "Superficial"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "Internal Readiness",
                    "parent": "Internal"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Robot Inherent Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 101,
            "slug": "paper_101",
            "title": " Communicating Robot Motion Intent with Augmented Reality",
            "abstract": "Humans coordinate teamwork by conveying intent through social cues, such as gestures and gaze behaviors. However, these methods may not be possible for appearance-constrained robots that lack anthropomorphic or zoomorphic features, such as aerial robots. We explore a new design space for communicating robot motion intent by investigating how augmented reality (AR) might mediate human-robot interactions. We develop a series of explicit and implicit designs for visually signaling robot motion intent using AR, which we evaluate in a user study. We found that several of our AR designs significantly improved objective task efficiency over a baseline in which users only received physically-embodied orientation cues. In addition, our designs offer several trade-offs in terms of intent clarity and user perceptions of the robot as a teammate.",
            "link": "https://dl.acm.org/doi/abs/10.1145/3171221.3171253?casa_token=82EBGGEW0s0AAAAA:_n9mSKg4il2ueVZjcAJgLliRG0rGVTC5Kww7Pj-rxD2MtTOS_iihseqGn0O_bwdIogwZ2j3ylpB2",
            "authors": "Walker, M., Hedayati, H., Lee, J., & Szafir, D.",
            "venue": "HRI",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Alterations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Morphological",
                    "parent": "Virtual Alterations"
                },
                {
                    "name": "Form Transformations",
                    "parent": "Morphological"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 102,
            "slug": "paper_102",
            "title": "Interactive Robot Knowledge patching using Augmented Reality",
            "abstract": "We present a novel Augmented Reality (AR) approach, through Microsoft HoloLens, to address the challenging problems of diagnosing, teaching, and patching interpretable knowledge of a robot. A Temporal And-Or graph (T-AOG) of opening bottles is learned from human demonstration and programmed to the robot. This representation yields a hierarchical structure that captures the compositional nature of the given task, which is highly interpretable for the users. By visualizing the knowledge structure represented by a T-AOG and the decision making process by parsing the T-AOG, the user can intuitively understand what the robot knows, supervise the robot's action planner, and monitor visually latent robot states ( e.g. , the force exerted during interactions). Given a new task, through such comprehensive visualizations of robot's inner functioning, users can quickly identify the reasons of failures, interactively teach the robot with a new action, and patch it to the current knowledge structure. In this way, the robot is capable of solving similar but new tasks only through minor modifications provided by the users interactively. This process demonstrates the interpretability of our knowledge representation and the effectiveness of the AR interface.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8462837?casa_token=Y-35iMNuDw8AAAAA:KGRz3xwwMePDYwelFIoSZCOsgNBTlorI8neInjRYUiLaiWcq8W9nRNBadDN9V9JzuuoouaMq",
            "authors": "Liu, H., Zhang, Y., Si, W., Xie, X., Zhu, Y., & Zhu, S. C.",
            "venue": "ICRA",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Command Options & Validity",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 103,
            "slug": "paper_103",
            "title": "Improving collocated robot teleoperation with augmented reality",
            "abstract": "Robot teleoperation can be a challenging task, often requiring a great deal of user training and expertise, especially for platforms with high degrees-of-freedom (e.g., industrial manipulators and aerial robots). Users often struggle to synthesize information robots collect (e.g., a camera stream) with contextual knowledge of how the robot is moving in the environment. We explore how advances in augmented reality (AR) technologies are creating a new design space for mediating robot teleoperation by enabling novel forms of intuitive, visual feedback. We prototype several aerial robot teleoperation interfaces using AR, which we evaluate in a 48-participant user study where participants completed an environmental inspection task. Our new interface designs provided several objective and subjective performance benefits over existing systems, which often force users into an undesirable paradigm that divides user attention between monitoring the robot and monitoring the robot\u00bbs camera feed(s).",
            "link": "https://dl.acm.org/doi/abs/10.1145/3171221.3171251?casa_token=rjbeMh4eEFsAAAAA:BUEWTvteexq7v0lifNLjoEUI24QhwAr9X6iBnzMEJo4nWKE-jr_LFlA_n3DetZxrf9lSNZ09e_dw",
            "authors": "Hedayati, H., Walker, M., & Szafir, D.",
            "venue": "HRI",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 104,
            "slug": "paper_104",
            "title": "Telewheelchair: the Remote Controllable electric Wheelchair System combined Human and Machine Intelligence",
            "abstract": "Wheelchairs are essential means of transport for the elderly people and the physically challenged. However, wheelchairs need to be accompanied by caregivers. As society ages and the number of care recipients increases, the burden on caregivers is expected to increase. In order to reduce the burden on caregivers, we present Telewheelchair, an electric wheelchair equipped with a remote control function and computational operation assistance function. The caregiver can remotely control the Telewheelchair by means of a head mounted display (HMD). In addition, the proposed system is equipped with a human detection system to stop the wheelchair automatically and avoid collisions. We conducted a user study on the wheelchair in four types of systems and investigated the time taken to achieve tasks. Telewheelchair will enhance geriatric mobility and improve society by combining human intelligence and machine intelligence.",
            "link": "https://dl.acm.org/doi/abs/10.1145/3174910.3174914?casa_token=19EBIPZOJCQAAAAA:esjx3V43Uj4XM2VfPVwxaNrmjicvznVfEKEIEYCle1CvRaw5WoWKxcLS9v_XbAe_2wwtJ7O3rF4q",
            "authors": "Hashizume, S., Suzuki, I., Takazawa, K., Sasaki, R., & Ochiai, Y.",
            "venue": "AH",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 105,
            "slug": "paper_105",
            "title": "Point Cloud Augmented Virtual Reality Environment with Haptic Constraints for Teleoperation",
            "abstract": "Remote manipulation of a robot without assistance in an unstructured environment is a challenging task for operators. In this paper, a novel methodology for haptic constraints in a point cloud augmented virtual reality environment is proposed to address this human operation limitation. The proposed method generates haptic constraints in real time for an unstructured environment, including regional constraints and guidance constraints. A modified implicit surface method is applied for regional constraint generation for the entire point cloud. Additionally, the isosurface derived from the implicit surface is proposed for real-time three-dimensional artificial force field estimation. For guidance constraint generation, a new incremental prediction and local artificial force field generation method based on the modified sigmoid model is proposed in an unstructured point cloud virtual reality environment. With the generated haptic constraints, the operator can control the robot to realize obstacle avoidance and easily reach the target tasks. System evaluation is conducted, and the result demonstrates the effectiveness of the proposed method. In addition, a 10-participant study with users who control the robot to three specific targets shows that the system can enhance human operation efficiency and reduce time costs by at least 59% compared with no-haptic-constraint operations. Additionally, the designed questionnaire also demonstrates that the proposed methodology can reduce the workload during human operations.",
            "link": "https://journals.sagepub.com/doi/full/10.1177/0142331217739953?casa_token=61nV4t83AZ8AAAAA%3A7PUor3tZRyuYemQvWvM5En7g_LzCWtFYCKXZ19QXvzq1puLQIKD_hb-PZWTsIVNpoxmAdraBUMd0",
            "authors": "Ni, D., Nee, A. Y., Ong, S. K., Li, H., Zhu, C., & Song, A.",
            "venue": "ToIMC",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 106,
            "slug": "paper_106",
            "title": "Robot Programming Through Augmented Trajectories in Augmented Reality",
            "abstract": "Point Cloud Augmented Virtual Reality Environment with Haptic Constraints for Teleoperation",
            "link": "https://ieeexplore.ieee.org/abstract/document/8593700?casa_token=cW-0TVzeSp8AAAAA:VKaOSlJtKko8YdYm5j1pEHzktQ6RopHvlxrG-FQXH-7u_UDeOKIeWzZeexbJHruFp2eEZ4AL",
            "authors": "Quintero, C. P., Li, S., Pan, M. K., Chan, W. P., Van der Loos, H. M., & Croft, E.",
            "venue": "IROS",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Attributes",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 107,
            "slug": "paper_107",
            "title": "Mapping System with Virtual Reality for Mobile Robot Teleoperation",
            "abstract": "Robots using teleoperation, especially linking robots to users in real-time have been studied. For a convenient and immersive operation to a robot, it should be able to use sensors which are mounted on the robot, and effectively visualize it to increase the immersive feeling of the user. We propose a system that creates a map using a camera and a sensor installed in the robot, recognizes the surrounding environment. It visualizes PointCloud data in real time on the user's virtual reality device. To demonstrate our proposed system, we used TurtleBot and Gear VR as robot and user device, respectively. Also, a laptop connected with TurtleBot processes the PointCloud data obtained from Depth Camera to Octomap, sends it to the VR device, and the VR device visualizes received data in real time. We experimented in a real environment by creating a VR-based teleoperation system that remotely manipulates TurtleBot and visualizes the data. The transmitted PointCloud data were reflected in the VR device in real time so that the user can remotely recognize the situation of the site.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8571684?casa_token=90W3NVHzWW8AAAAA:XRfhKi_FFaFzTzpffz1FxwVe7xCyghhz36XzvKKoFt0Vt2M2Thhaiy2Nw7Nj4CwgyolXoSq7",
            "authors": "Kim, S., Kim, Y., Ha, J., & Jo, S.",
            "venue": "ICCAS",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 108,
            "slug": "paper_108",
            "title": "Baxter\u2019s homunculus: Virtual reality spaces for teleoperation in manufacturing",
            "abstract": "We demonstrate a low-cost telerobotic system that leverages commercial virtual reality (VR) technology and integrates it with existing robotics control infrastructure. The system runs on a commercial gaming engine using off-the-shelf VR hardware and can be deployed on multiple network architectures. The system is based on the homunculus model of mind wherein we embed the user in a VR control room. The control room allows for multiple sensor displays, and dynamic mapping between the user and robot. This dynamic mapping allows for selective engagement between the user and the robot. We compared our system with state-of-the-art automation algorithms and standard VR-based telepresence systems by performing a user study. The study showed that new users were faster and more accurate than the automation or a direct telepresence system. We also demonstrate that our system can be used for pick and place, assembly, and manufacturing tasks.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8003431?casa_token=YLOg19g8hWgAAAAA:D6DvUR5mkKMNBMGSdr_7z3iY0Yf4XaIx87sR6voMpcYnkl38ttr69p53z_NymlGsUiefGnGM",
            "authors": "Lipton, J. I., Fay, A. J., & Rus, D.",
            "venue": "RA-L",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "3D Controllers",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Numerical Readings",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Command Options & Validity",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 109,
            "slug": "paper_109",
            "title": "The Effects of visual and Control Latency on Piloting a Quadcopter Using a Head-Mounted Display",
            "abstract": "Recent research has proposed teleoperation of robotic and aerial vehicles using head motion tracked by a head-mounted display (HMD). First-person views of the vehicles are usually captured by onboard cameras and presented to users through the display panels of HMDs. This provides users with a direct, immersive and intuitive interface for viewing and control. However, a typically overlooked factor in such designs is the latency introduced by the vehicle dynamics. As head motion is coupled with visual updates in such applications, visual and control latency always exists between the issue of control commands by head movements and the visual feedback received at the completion of the attitude adjustment. This causes a discrepancy between the intended motion, the vestibular cue and the visual cue and may potentially result in simulator sickness. No research has been conducted on how various levels of visual and control latency introduced by dynamics in robots or aerial vehicles affect users' performance and the degree of simulator sickness elicited. Thus, it is uncertain how much performance is degraded by latency and whether such designs are comfortable from the perspective of users. To address these issues, we studied a prototyped scenario of a head motion controlled quadcopter using an HMD. We present a virtual reality (VR) paradigm to systematically assess the effects of visual and control latency in simulated drone control scenarios.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8616501?casa_token=APnvA2s64f4AAAAA:1TbA2PBW-bLJrkZda33tPrBnTigKT5nHOEVhJpNI5xupm-T-YX8yM5oAsWWqKoIjM1DFjMsG",
            "authors": "Zhao, J., Allison, R. S., Vinnikov, M., & Jennings, S.",
            "venue": "SMC",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 110,
            "slug": "paper_110",
            "title": "Teaching a Robot to Grasp Real Fish by Imitation Learning from a Human Supervisor in Virtual Reality",
            "abstract": "We teach a real robot to grasp real fish, by training a virtual robot exclusively in virtual reality. Our approach implements robot imitation learning from a human supervisor in virtual reality. A deep 3D convolutional neural network computes grasps from a 3D occupancy grid obtained from depth imaging at multiple viewpoints. In virtual reality, a human supervisor can easily and intuitively demonstrate examples of how to grasp an object, such as a fish. From a few dozen of these demonstrations, we use domain randomization to generate a large synthetic training data set consisting of 100 000 example grasps of fish. Using this data set for training purposes, the network is able to guide a real robot and gripper to grasp real fish with good success rates. The newly proposed domain randomization approach constitutes the first step in how to efficiently perform robot imitation learning from a human supervisor in virtual reality in a way that transfers well to the real world.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8593954?casa_token=aYlcNccnBFUAAAAA:fc-T20ueEzIDkR-iogWyKpM8OrXFoIDVbvIKO_vWMrkGxdozXmoIsf5RvV2rWL0aZsREzjlu",
            "authors": "Dyrstad, J. S., \u00d8ye, E. R., Stahl, A., & Mathiassen, J. R.",
            "venue": "IROS",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Objects",
                    "parent": "Environmental"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 111,
            "slug": "paper_111",
            "title": "Get ready for automated driving using Virtual Reality",
            "abstract": "In conditionally automated vehicles, drivers can engage in secondary activities while traveling to their destination. However, drivers are required to appropriately respond, in a limited amount of time, to a take-over request when the system reaches its functional boundaries. Interacting with the car in the proper way from the first ride is crucial for car and road safety in general. For this reason, it is necessary to train drivers in a risk-free environment by providing them the best practice to use these complex systems. In this context, Virtual Reality (VR) systems represent a promising training and learning tool to properly familiarize drivers with the automated vehicle and allow them to interact with the novel equipment involved. In addition, Head-Mounted Display (HMD)-based VR (light VR) would allow for the easy deployment of such training systems in driving schools or car dealerships. In this study, the effectiveness of a light Virtual Reality training program for acquiring interaction skills in automated cars was investigated. The effectiveness of this training was compared to a user manual and a fixed-base simulator with respect to both objective and self-reported measures. Sixty subjects were randomly assigned to one of the systems in which they went through a training phase followed by a test drive in a high-end driving simulator. Results show that the training system affects the take-over performances. Moreover, self-reported measures indicate that the light VR training is preferred with respect to the other systems. Finally, another important outcome of this research is the evidence that VR plays a strategic role in the definition of the set of metrics for profiling proper driver interaction with the automated vehicle.",
            "link": "https://www.sciencedirect.com/science/article/pii/S0001457518302197?casa_token=B36N6yjzscIAAAAA:VxmxffhBaR0mQfWR4JXJ3eB6ybeW7fxfdgqX616tUQyollQwa24CJCFGEhVYuReW2anGlaTJig",
            "authors": "Sportillo, D., Paljic, A., & Ojeda, L.",
            "venue": "Accident Analysis and Prevention",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "3D Controllers",
                    "parent": "Control Objects"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 112,
            "slug": "paper_112",
            "title": "Prototyping proactive and adaptive techniques for human-robot collaboration in manufacturing using virtual reality",
            "abstract": "Human-Robot Interaction (HRI) has emerged in recent years as a need for common collaborative execution of manufacturing tasks. This work examines two types of techniques of safe collaboration that do not interrupt the flow of collaboration as far as possible, namely proactive and adaptive. The former are materialised using audio and visual cognitive aids, which the user receives as dynamic stimuli in real time during collaboration, and are aimed at information enrichment of the latter. Adaptive techniques investigated refer to the robot; according to the first one of them the robot decelerates when a forthcoming contact with the user is traced, whilst according to the second one the robot retracts and moves to the final destination via a modified, safe trajectory, so as to avoid the human. The effectiveness as well as the activation criteria of the above techniques are investigated in order to avoid possible pointless or premature activation. Such investigation was implemented in a prototype highly interactive and immersive Virtual Environment (VE), in the framework of H-R collaborative hand lay-up process of carbon fabric in an industrial workcell. User tests were conducted, in which both subjective metrics of user satisfaction and performance metrics of the collaboration (task completion duration, robot mean velocity, number of detected human-robot collisions etc.) After statistical processing, results do verify the effectiveness of safe collaboration techniques as well as their acceptability by the user, showing that collaboration performance is affected to a different extent.",
            "link": "https://www.sciencedirect.com/science/article/pii/S0736584516303611?casa_token=njaHJtqSIoEAAAAA:SoDMVPIWJXa6F6jFg4w-Wutxjydx388U0McPt4M5wES5Pl9JqPosU3dppl9HQJ8EjV7PYHllGQ",
            "authors": "Matsas, E., Vosniakos, G. C., & Batras, D.",
            "venue": "Robotics and Computer-Integrated Manufacturing",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Objects",
                    "parent": "Environmental"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Robot Inherent Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 113,
            "slug": "paper_113",
            "title": "Enhancing Perceived Safety in Human-Robot Collaborative Construction Using Immersive Virtual Environments",
            "abstract": "Advances in robotics now permit humans to work collaboratively with robots. However, humans often feel unsafe working alongside robots. Our knowledge of how to help humans overcome this issue is limited by two challenges. One, it is difficult, expensive and time-consuming to prototype robots and set up various work situations needed to conduct studies in this area. Two, we lack strong theoretical models to predict and explain perceived safety and its influence on human\u2013robot work collaboration (HRWC). To address these issues, we introduce the Robot Acceptance Safety Model (RASM) and employ immersive virtual environments (IVEs) to examine perceived safety of working on tasks alongside a robot. Results from a between-subjects experiment done in an IVE show that separation of work areas between robots and humans increases perceived safety by promoting team identification and trust in the robot. In addition, the more participants felt it was safe to work with the robot, the more willing they were to work alongside the robot in the future.",
            "link": "https://www.sciencedirect.com/science/article/pii/S0926580517302868?casa_token=3EC-OJUc0ksAAAAA:uo9eYiI3BomFQ_KVXVyqGu7SRpz1qJOMbQb54en1ScP0VMU6lZ_xY7npt5kiAfl7RyhC097ryw",
            "authors": "You, S., Kim, J. H., Lee, S., Kamat, V., & Robert Jr, L. P.",
            "venue": "Automation in Construction",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Objects",
                    "parent": "Environmental"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 114,
            "slug": "paper_114",
            "title": "Virtual-to-Real-World Transfer Learning for Robots on Wilderness Trails",
            "abstract": "Robots hold promise in many scenarios involving outdoor use, such as search-and-rescue, wildlife management, and collecting data to improve environment, climate, and weather forecasting. However, autonomous navigation of outdoor trails remains a challenging problem. Recent work has sought to address this issue using deep learning. Although this approach has achieved state-of-the-art results, the deep learning paradigm may be limited due to a reliance on large amounts of annotated training data. Collecting and curating training datasets may not be feasible or practical in many situations, especially as trail conditions may change due to seasonal weather variations, storms, and natural erosion. In this paper, we explore an approach to address this issue through virtual-to-real-world transfer learning using a variety of deep learning models trained to classify the direction of a trail in an image. Our approach utilizes synthetic data gathered from virtual environments for model training, bypassing the need to collect a large amount of real images of the outdoors. We validate our approach in three main ways. First, we demonstrate that our models achieve classification accuracies upwards of 95% on our synthetic data set. Next, we utilize our classification models in the control system of a simulated robot to demonstrate feasibility. Finally, we evaluate our models on real-world trail data and demonstrate the potential of virtual-to-real-world transfer learning.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8593883?casa_token=gFsXnRdUtCgAAAAA:IdrrK2KqJoAQHn1OZO5iRQzfn9SqEZ4gnoU2qVAOp9N8aOzX1ywZmoUmWo9mUZQNqC3t0i9S",
            "authors": "Iuzzolino, M. L., Walker, M. E., & Szafir, D.",
            "venue": "IROS",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Agents",
                    "parent": "Environmental"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 115,
            "slug": "paper_115",
            "title": "A Multimodal System using Augmented Reality, gestures, and Tactile Feedback for Robot Trajectory Programming and Execution",
            "abstract": "Currently available interfaces for programming industrial robots, e.g., teach pendants and computer consoles, are often unintuitive, resulting in a slow and tedious process for teaching robot tasks. Kinesthetic teaching, i.e., teaching robot motions by placing the robot in a gravity compensated state and then moving the robot though the desired motions, provides an alternative for small robots for which safe interaction can be guaranteed. However for many large industrial robots physical interaction is not an option. Emerging augmented reality technology offers an alternative interface with the potential to make robotic programming faster, safer, and more intuitive. The use of augmented reality admits the presentation of large amounts of rich, visual, in-situ information. However, it may also overload the user\u2019s visual information capacity, or may not provide sufficient feedback regarding the state of the robot. With the addition of gestural control and tactile feedback to augmented reality, we propose a system that allows users to program and execute robot tasks in an efficient and intuitive manner, by providing relevant feedback through different channels to maximize clear communication of the task commands and outcomes.",
            "link": "https://www.academia.edu/download/60799408/workshop20191004-114415-alzoyy.pdf",
            "authors": "Chan, W. P., Quintero, C. P., Pan, M. K. X. J., Sakr, M., Van der Loos, H. M., & Croft, E.",
            "venue": "ICRA",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Numerical Readings",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Attributes",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Appearances",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Alteration Previews",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 116,
            "slug": "paper_116",
            "title": "Collaborative Meat Processing in Virtual Reality",
            "abstract": "Efficient and safe collaboration between humans and industrial robots requires a mutual understanding that can be achieved by clearly signalling intentions and acting predictably. We present a preliminary version of a virtual reality environment, which is intended as a platform for development and evaluation of techniques that enable collaboration with industrial robots. In addition, we presents a user study for evaluating two speed profiles for having large industrial robots approach a human coworker. We compare the predictability and the perceived safety of the speed profiles, showing that test participants prefer a gradual slowdown, as the robot approaches, opposed to a faster approach where the slowdown is more abrupt.",
            "link": "https://vbn.aau.dk/en/publications/collaborative-meat-processing-in-virtual-reality-evaluating-perce",
            "authors": "Hansen, L. I., Vinther, N., Stranovsky, L., Philipsen, M. P., Wu, H., & Moeslund, T. B.",
            "venue": "VAM-HRI",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Agents",
                    "parent": "Environmental"
                },
                {
                    "name": "Simulated Objects",
                    "parent": "Environmental"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 117,
            "slug": "paper_117",
            "title": "Excuse me! Perception of Abrupt Direction Changes Using Body Cues and Paths on Mixed Reality Avatars",
            "abstract": "We evaluate abrupt turn signalling using Mixed Reality avatars with two methods: A method where the avatar signals using its body, and a method where a path is rendered on the floor. Results indicate that study participants prefer the body method but that the path method is more accurate when the path is longer.",
            "link": "https://dl.acm.org/doi/abs/10.1145/3173386.3177040?casa_token=94rIzzXY0tkAAAAA:gKldkL7Pu26aKslr3rXkVc_xbZvM_XcJCNk6PRqf2WW6b7kQ3PSrLGd_AHENMuwzdlYA4j66iChs",
            "authors": "Katzakis, N., & Steinicke, F.",
            "venue": "VAM-HRI",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Agents",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 118,
            "slug": "paper_118",
            "title": "Improving Human-Robot Handover Research by Mixed-Reality Techniques",
            "abstract": "Virtual Reality (VR) and Mixed Reality (MR) have already been incorporated into robotic systems for various applications in the domain of Human-Robot Interaction (HRI), such as telerobotics or as a tool for robotics software development and debugging. In this context, MR techniques allow to enrich the user\u2019s experience so that he or she is able to interact with the robot more efficiently. Moreover, robots can be simulated in an immersive VR environment to conduct highly controlled experiments. Close interaction scenarios like handovers, conducted either in the physical or virtual world, have their own inherent limits, e.g., with respect to safety as well as a fixed body structure of the robot in the physical world and degree of realism or level of detail in virtual environments. Therefore, we explore several MR techniques to overcome these limitations. MR devices, such as the Microsoft HoloLens, allow to augment real environments with visualized sensor data as well as to simulate robotic parts like virtual robot heads or arms attached to a physical robot. This enables HRI experiments that were difficult or impossible to conduct before.",
            "link": "https://core.ac.uk/download/pdf/211837036.pdf",
            "authors": "zu Borgsen, S. M., Renner, P., Lier, F., Pfeiffer, T., & Wachsmuth, S.",
            "venue": "VAM-HRI",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Agents",
                    "parent": "Environmental"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                },
                {
                    "name": "Virtual Alterations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Morphological",
                    "parent": "Virtual Alterations"
                },
                {
                    "name": "Body Extensions",
                    "parent": "Morphological"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Robot Inherent Spatial Regions",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 119,
            "slug": "paper_119",
            "title": "Teleoperating a Humanoid Robot with Virtual Reality",
            "abstract": "In this paper, we describe our ongoing work to develop cooperative control of NASA's R5 Valkyrie humanoid robot for performing dexterous manipulation tasks inside gloveboxes commonly found in many nuclear facilities. These tasks can be physically demanding and provide some element of risk to the operator when done by a person in situ. For example, if a glove is ruptured, the operator could be exposed to radioactive material. In many cases, the operator has low visibility and is unable to reach the entire task space, requiring the use of additional tools located inside the glovebox. Such tasks include cleaning particulate from inside the glovebox via sweeping or vacuuming, separating a specific amount of a compound to be weighed on a scale, or grasping and manipulating objects inside the glovebox. There is potential to move the operator to a nearby, safe location and instead place a humanoid robot in the potentially hazardous environment. However, teleoperating a humanoid robot to perform dexterous tasks at a comparable level to a human hand remains a difficult problem. Previous work for controlling humanoid robots often involves one or more operators using a standard 2D display with a mouse and keyboard as controllers. Successful interfaces use sensor fusion to provide information to the operator for increased situation awareness, but these designs have limitations. Gaining proper situation awareness by visualizing 3D information on a 2D screen requires time and increases the cognitive load on the user. Instead, if the operator is able to visualize and control the robot properly in three dimensions, it can increase situation and task awareness, reduce task time, reduce the chance of mistakes, and increase the likelihood of overall task success. We propose a two-part system that combines an HTC Vive virtual reality headset with either the Vive handheld controllers, or the Manus VR wearable gloves as the primary control. The operator wears the headset in a remote location and can visualize a reconstruction of the glovebox, created live by sensor scans from the robot and with sensors located inside the glovebox for a perspective traditionally unavailable to operators. By using the controllers or gloves to control the humanoid robots hands directly, they can plan actions in the virtual reconstruction. When the operator is satisfied with the plan, the actions are sent to the real robot. To test this system we have created a mockup of a glovebox that is accessible by Valkyrie, as well as several tasks that are a subsample of the tasks that might be required when working in a real glovebox.",
            "link": "https://www.osti.gov/biblio/22977740",
            "authors": "Allspaw, J., Roche, J., Lemiesz, N., Yannuzzi, M., & Yanco, H. A.",
            "venue": "VAM-HRI",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 120,
            "slug": "paper_120",
            "title": "360 VR Based Robot Teleoperation Interface for Virtual Tour",
            "abstract": "We propose a novel mobile robot teleoperation interface that demonstrates the applicability of robot-aided remote telepresence system with a virtual reality (VR) device in a virtual tour scenario. To improve the realism and provide an intuitive replica of the remote environment at the user interface, we implemented a system that automatically moves a mobile robot (viewpoint) while displaying a 360-degree live video streamed from the robot on a virtual reality gear (Oculus Rift). Once the user chooses a destination location among a given set of options, the robot generates a route to the destination based on a shortest path graph, and travels along the route using a wireless signal tracking method that depends on measuring the Direction of Arrival (DOA) of radio signal. In this paper, we present an overview of the system and the architecture, highlight the current progress in the system development, and discuss the implementation aspects of the above system.",
            "link": "https://www.researchgate.net/profile/Ramviyas-Parasuraman/publication/324896965_360_VR_Based_Robot_Teleoperation_Interface_for_Virtual_Tour/links/5aeb3c5ba6fdcc8508b6c4f3/360-VR-Based-Robot-Teleoperation-Interface-for-Virtual-Tour.pdf",
            "authors": "Oh, Y., Parasuraman, R., McGraw, T., & Min, B. C.",
            "venue": "VAM-HRI",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Command Options & Validity",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 121,
            "slug": "paper_121",
            "title": "Robot Learning from Human Demonstration in Virtual Reality",
            "abstract": "To best leverage the adoption of robotics systems, their capability to learn and adapt to novel situations is essential. We propose an immersive Virtual Reality (VR) environment in which a simulated robot is trained through human demonstration; the VR environment is leveraged to communicate to the robot the intent of a task, so that the robot can then replicate it in a broader set of initial conditions. The skills acquired by the simulated robot are then transferred to the real robot in the physical world. The approach is intuitive, safe for the human demonstrator and reduces the downtime of the robot during the training process. We demonstrate this proposed framework for a pick and pass operation.",
            "link": "https://www.researchgate.net/profile/Francesca_Stramandinoli/publication/324825053_Robot_Learning_from_Human_Demonstration_in_Virtual_Reality/links/5ae4ba8f458515760ac07d93/Robot-Learning-from-Human-Demonstration-in-Virtual-Reality.pdf",
            "authors": "Stramandinoli, F., Lore, K.G., Peters, J.R., O\u2019Neill, P.C., Nair, B.M., Varma, R., Ryde, J.C., Miller, J.T. and Reddy, K.K.",
            "venue": "VAM-HRI",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 122,
            "slug": "paper_122",
            "title": "Alternative Modes of Interaction in Proximal Human-in-the-Loop Operation of Robots",
            "abstract": "Ambiguity and noise in natural language instructions create a significant barrier towards adopting autonomous systems into safety critical workflows involving humans and machines. In this paper, we propose to build on recent advances in electrophysiological monitoring methods and augmented reality technologies, to develop alternative modes of communication between humans and robots involved in large-scale proximal collaborative tasks. We will first introduce augmented reality techniques for projecting a robot's intentions to its human teammate, who can interact with these cues to engage in real-time collaborative plan execution with the robot. We will then look at how electroencephalographic (EEG) feedback can be used to monitor human response to both discrete events, as well as longer term affective states while execution of a plan. These signals can be used by a learning agent, a.k.a an affective robot, to modify its policy. We will present an end-to-end system capable of demonstrating these modalities of interaction. We hope that the proposed system will inspire research in augmenting human-robot interactions with alternative forms of communications in the interests of safety, productivity, and fluency of teaming, particularly in engineered settings such as the factory floor or the assembly line in the manufacturing industry where the use of such wearables can be enforced.",
            "link": "https://arxiv.org/abs/1703.08930",
            "authors": "Chakraborti, T., Sreedharan, S., Kulkarni, A., & Kambhampati, S.",
            "venue": "VAM-HRI",
            "sessions": "",
            "year": 2018,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Robot Inherent Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Appearances",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 123,
            "slug": "paper_123",
            "title": "Human Intention estimation based on Hidden Markov Model Motion validation for Safe Flexible Robotized Warehouses",
            "abstract": "With the substantial growth of logistics businesses the need for larger warehouses and their automation arises, thus using robots as assistants to human workers is becoming a priority. In order to operate efficiently and safely, robot assistants or the supervising system should recognize human intentions in real-time. Theory of Mind (ToM) is an intuitive human conception of other humans\u2019 mental state, i.e., beliefs and desires, and how they cause behavior. In this paper we propose a ToM based human intention estimation algorithm for flexible robotized warehouses. We observe human\u2019s, i.e., worker\u2019s motion and validate it with respect to the goal locations using generalized Voronoi diagram based path planning. These observations are then processed by the proposed hidden Markov model framework which estimates worker intentions in an online manner, capable of handling changing environments. To test the proposed intention estimation we ran experiments in a real-world laboratory warehouse with a worker wearing Microsoft Hololens augmented reality glasses. Furthermore, in order to demonstrate the scalability of the approach to larger warehouses, we propose to use virtual reality digital warehouse twins in order to realistically simulate worker behavior. We conducted intention estimation experiments in the larger warehouse digital twin with up to 24 running robots. We demonstrate that the proposed framework estimates warehouse worker intentions precisely and in the end we discuss the experimental results.",
            "link": "https://www.sciencedirect.com/science/article/pii/S0736584518302965?casa_token=ZFI0yretd94AAAAA:hxav5Ot8TRVFKU_2d-TuY22zB0ifmV2rtw0o4XTZI0HGArB-q7uXEaZtJmdIwA6EuKITVDd-OA",
            "authors": "Petkovi\u0107, T., Puljiz, D., Markovi\u0107, I., & Hein, B.",
            "venue": "ROBOT",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 124,
            "slug": "paper_124",
            "title": "InvisibleRobot: Facilitating Robot Manipulation Through Diminished Reality",
            "abstract": "When controlling robots, users often face the issue of an operating area that is occluded by an element in the environment or the robot's body. To gain an unobstructed view of the scene, users have to either adjust the pose of the robot or their own viewpoint. This presents a problem, especially for users who rely on assistive robots as they can't easily change their point of view. We introduce InvisibleRobot, a diminished reality-based approach that overlays background information onto the robot in the user's view through an Optical See-Through Head-Mounted Display. We consider two visualization modes for InvisibleRobot: removing the robot body from the user's view entirely, or removing the interior of the robot while maintaining its outline. In a preliminary user study, we compare InvisibleRobot with traditional robot manipulation under different occlusion conditions. Our results suggest that InvisibleRobot can support manipulation in occluded conditions and could be an efficient method to simplify control in assistive robotics.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8951935",
            "authors": "Plopski, A., Taylor, A. V., Carter, E. J., & Admoni, H.",
            "venue": "ISMAR",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Alterations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Morphological",
                    "parent": "Virtual Alterations"
                },
                {
                    "name": "Body Diminishments",
                    "parent": "Morphological"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 125,
            "slug": "paper_125",
            "title": "Augmented reality for human-robot teaming in field environments",
            "abstract": "For teams of humans and mobile robots to work together, several challenges must be overcome, including understanding each others\u2019 position, merging map information, sharing recognition of salient features of the environment, and establishing contextually-grounded communication. These challenges are further compounded for teams operating in field environments, which are unconstrained, uninstrumented, and unknown. While most modern studies that use augmented reality (AR) in human-robot teaming side-step these challenges by focusing on problems addressable in instrumented environments, we argue that current AR technology combined with novel approaches can enable successful teaming in such challenging, real-world settings. To support this, we present a set of prototypes that combine AR with an intelligent, autonomous robot to enable better human-robot teaming in field environments.",
            "link": "https://link.springer.com/chapter/10.1007/978-3-030-21565-1_6",
            "authors": "Reardon, C., Lee, K., Rogers, J. G., & Fink, J.",
            "venue": "HCII ",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Labels",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 126,
            "slug": "paper_126",
            "title": "Measuring Performance in Robotic Teleoperation Tasks with Virtual Reality Headgear",
            "abstract": "With the current rise of a new wave of Virtual Reality technologies, wider range of their applications are to be expected. Head Mounted Displays (HMDs) are starting to find use in some of human-machine interfaces for robots. We seek to measure and evaluate the impact that these devices can have on performance in standard robotic teleoperation tasks such as driving, observation and manipulation. In a study conducted with two real robots and standardized testing environment, including a total of 17 operators and 5 different tasks, an interface consisting of HMD and stereo vision module is compared to a more traditional one.",
            "link": "https://link.springer.com/chapter/10.1007/978-3-030-13273-6_39",
            "authors": "Macia\u015b, Mateusz, Adam D\u0105browski, Jan Fra\u015b, Micha\u0142 Karczewski, S\u0142awomir Puchalski, Sebastian Tabaka, and Piotr Jaroszek.",
            "venue": "Automation",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 127,
            "slug": "paper_127",
            "title": "Learning Virtual Borders thorugh Semantic Scene Understanding and Augmented reality",
            "abstract": "Virtual borders are an opportunity to allow users the interactive restriction of their mobile robots' workspaces, e.g. to avoid navigation errors or to exclude certain areas from working. Currently, works in this field have focused on human-robot interaction (HRI) methods to restrict the workspace. However, recent trends towards smart environments and the tremendous progress in semantic scene understanding give new opportunities to enhance the HRI-based methods. Therefore, we propose a novel learning and support system (LSS) to support users during teaching of virtual borders. Our LSS learns from user interactions employing methods from visual scene understanding and supports users through recommendations for interactions. The bidirectional interaction between the user and system is realized using augmented reality. A validation of the approach shows that the LSS robustly recognizes a limited set of typical areas for virtual borders based on previous user interactions (F1 - Score= 91.5%) while preserving the high accuracy of standard HRI-based methods with a median of Mdn= 84.6%. Moreover, this approach allows the reduction of the interaction time to a constant mean value of M = 2 seconds making it independent of the border length. This avoids a linear interaction time of standard HRI-based methods.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8967576?casa_token=hxamOUau6MUAAAAA:tt5thPEkIUEcghGEnC5-jLRD_3cRDKh04bY6MiZtuMj5WcoYVLjha-W-8CNyYonijKV4lnkA",
            "authors": "Sprute, D., Viertel, P., T\u00f6nnies, K., & K\u00f6nig, M.",
            "venue": "IROS",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "User-Defined Spatial Regions",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 128,
            "slug": "paper_128",
            "title": "On-Road Evaluation of Autonomous Driving Training",
            "abstract": "Driver interaction with increasingly automated vehicles requires prior knowledge of system capabilities, operational know-how to use novel car equipment and responsiveness to unpredictable situations. With the purpose of getting drivers ready for autonomous driving, in a between-subject study sixty inexperienced participants were trained with an on-board video tutorial, an Augmented Reality (AR) program and a Virtual Reality (VR) simulator. To evaluate the transfer of training to real driving scenarios, a test drive on public roads was conducted implementing, for the first time in these conditions, the Wizard of Oz (WoZ) protocol. Results suggest that VR and AR training can foster knowledge acquisition and improve reaction time performance in take-over requests. Moreover, participants' behavior during the test drive highlights the ecological validity of the experiment thanks to the effective implementation of the WoZ methodology.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8673277?casa_token=ILC2e9v36kMAAAAA:O_-v1Kn5zfzfD7JWy99uKEjpX7bLBtoZ_L-Hf5uifQxERdwOcrzNo4qCXSgEEuoPIeA_JYYa",
            "authors": "Sportillo, D., Paljic, A., & Ojeda, L.",
            "venue": "HRI",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 129,
            "slug": "paper_129",
            "title": "Projection-based Augmented Reality Interface for Robot Grasping Tasks",
            "abstract": "This paper presents an augmented reality (AR) interface for robot programming of pick-and-place tasks as well as assembly operations. The aim of the AR interface is to increase the intuitiveness and ease of robot programming. Marker tracking is used to automate sensor registration for easier workspace setup. Object recognition is employed to transform robot programming from an absolute coordinate system to an object-based system. This transformation provides flexibility to robot programming as the AR interface can be applied to workspaces that are configured differently to accomplish the same task. A spatially immersive display method is adopted to provide a projection-based direct overlay virtual workspace, so as to give users a better frame of reference in relation to the real workspace.",
            "link": "https://dl.acm.org/doi/abs/10.1145/3351180.3351204?casa_token=OfHWuSgFrhQAAAAA:n-l3WoBzrcHJjm4_9ImAR0xRSUdE4y_c0P-0b1yIWHLIjwzGHPH2lnNJfO-NMAbmLz7LhaAK0uDj",
            "authors": "Gong, L. L., Ong, S. K., & Nee, A. Y. C.",
            "venue": "ICRCA",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 130,
            "slug": "paper_130",
            "title": "End-User Robot Programming Using Mixed Reality",
            "abstract": "Mixed Reality (MR) is a promising interface for robot programming because it can project an immersive 3D visualization of a robot's intended movement onto the real world. MR can also support hand gestures, which provide an intuitive way for users to construct and modify robot motions. We present a Mixed Reality Head-Mounted Display (MRHMD) interface that enables end-users to easily create and edit robot motions using waypoints. We describe a user study where 20 participants were asked to program a robot arm using 2D and MR interfaces to perform two pick-and-place tasks. In the primitive task, participants created typical pickand-place programs. In the adapted task, participants adapted their primitive programs to address a more complex pickand-place scenario, which included obstacles and conditional reasoning. Compared to the 2D interface, a higher number of users were able to complete both tasks in significantly less time, and reported experiencing lower cognitive workload, higher usability, and higher naturalness with the MR-HMD interface.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8793988?casa_token=4Wk57onIetsAAAAA:3Xj-Ub9Uwe2QO4VWETHnrk-X6_qn72gI0S3GwoNxowdkwHf_TNpUuh4rxisAYlqY7pXnegxr",
            "authors": "Gadre, S. Y., Rosen, E., Chien, G., Phillips, E., Tellex, S., & Konidaris, G.",
            "venue": "ICRA",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Labels",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 131,
            "slug": "paper_131",
            "title": "V.Ra: An In-Situ Visual Authoring system for Robot-IoT Task Planning with Augmented Reality",
            "abstract": "We present V.Ra, a visual and spatial programming system for robot-IoT task authoring. In V.Ra, programmable mobile robots serve as binding agents to link the stationary IoTs and perform collaborative tasks. We establish an ecosystem that coherently connects the three key elements of robot task planning , the human, robot and IoT, with one single mobile AR device. Users can perform task authoring with the Augmented Reality (AR) handheld interface, then placing the AR device onto the mobile robot directly transfers the task plan in a what-you-do-is-what-robot-does (WYDWRD) manner. The mobile device mediates the interactions between the user, robot, and the IoT oriented tasks, and guides the path planning execution with the embedded simultaneous localization and mapping (SLAM) capability. We demonstrate that V.Ra enables instant, robust and intuitive room-scale navigatory and interactive task authoring through various use cases and preliminary studies.",
            "link": "https://dl.acm.org/doi/abs/10.1145/3322276.3322278",
            "authors": "Cao, Y., Xu, Z., Li, F., Zhong, W., Huo, K., & Ramani, K.",
            "venue": "DIS",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Labels",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Command Options & Validity",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 132,
            "slug": "paper_132",
            "title": "Augmented Reality for Interactive Robot Control",
            "abstract": "Robots are widely used to support mission-critical, high-risk and complex operations. Human supervision and remote robot control are often required to operate robots in unpredictable and changing scenarios. Often, robots are controlled remotely by technicians via joystick interfaces which require training and experience to operate. To improve robot usage and practicality, we propose using augmented reality (AR) to create a more intuitive, less training-intensive means of controlling robots than traditional joystick control. AR is a creative platform for developing robot control systems, because AR combines the real world (the environment around the user, the physical robot, etc.) with the digital world (holograms, digital displays, etc.); it can even interpret physical gestures, such as pinching two fingers. In this research, a Microsoft Hololens headset is used to create an AR environment to control a Yaskawa Motoman SIA5D robot. The control process begins with the user placing an interactable holographic robot in 3D space. The user can then select between two control methods: manual control and automatic control. In manual control, the user can move the end effector of the holographic robot and the physical robot will respond immediately. In automatic control, the user can move the end effector of the holographic robot to a desired location, view a holographic preview of the motion, and select execute if the motion plan is satisfactory. In this preview mode, the user is able to preview both the motion of the robot and the torques experienced by the joints of the manipulator. This gives the user additional feedback on the planned motion. In this project we succeeded in creating an AR control system that makes controlling a robotic manipulator intuitive and effective.",
            "link": "https://link.springer.com/chapter/10.1007/978-3-030-12243-0_2",
            "authors": "Manring, L., Pederson, J., Potts, D., Boardman, B., Mascarenas, D., Harden, T., & Cattaneo, A.",
            "venue": "CPSEMS",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 133,
            "slug": "paper_133",
            "title": "GhostAR: A Time-space Editor for Embodied Authoring of Human-Robot Collaborative Task with Augmented Reality",
            "abstract": "We present GhostAR, a time-space editor for authoring and acting Human-Robot-Collaborative (HRC) tasks in-situ. Our system adopts an embodied authoring approach in Augmented Reality (AR), for spatially editing the actions and programming the robots through demonstrative role-playing. We propose a novel HRC workflow that externalizes user's authoring as demonstrative and editable AR ghost, allowing for spatially situated visual referencing, realistic animated simulation, and collaborative action guidance. We develop a dynamic time warping (DTW) based collaboration model which takes the real-time captured motion as inputs, maps it to the previously authored human actions, and outputs the corresponding robot actions to achieve adaptive collaboration. We emphasize an in-situ authoring and rapid iterations of joint plans without an offline training process. Further, we demonstrate and evaluate the effectiveness of our workflow through HRC use cases and a three-session user study.",
            "link": "https://dl.acm.org/doi/abs/10.1145/3332165.3347902",
            "authors": "Cao, Y., Wang, T., Qian, X., Rao, P. S., Wadhawan, M., Huo, K., & Ramani, K.",
            "venue": "UIST",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 134,
            "slug": "paper_134",
            "title": "Interactive Robots Control Using Mixed Reality",
            "abstract": "The paper presents a mixed reality-based approach for interactive control of robotic manipulators and mobile platforms. In particular, we designed an interactive and understandable interface for human-robot interaction. The interface provides tools for robot path programming and visualizes it. Path visualization helps workers understand robot behavior, it is important for safety human-robot interaction. The paper presents an architecture of that system and the implementation for an industrial robot KUKA iiwa and mobile robot platform Plato. The main issue of a multi-platform system is related to the synchronization of coordinate frames for all elements. To deal with this problem we realized 3 setting options: manually, by a camera with markers, point clouds processing. We implemented our interface on Microsoft HoloLens and evaluated it on users.",
            "link": "https://www.sciencedirect.com/science/article/pii/S2405896319312613",
            "authors": "Ostanin, M., Yagfarov, R., & Klimchik, A.",
            "venue": "IFAC",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 135,
            "slug": "paper_135",
            "title": "PATI: a projection-based augmented table-top interface for robot programming",
            "abstract": "As robots begin to provide daily assistance to individuals in human environments, their end-users, who do not necessarily have substantial technical training or backgrounds in robotics or programming, will ultimately need to program and \"re-task\" their robots to perform a variety of custom tasks. In this work, we present PATI---a Projection-based Augmented Table-top Interface for robot programming---through which users are able to use simple, common gestures (e.g., pinch gestures) and tools (e.g., shape tools) to specify table-top manipulation tasks (e.g., pick-and-place) for a robot manipulator. PATI allows users to interact with the environment directly when providing task specifications; for example, users can utilize gestures and tools to annotate the environment with task-relevant information, such as specifying target landmarks and selecting objects of interest. We conducted a user study to compare PATI with a state-of-the-art, standard industrial method for end-user robot programming. Our results show that participants needed significantly less training time before they felt confident in using our system than they did for the industrial method. Moreover, participants were able to program a robot manipulator to complete a pick-and-place task significantly faster with PATI. This work indicates a new direction for end-user robot programming.",
            "link": "https://dl.acm.org/doi/abs/10.1145/3301275.3302326?casa_token=wuqdymxAAeoAAAAA:h4jxuCltegperbP3X_N32gE6qmo2hAWuiS1_plzQIuOQ-7rgW-AtZdJcvlXDy4PRK9PlGgR4vCzK",
            "authors": "Gao, Y., & Huang, C. M.",
            "venue": "IUI",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "User-Defined Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 136,
            "slug": "paper_136",
            "title": "Evaluation of Proxemics in Dynamic Interaction with a Mixed Reality Avatar Robot",
            "abstract": "We proposed a method that using arm swing of a mixed-reality avatar upon the robot platform to present the moving speed of it. Then a series of studies were designed and performed to investigate the effectiveness of this method and the proxemics when human have dynamic interaction with the mixed-reality avatar robot (Figure 3). Our findings suggest that robot moving speed has a significant effect on the proxemics between human and mixed-reality avatar robot. While attaching an avatar upon the robot did not show a significant influence on the proxemics in dynamic interaction comparing to the baseline situation (robot only). But participants gave positive comments on this method which helped improving perception and prediction on the robot state and its potential applications like marking tiny ground robot. Our work provided references and guidelines for external expression on the robot state with mixed reality for the further research.",
            "link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C34&q=Jingxin+Zhang%2C+Omar+Janeh%2C+Nikolaos+Katzakis%2C+Dennis+Krupke%2C+Frank+Steinicke%09Evaluation+of+Proxemics+in+Dynamic+Interaction+with+a+Mixed+Reality+Avatar+Robot&btnG=",
            "authors": "Zhang, J., Janeh, O., Katzakis, N., Krupke, D., & Steinicke, F.",
            "venue": "ICAT-EGVE",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Agents",
                    "parent": "Environmental"
                },
                {
                    "name": "Virtual Alterations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Morphological",
                    "parent": "Virtual Alterations"
                },
                {
                    "name": "Form Transformations",
                    "parent": "Morphological"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 137,
            "slug": "paper_137",
            "title": "Augmented Robotics for Learners: A Case Study on Optics",
            "abstract": "In recent years, robots have been surfing on a trendy wave as standard devices for teaching programming. The tangibility of robotics platforms allows for collaborative and interactive learning. Moreover, with these robot platforms, we also observe the occurrence of a shift of visual attention from the screen (on which the programming is done) to the physical environments (i.e. the robot). In this paper, we describe an experiment aiming at studying the effect of using augmented reality (AR) representations of sensor data in a robotic learning activity. We designed an AR system able to display in real-time the data of the Infra-Red sensors of the Thymio robot. In order to evaluate the impact of AR on the learner's understanding on how these sensors worked, we designed a pedagogical lesson that can run with or without the AR rendering. Two different age groups of students participated in this between-subject experiment, counting a total of 74 children. The tests were the same for the experimental (AR) and control group (no AR). The exercises differed only through the use of AR. Our results show that AR was worth being used for younger groups dealing with difficult concepts. We discuss our findings and propose future works to establish guidelines for designing AR robotic learning sessions.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8956363?casa_token=SiTo09Cs1VoAAAAA:BzO0cgXVl-jLIZZDh_E5GZ75gPa-mSIIeOPYrJ8kB2YoWlNjGpZhONJz9Kz4Xrl_GigTzf6Y",
            "authors": "Johal, W., Robu, O., Dame, A., Magnenat, S., & Mondada, F.",
            "venue": "ROMAN",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor Numerical Readings",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 138,
            "slug": "paper_138",
            "title": "Negotiation-based Human-Robot Collaboration via Augmented Reality",
            "abstract": "Effective human-robot collaboration (HRC) requires extensive communication among the human and robot teammates, because their actions can potentially produce conflicts, synergies, or both. We develop a novel augmented reality (AR) interface to bridge the communication gap between human and robot teammates. Building on our AR interface, we develop an AR-mediated, negotiation-based (ARN) framework for HRC. We have conducted experiments both in simulation and on real robots in an office environment, where multiple mobile robots work on delivery tasks. The robots could not complete the tasks on their own, but sometimes need help from their human teammate, rendering human-robot collaboration necessary. Results suggest that ARN significantly reduced the human-robot team's task completion time compared to a non-AR baseline approach.",
            "link": "https://arxiv.org/abs/1909.11227",
            "authors": "Chandan, K., Kudalkar, V., Li, X., & Zhang, S.",
            "venue": "AI-HRI",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Labels",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Command Options & Validity",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 139,
            "slug": "paper_139",
            "title": "An Augmented Reality Interface for Human-Robot Interaction in Unconstrained Environments",
            "abstract": "As robots start to become ubiquitous in the personal workspace, it is necessary to have simple and intuitive interfaces to interact with them. In this paper, we propose an augmented reality (AR) interface for human-robot interaction (HRI) in a shared working environment. By fusing marker-based and markerless AR technologies, a mobile AR interface is created that enables a smartphone to detect planar surfaces and localize a manipulator robot in its working environment while obviating the need for a controlled or constrained environment. The AR interface and robot manipulator are integrated to render a system that enables users to perform pick-and-place task effortlessly. Specifically, a smartphone-based AR application is developed that allows a user to select any location within the robot's workspace by merely touching on the smartphone screen. Virtual objects, rendered at user-selected locations, are used to determine the pick and place locations of objects in the real world. The virtual object's start and end points, originally specified in the smartphone camera coordinate frame, are transformed into the robot coordinate frame for the robot manipulator to autonomously perform the assigned task. A user study is conducted with participants to evaluate the system performance and user experience. The results show that the proposed AR interface is user-friendly and intuitive to operate the robot, and it allows users to communicate their intentions through the virtual object easily.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8967973?casa_token=fsxKZkqn11AAAAAA:T2nfvpk9TXtpH0izIMqV-NuoGrVFdraPhkfiEPqC0mnJ1Je-v9Fq1cFf1JCCSV5xx-DIBCco",
            "authors": "Chacko, S. M., & Kapila, V.",
            "venue": "IROS",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Robot Inherent Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 140,
            "slug": "paper_140",
            "title": "Towards Explainable Shared Control using Augmented Reality",
            "abstract": "Shared control plays a pivotal role in establishing effective human-robot interactions. Traditional control-sharing methods strive to complement a human's capabilities at safely completing a task, and thereby rely on users forming a mental model of the expected robot behaviour. However, these methods can often bewilder or frustrate users whenever their actions do not elicit the intended system response, forming a misalignment between the respective internal models of the robot and human. To resolve this model misalignment, we introduce Explainable Shared Control as a paradigm in which assistance and information feedback are jointly considered. Augmented reality is presented as an integral component of this paradigm, by visually unveiling the robot's inner workings to human operators. Explainable Shared Control is instantiated and tested for assistive navigation in a setup involving a robotic wheelchair and a Microsoft HoloLens with add-on eye tracking. Experimental results indicate that the introduced paradigm facilitates transparent assistance by improving recovery times from adverse events associated with model misalignment.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8968117?casa_token=J4PidWJw5NIAAAAA:cNisDB_DKGezyhY1V8o4xvQA1RuP3kh04cZfj5xtWwEWUYwoKztcWU3q-uMWNyRyY8FWPt0D",
            "authors": "Zolotas, M., & Demiris, Y.",
            "venue": "IROS",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 141,
            "slug": "paper_141",
            "title": "Enabling Intuitive Human-Robot Teaming Using Augmented Reality and Gesture Control",
            "abstract": "Human-robot teaming offers great potential because of the opportunities to combine strengths of heterogeneous agents. However, one of the critical challenges in realizing an effective human-robot team is efficient information exchange - both from the human to the robot as well as from the robot to the human. In this work, we present and analyze an augmented reality-enabled, gesture-based system that supports intuitive human-robot teaming through improved information exchange. Our proposed system requires no external instrumentation aside from human-wearable devices and shows promise of real-world applicability for service-oriented missions. Additionally, we present preliminary results from a pilot study with human participants, and highlight lessons learned and open research questions that may help direct future development, fielding, and experimentation of autonomous HRI systems.",
            "link": "https://arxiv.org/abs/1909.06415",
            "authors": "Gregory, J. M., Reardon, C., Lee, K., White, G., Ng, K., & Sims, C.",
            "venue": "AI-HRI",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 142,
            "slug": "paper_142",
            "title": "Augmented Reality for Supervising Multirobot System in Agricultural Field Operation ",
            "abstract": "Agriculture is shifting from farmers manually operating machines to monitoring autonomous machines. Thus, the task of a farmer is fleet management and taking care of safe operation. Situational awareness during the operation is important. Augmented reality (AR) is a powerful tool for visualizing information in real-time. To demonstrate the use of AR in agricultural fleet management, in this paper we present a novel AR system to help the farmer supervise the operation of two autonomous agricultural machines. The paper discusses the requirements for AR application, and we present the architecture of the system and the results of a demonstration carried out in a test field.",
            "link": "https://www.sciencedirect.com/science/article/pii/S2405896319324899",
            "authors": "Huuskonen, J., & Oksanen, T.",
            "venue": "AGRICONTROL",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 143,
            "slug": "paper_143",
            "title": "Transparent Robot Behavior Using Augmented Relaity in Close Human-Robot Interaction",
            "abstract": "Most robots consistently repeat their motion with- out changes in a precise and consistent manner. But nowadays there are also robots able to dynamically change their motion and plan according to the people and environment that surround them. Furthermore, they are able to interact with humans and cooperate with them. With no information about the robot targets and intentions, the user feels uncomfortable even with a safe robot. In close human-robot collaboration, it is very important to make the user able to understand the robot intentions in a quick and intuitive way. In this work we have developed a system to use augmented reality to project directly into the workspace useful information. The robot intuitively shows its planned motion and task state. The AR module interacts with a vision system in order to display the changes in the workspace in a dynamic way. The representation of information about possible collisions and changes of plan allows the human to have a more comfortable and efficient interaction with the robot. The system is evaluated in different setups.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8956296?casa_token=27_JtFB27NoAAAAA:E0unw-nGxCHlIUdq70gcRFt9-nTN9KLnSMAuxoHUsz_KVyWeLmsjjt4WNBgnoeyrAR4ogTPo",
            "authors": "Bolano, G., Juelg, C., Roennau, A., & Dillmann, R.",
            "venue": "ROMAN",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Labels",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 144,
            "slug": "paper_144",
            "title": "Debugging Quadrocopter Trajectories in Mixed Reality",
            "abstract": "Debugging and monitoring robotic applications is a very intricate and error-prone task. To this end, we propose a mixed-reality approach to facilitate this process along a concrete scenario. We connected the Microsoft HoloLens smart glass to the Robot Operating System (ROS), which is used to control robots, and visualize arbitrary flight data of a quadrocopter. Hereby, we display holograms correctly in the real world based on a conversion of the internal tracking coordinates into coordinates provided by a motion capturing system. Moreover, we describe the synchronization process of the internal tracking with the motion capturing. Altogether, the combination of the HoloLens and the external tracking system shows promising preliminary results. Moreover, our approach can be extended to directly manipulate source code through its mixed-reality visualization and offers new interaction methods to debug and develop robotic applications.",
            "link": "https://link.springer.com/chapter/10.1007/978-3-030-25999-0_4",
            "authors": "Hoppenstedt, B., Witte, T., Ruof, J., Kammerer, K., Tichy, M., Reichert, M., & Pryss, R.",
            "venue": "AVR",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 145,
            "slug": "paper_145",
            "title": "Augmented-Reality-Based Visualization of Navigation Data of Mobile Robots on the Microsoft Hololens -- Possibilities and Limitations",
            "abstract": "The demand for mobile robots has rapidly increased in recent years due to the flexibility and high variety of application fields comparing to static robots. To deal with complex tasks such as navigation, they work with high amounts of different sensor data making it difficult to operate with for non-experts. To enhance user understanding and human robot interaction, we propose an approach to visualize the navigation stack within a cutting edge 3D Augmented Reality device -the Microsoft Hololens. Therefore, relevant navigation stack data including laser scan, environment map and path planing data are visualized in 3D within the head mounted device. Based on that prototype, we evaluate the Hololens in terms of computational capabilities and limitations for dealing with huge amount of real-time data. Results show that the Hololens is capable of a proper visualization of huge amounts of sensor data. We demonstrate a proper visualization of navigation stack data in 3D within the Hololens. However, there are limitations when transferring and displaying different kinds of data simultaneously.",
            "link": "https://ieeexplore.ieee.org/abstract/document/9095836?casa_token=c0ErU3_u_qUAAAAA:Yx7u0kvSMHlrheESliKILKEzenMNcBwJUxWN7E4Ztf0rzGAQyJ0Zy1mAv9IgdnJRzJSQgeP5",
            "authors": "K\u00e4stner, L., & Lambrecht, J.",
            "venue": "CISRAM",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 146,
            "slug": "paper_146",
            "title": "A Study on Different User Interfaces for Teaching Virtual Borders to Mobile Robots",
            "abstract": "Human-aware robot navigation is an essential aspect to increase the acceptance of mobile service robots in human-centered environments, e.g. home environments. Robots need to navigate in a human-acceptable way according to the users\u2019 conventions, presence and needs. In order to address the users\u2019 needs, we employ virtual borders, which are non-physical borders and respected by the robots while working, to effectively restrict the workspace of a mobile robot and change its navigational behavior. To this end, we consider different user interfaces, i.e. visual markers, a laser pointer, a graphical user interface and a RGB-D Google Tango tablet with augmented reality application, to allow non-expert users the flexible and interactive definition of virtual borders. These user interfaces were evaluated with respect to their correctness, flexibility, accuracy, teaching effort and user experience. Experimental results show that the RGB-D Google Tango tablet as user interface yields the best overall results compared to the other user interfaces. Apart from a low teaching effort and high flexibility and accuracy, it features the highest user ratings acquired from a comprehensive user study with 25 participants for intuitiveness, comfort, learnability and its feedback system.",
            "link": "https://link.springer.com/article/10.1007/s12369-018-0506-3",
            "authors": "Sprute, D., T\u00f6nnies, K., & K\u00f6nig, M.",
            "venue": "International Journal of Social Robotics",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "User-Defined Spatial Regions",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 147,
            "slug": "paper_147",
            "title": "Robot Teleoperation with Augmented Reality Virtual Surrogates",
            "abstract": "Teleoperation remains a dominant control paradigm for human interaction with robotic systems. However, teleoperation can be quite challenging, especially for novice users. Even experienced users may face difficulties or inefficiencies when operating a robot with unfamiliar and/or complex dynamics, such as industrial manipulators or aerial robots, as teleoperation forces users to focus on low-level aspects of robot control, rather than higher level goals regarding task completion, data analysis, and problem solving. We explore how advances in augmented reality (AR) may enable the design of novel teleoperation interfaces that increase operation effectiveness, support the user in conducting concurrent work, and decrease stress. Our key insight is that AR may be used in conjunction with prior work on predictive graphical interfaces such that a teleoperator controls a virtual robot surrogate, rather than directly operating the robot itself, providing the user with foresight regarding where the physical robot will end up and how it will get there. We present the design of two AR interfaces using such a surrogate: one focused on real-time control and one inspired by waypoint delegation. We compare these designs against a baseline teleoperation system in a laboratory experiment in which novice and expert users piloted an aerial robot to inspect an environment and analyze data. Our results revealed that the augmented reality prototypes provided several objective and subjective improvements, demonstrating the promise of leveraging AR to improve human-robot interactions.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8673306?casa_token=VfcTuR51wLcAAAAA:qqaKpA8BI-Yy567LxsFfbU3rM3wmHjMarXRVO3LTzxFF_t33XnpncADHEQ11iT_e0Nc8nBlJ",
            "authors": "Walker, M. E., Hedayati, H., & Szafir, D.",
            "venue": "HRI",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 148,
            "slug": "paper_148",
            "title": "Audio-visual AR to Improve Awareness of hazard Zones Around Robots",
            "abstract": "Navigating a space populated by fenceless industrial robots while carrying out other tasks can be stressful, as the worker is unsure about when she is invading the area of influence of a robot, which is a hazard zone. Such areas are difficult to estimate and standing in one may have consequences for worker safety and for the productivity of the robot. We investigate the use of multimodal (auditory and/or visual) head-mounted AR displays to warn about entering hazard zones while performing an independent navigation task. As a first step in this research, we report a design-research study (including a user study), conducted to obtain a visual and an auditory AR display subjectively judged to approach equivalence. The goal is that these designs can serve as the basis for a future modality comparison study.",
            "link": "https://dl.acm.org/doi/abs/10.1145/3290607.3312996?casa_token=YNYzpFhX7r8AAAAA:DTZZPskBHeSzSloVQB4PFwa9TXDSqakVumvGsi7IzygVuvMqg7-8C3_WYO8As6TcEZ8CQ_SSvb3H",
            "authors": "San Mart\u00edn, A., & Kildal, J.",
            "venue": "CHI EA ",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 149,
            "slug": "paper_149",
            "title": "General hand Guidance Framework using Microsoft HoloLens",
            "abstract": "Hand guidance emerged from the safety requirements for collaborative robots, namely possessing joint-torque sensors. Since then it has proven to be a powerful tool for easy trajectory programming, allowing lay-users to reprogram robots intuitively. Going beyond, a robot can learn tasks by user demonstrations through kinesthetic teaching, enabling robots to generalise tasks and further reducing the need for reprogramming. However, hand guidance is still mostly relegated to collaborative robots. Here we propose a method that does not require any sensors on the robot or in the robot cell, by using a Microsoft HoloLens augmented reality head mounted display. We reference the robot using a registration algorithm to match the robot model to the spatial mesh. The in-built hand tracking and localisation capabilities are then used to calculate the position of the hands relative to the robot. By decomposing the hand movements into orthogonal rotations and propagating it down through the kinematic chain, we achieve a generalised hand guidance without the need to build a dynamic model of the robot itself. We tested our approach on a commonly used industrial manipulator, the KUKA KR-5.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8967649?casa_token=7lu-vqHhkPMAAAAA:U_c9YPJlgWtNd1svGik6ohmkYhTbEUvgVhAorAj5OIAOV4nnBqzA9ABIPztauWObr9IORIaH",
            "authors": "Puljiz, D., St\u00f6hr, E., Riesterer, K. S., Hein, B., & Kr\u00f6ger, T.",
            "venue": "IROS",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "3D Controllers",
                    "parent": "Control Objects"
                },
                {
                    "name": "Virtual Alterations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Superficial",
                    "parent": "Virtual Alterations"
                },
                {
                    "name": "Cosmetic Alterations",
                    "parent": "Superficial"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 150,
            "slug": "paper_150",
            "title": "An Augmented Reality System to Support Fault Visualization in Industrial Robotic Tasks",
            "abstract": "The digitalization is transforming the very nature of factories, from automated systems to intelligent ones. In this process, industrial robots play a key role. Even if repeatability, precision and velocity of the industrial manipulators enable reaching considerable production levels, factories are required to face an increasingly competitive market, which requires being able to dynamically adapt to different situations and conditions. Hence, facilities are moving toward systems that rely on the collaboration between humans and machines. Human workers should understand the behavior of the robots, placing trust in them to properly collaborate. If a fault occurs on a manipulator, its movements are suddenly stopped for security reasons, thus workers may not be able to understand what happened to the robot. Therefore, the operators' stress and anxiety may increase, compromising the human-robot collaborative scenario. This work fits in this context and it proposes an adaptive Augmented Reality system to display industrial robot faults by means of the Microsoft HoloLens device. Starting from the methodology employed to identify which virtual metaphors best evoke robot faults, an adaptive modality is presented to dynamically display the metaphors in positions close to the fault location, always visible from the user and not occluded by the manipulator. A comparison with a non adaptive modality is proposed to assess the effectiveness of the adaptive solution. Results show that the adaptive modality allows users to recognize faults faster and with fewer movements than the non adaptive one, thus overcoming the limitation of the narrow field-of-view of the HoloLens device.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8832130",
            "authors": "Avalle, G., De Pace, F., Fornaro, C., Manuri, F., & Sanna, A.",
            "venue": "Access",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Alterations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Superficial",
                    "parent": "Virtual Alterations"
                },
                {
                    "name": "Cosmetic Alterations",
                    "parent": "Superficial"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Readiness",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 151,
            "slug": "paper_151",
            "title": "Creating a Shared Reality with Robots",
            "abstract": "This paper outlines the system design, capabilities and potential applications of an Augmented Reality (AR) framework developed for Robot Operating System (ROS) powered robots. The goal of this framework is to enable high-level human-robot collaboration and interaction. It allows the users to visualize the robot's state in intuitive modalities overlaid onto the real world and interact with AR objects as a means of communication with the robot. Thereby creating a shared environment in which humans and robots can interact and collaborate.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8673191?casa_token=URnVNwJePBsAAAAA:6W6y5zCe7vUiYHzp81hZrFpYg6yLePRxUqKOKEmM-fetJFIOnHy0hBRH15B7WU72-lbNnTaR",
            "authors": "Muhammad, F., Hassan, A., Cleaver, A., & Sinapov, J.",
            "venue": "HRI",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Labels",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 152,
            "slug": "paper_152",
            "title": "Augmented Reality Assisted Instrument Insertion and Tool Manipulation for the First Assistant in Robotic Surgery",
            "abstract": "In robotic-assisted laparoscopic surgery, the first assistant (FA) stands at the bedside assisting the intervention, while the surgeon sits at the console teleoperating the robot. Tasks for the FA include navigating new instruments into the surgeon's field-of-view and passing in or retracting materials from the body using hand-held tools. We previously developed ARssist, an augmented reality application based on an optical see-through head-mounted display, to aid the FA. In this paper, we refine the system and first perform a pilot study with three experienced surgeons for two specific tasks: instrument insertion and tool manipulation. The results suggest that ARssist would be especially useful for less experienced assistants and for difficult hand-eye configurations. We then perform a multi-user study with inexperienced subjects. The results show that ARssist can reduce navigation time by 34.57%, enhance insertion path consistency by 41.74%, reduce root-mean-square path deviation by 40.04%, and reduce tool manipulation time by 72.25%. Thus, ARssist has the potential to improve efficiency, safety and hand-eye coordination, especially for novice assistants.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8794263?casa_token=La7mDyuGkK4AAAAA:iHWjCnAE__nfO459eJV3kNybTQlovqyiZlDjPalR0GEAa2KAewgz9dj3x8Wu4WAzBikJs-IN",
            "authors": "Qian, L., Deguet, A., Wang, Z., Liu, Y. H., & Kazanzides, P.",
            "venue": "ICRA",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 153,
            "slug": "paper_153",
            "title": "Using Socially Expressive Mixed Reality Arms for Enhancing Low-Expressivity Robots",
            "abstract": "Expressivity-the use of multiple modalities to convey internal state and intent of a robot-is critical for interaction. Yet, due to cost, safety, and other constraints, many robots lack high degrees of physical expressivity. This paper explores using mixed reality to enhance a robot with limited expressivity by adding virtual arms that extend the robot's expressiveness. The arms, capable of a range of non-physically-constrained gestures, were evaluated in a between-subject study (n =34) where participants engaged in a mixed reality mathematics task with a socially assistive robot. The study results indicate that the virtual arms added a higher degree of perceived emotion, helpfulness, and physical presence to the robot. Users who reported a higher perceived physical presence also found the robot to have a higher degree of social presence, ease of use, usefulness, and had a positive attitude toward using the robot with mixed reality. The results also demonstrate the users' ability to distinguish the virtual gestures' valence and intent.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8956458?casa_token=6jKTKwpt3zUAAAAA:79_VX0ZOQoC0fdrMhabxs9wGZ0N7cRVu281OCCsH_qrWJUiR2p77VVLa-NIVaFijcaNrVAI6",
            "authors": "Groechel, T., Shi, Z., Pakkar, R., & Matari\u0107, M. J.",
            "venue": "ROMAN",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Alterations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Morphological",
                    "parent": "Virtual Alterations"
                },
                {
                    "name": "Body Extensions",
                    "parent": "Morphological"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 154,
            "slug": "paper_154",
            "title": "Augmented Reality for Human-Swarm Interaction in a Swarm-Robotic Chemistry Simulation",
            "abstract": "We present a method to register individual members of a robotic swarm in an augmented reality display while showing relevant information about swarm dynamics to the user that would be otherwise hidden. Individual swarm members and clusters of the same group are identified by their color, and by blinking at a specific time interval that is distinct from the time interval at which their neighbors blink. We show that this problem is an instance of the graph coloring problem, which can be solved in a distributed manner in O(log(n)) time. We demonstrate our approach using a swarm chemistry simulation in which robots simulate individual atoms that form molecules following the rules of chemistry. Augmented reality is then used to display information about the internal state of individual swarm members as well as their topological relationship, corresponding to molecular bonds.",
            "link": "https://arxiv.org/abs/1912.00951",
            "authors": "Batra, S., Klingner, J., & Correll, N.",
            "venue": "arXiv",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Labels",
                    "parent": "Entity"
                },
                {
                    "name": "Entity Attributes",
                    "parent": "Entity"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 155,
            "slug": "paper_155",
            "title": "Combining Interactive Spatial Augmented Reality with Head-Mounted Display for End-User Collaborative Robot Programming",
            "abstract": "This paper proposes an intuitive approach for collaborative robot end-user programming using a combination of interactive spatial augmented reality (ISAR) and headmounted display (HMD). It aims to reduce user's workload and to let the user program the robot faster than in classical approaches (e.g. kinesthetic teaching). The proposed approach, where user is using a mixed-reality HMD - Microsoft HoloLens - and touch-enabled table with SAR projected interface as input devices, is compared to a baseline approach, where robot's arms and a touch-enabled table are used as input devices. Main advantages of the proposed approach are the possibility to program the collaborative workspace without the presence of the robot, its speed in comparison to the kinesthetic teaching and an ability to quickly visualize learned program instructions, in form of virtual objects, to enhance the users' orientation within those programs. The approach was evaluated on a set of 20 users using the within-subject experiment design. Evaluation consisted of two pick and place tasks, where users had to start from the scratch as well as to update the existing program. Based on the experiment results, the proposed approach is better in qualitative measures by 33.84% and by 28.46% in quantitative measures over the baseline approach for both tasks.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8956315?casa_token=1kZzHS0y5McAAAAA:ThK9ew8o1Qm3QHxIv_U5ZPO1_6_GJUSHW1NkgyKv22y5Ug8p2HdXSIpSY4as_Hd56pd7lkLw",
            "authors": "Bambu\u015dek, D., Materna, Z., Kapinus, M., Beran, V., & Smr\u017e, P.",
            "venue": "ROMAN",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "3D Controllers",
                    "parent": "Control Objects"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 156,
            "slug": "paper_156",
            "title": "Spatially Situated End-User Robot Programming in Augmented Reality",
            "abstract": "Nowadays, industrial robots are being programmed using proprietary tools developed by robot manufacturer. A skilled robot programmer is needed to create even as simple task as pick a well-known object and put it somewhere else. Contrary, in every-day life people are using end-user programming to make different electronic devices work in expected manner, without even noticing they are actually programming. We propose augmented reality-enabled end-user programming system allowing regular shop-floor workers to program industrial robotic tasks. The user interface prototype for this system was evaluated in the user study with 7 participants with respect to usability, mental workload and user experience.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8956336?casa_token=00vDcWHkDgsAAAAA:1bqYnpMc7DUexm0wgiLdNvtTkt8bS2AGHkNSsLYBfUoLaX8hvjNl3NJVTcMLxqeUqoWRWBXc",
            "authors": "Kapinus, M., Beran, V., Materna, Z., & Bambu\u0161ek, D.",
            "venue": "ROMAN",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Objects",
                    "parent": "Environmental"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Labels",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 157,
            "slug": "paper_157",
            "title": "Augmented Reality Based Actuated Monitor Manipulation from Dual Point of View",
            "abstract": "The mobile robots are increasingly used in domestic space for room surveillance. However, joystick based controller dose not realize direct and intuitive motion control for monitor. To solve this problem, we propose an augmented reality based interface to control monitor from dual point of view, referring to the third-person view and rear first-person view. In this system, augmented reality models used to represent monitor are superimposed on the dual point of view, which enables users to control each part of actuated monitor intuitively with interaction of augmented reality models on screen. Through augmented reality models, our system realized the concept of user-centered manipulation since the control of target object is described in user\u2019s coordination system. In addition, we carried out a preliminary user study to evaluate our design and the performance of the system, and a positive feedback has been received.",
            "link": "https://link.springer.com/chapter/10.1007/978-3-030-21565-1_7",
            "authors": "Ren, Y., & Tanaka, J.",
            "venue": "HCII ",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 158,
            "slug": "paper_158",
            "title": "DronePick: Object Picking and Delivery Teleoperation with the Drone Controlled by a Wearable Tactile Display",
            "abstract": "We report on the teleoperation system DronePick which provides remote object picking and delivery by a human- controlled quadcopter. The main novelty of the proposed system is that the human user continuously gets the visual and haptic feedback for accurate teleoperation. DronePick consists of a quadcopter equipped with a magnetic grabber, a tactile glove with finger motion tracking sensor, hand tracking system, and the Virtual Reality (VR) application. The human operator teleoperates the quadcopter by changing the position of the hand. The proposed vibrotactile patterns representing the location of the remote object relative to the quadcopter are delivered to the glove. It helps the operator to determine when the quadcopter is right above the object. When the \u201cpick\u201d command is sent by clasping the hand in the glove, the quadcopter decreases its altitude and the magnetic grabber attaches the target object. The whole scenario is in parallel simulated in VR. The air flow from the quadcopter and the relative positions of VR objects help the operator to determine the exact position of the delivered object to be picked. The experiments showed that the vibrotactile patterns were recognized by the users at the high recognition rates: the average 99% recognition rate and the average 2.36s recognition time. The real-life implementation of DronePick featuring object picking and delivering to the human was developed and tested.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8956344?casa_token=xWiZlIRFV74AAAAA:V5bbm4eihi45q3jfaseDX0A9I8jtHeb77tqDMkx5Ok1onrFpFdi9NFtMFMssAk-wjoGAENAp",
            "authors": "Ibrahimov, R., Tsykunov, E., Shirokun, V., Somov, A., & Tsetserukou, D.",
            "venue": "ROMAN",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 159,
            "slug": "paper_159",
            "title": "A VR System for Immersive teleoperation and Live Exploration with a Mobile Robot",
            "abstract": "Applications like disaster management and industrial inspection often require experts to enter contaminated places. To circumvent the need for physical presence, it is desirable to generate a fully immersive individual live teleoperation experience. However, standard video-based approaches suffer from a limited degree of immersion and situation awareness due to the restriction to the camera view, which impacts the navigation. In this paper, we present a novel VR-based practical system for immersive robot teleoperation and scene exploration. While being operated through the scene, a robot captures RGB-D data that is streamed to a SLAM-based live multiclient telepresence system. Here, a global 3D model of the already captured scene parts is reconstructed and streamed to the individual remote user clients where the rendering for e.g. head-mounted display devices (HMDs) is performed. We introduce a novel lightweight robot client component which transmits robot-specific data and enables a quick integration into existing robotic systems. This way, in contrast to first- person exploration systems, the operators can explore and navigate in the remote site completely independent of the current position and view of the capturing robot, complementing traditional input devices for teleoperation. We provide a proof-of-concept implementation and demonstrate the capabilities as well as the performance of our system regarding interactive object measurements and bandwidth-efficient data streaming and visualization. Furthermore, we show its benefits over purely video-based teleoperation in a user study revealing a higher degree of situation awareness and a more precise navigation in challenging environments.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8968598?casa_token=IGaUmYl7vl0AAAAA:Rs6l77gaTwHA-B9qGaiDOWc0naaVux0rt_wCpF0qBzeTeE-Ve9g_I0tjcsZwO8iIn6a9lQr_",
            "authors": "Stotko, P., Krumpen, S., Schwarz, M., Lenz, C., Behnke, S., Klein, R., & Weinmann, M.",
            "venue": "IROS",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Purviews",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 160,
            "slug": "paper_160",
            "title": "Modeling of Human Welders' Operations in Virtual Reality Human-robot Interaction",
            "abstract": "This letter presents a virtual reality (VR) human\u2013robot interaction welding system that allows human welders to manipulate a welding robot and undertake welding tasks naturally and intuitively via consumer-grade VR hardware (HTC Vive). In this system, human welders\u2019 operations are captured by motion-tracked handle controllers and used as commands to teleoperate a 6-DoF industrial robot (UR-5) and to request welding current from a controllable welding power supply (Liburdi Pulsweld P200). The three-dimensional (3-D) working scene is rendered in real time based on feedback information and shown to the human welder by head-mounted display via a motion-tracked headset. To compensate for the time delay between command motion and real motion of the robot, a hidden Markov model is proposed to model and predict human welders\u2019 operations. The K-means clustering algorithm is applied to cluster human welders\u2019 operation data (traveling speed) into latent states. Based on the developed prediction algorithm, the motion of human welders is predicted with an root mean square error (RMSE) accuracy of between 2.1 and 4.6 mm/s. The position data used as final commands to teleoperate a robot are predicted with an RMSE accuracy of between 1.1 and 2.3 mm. This letter presents a general cyber-physical model for human\u2013robot interactive welding based on VR, building a foundation for welding robot teleoperation.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8733861?casa_token=t95KeCAQqyEAAAAA:Eyi0BooD7O7z5xDvcZemCdUD1KSOe-ihLEZXWzVy5U3-xH8a7j9MI3Ctftb0-F55Dz9GAN1e",
            "authors": "Wang, Q., Jiao, W., Yu, R., Johnson, M. T., & Zhang, Y.",
            "venue": "RA-L",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 161,
            "slug": "paper_161",
            "title": "High Precision, Intuitive Teleoperation of Multiple Micro Aerial Vehicles Using Virtual Reality",
            "abstract": "In this work, we present a method for the individual, simultaneous teleoperation of multiple multirotors. Instead of using a traditional four axis remote controller to steer the small sized quadrotors used in this work, all degrees of freedom of the device are intuitively controlled by a human operator in virtual reality, allowing high precision maneuvering of the vehicles through obstacles in three dimensional space. The proposed method enables the human operator to precisely control multiple micro aerial vehicles simultaneously, without the use of formation flight or other grouping methods. The teleoperation control performance of this approach is compared to a traditional four axis control method in several experiments.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8914237?casa_token=OmfQBU1H_hcAAAAA:zaqVoH6_9FAYX_tpRN73skdWvJsniIfxqe1BYAFuyNr1JdPc7j-DSjlKbOz12lMk5HtCyusi",
            "authors": "Ladig, R., & Shimonomura, K.",
            "venue": "SMC",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 162,
            "slug": "paper_162",
            "title": "Telerobotic Control in Virtual Reality",
            "abstract": "More than 400 underwater sites in and around the U.S. are potentially contaminated with hazardous undetonated munitions due to military testing activities [1]. The main remediation methods currently employed are 1. sending divers to manually retrieve the munitions and 2. blowing the munitions in place. Manual remediation is dangerous to the divers and blow-in-place strategies are environmentally damaging and harmful to nearby marine life. Teleoperation is an alternative remediation method that does not put people's lives at risk, and, if successfully carried out, is environmentally friendly. However, traditional teleoperation methods suffer from unintuitive and ineffective operator input control due to limitations such as poor depth perception by viewing the worksite from 2D displays and poor operator input control methods by using awkward input devices. Virtual reality interfaces immerse users in simulated environments and allow them to view and interact with simulated objects naturally. In this work, intuitive and effective input control methods based on Virtual Reality were designed and implemented as a ROS (Robot Operating System) package to teleoperate a robot arm for the remediation of underwater munitions using commercial off-the-shelf VR hardware. The control methods and ROS package are generalizable for other telerobotic applications.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8962616?casa_token=RgNrfwGKQ5kAAAAA:Q44YGvKtwqzvvfSZP3_dYW8J_HurSYbgTKy-dgUJDOzNqTeWW4r0HTQjSC2fxuTCTDidAyEg",
            "authors": "Gharaybeh, Z., Chizeck, H., & Stewart, A.",
            "venue": "OCEANS",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "3D Controllers",
                    "parent": "Control Objects"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "User-Defined Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 163,
            "slug": "paper_163",
            "title": "Towards a Virtual Reality Interface for Remote Robotic Teleoperation",
            "abstract": "Intuitive interaction is the cornerstone of efficient and effective task execution in remote robotic teleoperation. It requires high-fidelity in control actions as well as perception (vision, haptic, and sensory feedback) from the remote environment. This paper presents Vicarios, a VR-based teleoperation interface with the aim of facilitating intuitive real-time remote teleoperation, while utilizing the inherent benefits of VR, including immersive visualization, freedom of viewpoint selection, and fluidity of interaction through natural action interfaces. The overall architecture of Vicarios is described, with its components and framework. A preliminary comparative user study, with a real-world tele-manipulation task, was conducted to quantify the effectiveness of Vicarios. It is shown that the VR-based interface is easy-to-use and learn, with an overall learning rate of 0.93 sec. per trial for the users. The smoothness metric shows that the users can improve on their task performance over time as well. Vicarios allows the intuitiveness and flexibility to maintain efficiency in tele-robotic tasks. It forms the basis for an immersive perception interface in remote robotic teleoperation.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8981649?casa_token=N_czLlKgcHMAAAAA:5HRgzNrRMdqqGwj9srkauSuLj-7SDEV7lhiOrDv_6985v-6pH3z6HO-b9w1wS1pqJXNnctHm",
            "authors": "Naceri, A., Mazzanti, D., Bimbo, J., Prattichizzo, D., Caldwell, D. G., Mattos, L. S., & Deshpande, N.",
            "venue": "ICAR",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 164,
            "slug": "paper_164",
            "title": "UniNet A Mixed Reality Driving Simulator",
            "abstract": "Driving simulators play an important role in vehicle research. However, existing virtual reality simulators do not give users a true sense of presence. UniNet is our driving simulator, designed to allow users to interact with and visualize simulated traffic in mixed reality. It is powered by SUMO and Unity. UniNet's modular architecture allows us to investigate interdisciplinary research topics such as vehicular ad-hoc networks, human-computer interaction, and traffic management. We accomplish this by giving users the ability to observe and interact with simulated traffic in a high fidelity driving simulator. We present a user study that subjectively measures user's sense of presence in UniNet. Our findings suggest that our novel mixed reality system does increase this sensation.",
            "link": "https://search.proquest.com/openview/ced239bbcedca45acef5c714b9def0af/1?pq-origsite=gscholar&cbl=18750&diss=y&casa_token=CeImRi8eSlcAAAAA:EXE0eVd2rTeWLHWL4gSfQB_hPpzliDDTnQ41s5eSGM1jJH-wB6sa64J12ZyDLiPHJzZqiuXf3w",
            "authors": "Arppe, D. F.",
            "venue": "Thesis",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Agents",
                    "parent": "Environmental"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Internal",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Internal Reading",
                    "parent": "Internal"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Numerical Readings",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Headings",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 165,
            "slug": "paper_165",
            "title": "Remote Robot Control with Human-in-the-Loop over Long Distances Using Digital Twins",
            "abstract": "The sharing of skills over the Internet enables professionals to democratize their expertise and skills without exhausting their availability, e.g., through excessive traveling. To enable this Internet of Skills, we present a novel Digital Twin (DT) platform for the remote control of machines with human-in-the-loop. The DT of a remotely controlled machine acts effectively as an inter-layer between the operator and the controlled machine, e.g., robot arm. The DT can be optimized for a particular application to interact with the operator with an intuitive low-latency interface and, on other side, to control and monitor the quality of the remote task. Essentially, the human operator controls the DT, while the DT controls the remote robot. This paper introduces the DT framework for the remote control. The human-machine-human control loop is split into Virtual Reality (VR), remote control, and robot control loops. The proposed framework achieves low latency visual feedback and very short system reaction times for unexpected changes with arbitrary distances between operator and robot. Within the DT framework, this paper proposes a robot control algorithm for controlling time-critical robot applications over networks with considerable delays and jitter. The proposed framework has been implemented in a demonstrator with a robot arm and its DT in VR.",
            "link": "https://ieeexplore.ieee.org/abstract/document/9013428?casa_token=gtCORWybmZUAAAAA:0h_DZQzOqfQ-NQD-kys0qKckSXqdnFTavVqyuGBUj6eRQWJqzYi1jNHqgoCYrJGgsnhBILp2",
            "authors": "Tsokalo, I. A., Kuss, D., Kharabet, I., Fitzek, F. H., & Reisslein, M.",
            "venue": "GLOBECOM",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 166,
            "slug": "paper_166",
            "title": "An application to simulate and control industrial robot in virtual reality environment integrated with IR stereo camera sensor",
            "abstract": "The main goal of this research is to test for potential ways to control KUKA KR10 industrial arm manipulator using Virtual Reality technology and check for the advantages of applying this control methods. The final version of this application aims to achieve this goal by establishing an interaction between the user and the manipulator inside a virtual environment developed using the game engine Unity3D and the HTC VIVE Pro headset for the virtual visualization. By applying this control method, the user does not have to operate on site and instead he can work remotely. In addition to the ability to use off-line programming of the manipulator. The application is designed to simplify the controlling ways by displaying a complete virtual environment where the tridimensional model of the robotic arm can be visualized and programmed according to the real manipulator\u2019s parameters and specifications. All the movements and parameters in the virtual environment can be synchronized with the real robot in an on-line or off-line path planning depending on the application or the task. The system integrates a set of virtual reality controllers and Leapmotion sensor as options to allow the user to control and see the robot and its parameters in the virtual environment. As a result of this research, the manipulator moves on the planned trajectory in a smooth way after applying some filtering techniques without losing its accuracy.",
            "link": "https://www.sciencedirect.com/science/article/pii/S2405896319323821",
            "authors": "Murhij, Y., & Serebrenny, V.",
            "venue": "IFAC",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 167,
            "slug": "paper_167",
            "title": "Intuitive Bare-Hand Teleoperation of a Robotic manipulator Using Virtual Reality and Leap Motion (MoDSeM: Towards Semantic Mapping)",
            "abstract": "Despite various existing works on intuitive human-robot interaction (HRI) for teleoperation of robotic manipulators, to the best of our knowledge, the following research question has not been investigated yet: Can we have a teleoperated robotic manipulator that simply copies a human operator\u2019s bare hand posture and gesture in a real-time manner without having any hand-held devices? This paper presents a novel teleoperation system that attempts to address this question. Firstly, we detail how to set up the system practically by using a Universal Robots UR5, a Robotiq 3-finger gripper, and a Leap Motion based on Unity and ROS, and describe specifically what information is communicated between each other. Furthermore, we provide the details of the ROS nodes developed for controlling the robotic arm and gripper, given the information of a human\u2019s bare hands sensed by the Leap Motion. Then, we demonstrate our system executing a simple pick-and-place task, and discuss possible benefits and costs of this HRI concept.",
            "link": "https://link.springer.com/chapter/10.1007/978-3-030-25332-5_25",
            "authors": "Jang, I., Carrasco, J., Weightman, A., & Lennox, B.",
            "venue": "TAROS",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 168,
            "slug": "paper_168",
            "title": "Development of an Optical Tracking Based Teleoperation system with Virtual Reality",
            "abstract": "In this paper, we propose an optical tracking based teleoperation system with a virtual reality interface for mobile robots. The system allows the user to teleoperate a mobile robot using bare hands and the user can adjust the autonomy of the robot between two levels: direct control and autonomous navigation. A Leap Motion sensor based non-contact teleoperation method is developed to translate sensor messages into velocity commands to the robot in order to interact with a remote mobile robot in a natural manner. By incorporating HTC Vive virtual reality device, the user is fully immersed into the virtual space with visual feedback from the remote site. The system features cost-effective and extendable by leveraging commercial virtual reality devices and integrating it with open source robotic control middleware.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8833835?casa_token=ClVmY4OrhGIAAAAA:bSgidsJdEcD-ZyoIHwE3TZaYIHBiTZeMqRkLUYdx_PKuGFbKzv-NFpoFzPvq8W44qpotpW-x",
            "authors": "Su, Y., Ahmadi, M., Bartneck, C., Steinicke, F., & Chen, X.",
            "venue": "ICIEA",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 169,
            "slug": "paper_169",
            "title": "Investigating Augmented Reality for Improving Child-Robot Interaction",
            "abstract": "Communication in HRI, both verbal and non-verbal, can be hard for a robot to interpret and to convey which can lead to misinterpretations by both the human and the robot. In this thesis we look at answering the question if AR can be used to improve communication of a social robot\u2019s intentions when interacting with children. We looked at behaviors such as getting children to pick up a cube, place a cube, give the cube to another child, tap the cube and shake the cube. We found that picking the cube was the most successful and reliable behavior and that most behaviors were slightly better with AR. Additionally, endorsement behavior was found to be necessary to engage the children, however, it needs to be quicker, more responsive and clearer. In conclusion, there is potential for using AR to improve the intent communication of a robot, but in many cases, the robot behavior alone was already quite clear. A larger study would need to be conducted to further explore this.",
            "link": "https://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1349972&dswid=990",
            "authors": "Hansson, E.",
            "venue": "MS Thesis",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Task Instructions",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 170,
            "slug": "paper_170",
            "title": "There's More than Meets the Eye: Enhancing Robot Control through Augmented Visual Cues",
            "abstract": "In this paper, we present the design of a visual feedback mechanism using Augmented Reality, which we call augmented visual cues, to assist pick-and-place tasks during robot control. We propose to augment the robot operator's visual space in order to avoid attention splitting and increase situational awareness (SA). In particular, we aim to improve on the SA concepts of perception, comprehension, and projection as well as the overall task performance. For that, we built upon the interaction design paradigm proposed by Walker et al.. On the one hand, our design augments the robot to support picking-tasks; and, on the other hand, we augment the environment to support placing-tasks. We evaluated our design in a first user study, and results point to specific design aspects that need improvement while showing promise for the overall approach, in particular regarding user satisfaction and certain SA concepts.",
            "link": "https://dl.acm.org/doi/abs/10.1145/3371382.3378240?casa_token=fSQwbgubKzYAAAAA:iztBf4igkOOd7xzKI3CMNNZRK7dGzzhSwQ-p63AyXPPZiNdouRQGYSXFzkq_4vbcUCy4wXJrKOjQ",
            "authors": "Ar\u00e9valo Arboleda, S., Dierks, T., R\u00fccker, F., & Gerken, J.",
            "venue": "HRI Companion",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Objects",
                    "parent": "Environmental"
                },
                {
                    "name": "Simulated Environments",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Command Options & Validity",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 171,
            "slug": "paper_171",
            "title": "Digital Twin based synchronised control and simulation of the industrial robotic cell using virtual Reality",
            "abstract": "During the years common understanding of the possibilities and perspectives of Virtual Reality (VR) usage has been changed. It is thought that VR is mainly used in entertainment purposes, but it is being used already for many years in different industries, and now with easier access to the hardware it became a helpful and accessible tool that could be used and developed in any field of human activities. In manufacturing, immersive technologies are mainly used nowadays for the visualisation of processes and products combining those visuals into the factory Digital Twin (DT) which is possible to view from the inside look. This feature is already being used in several manufacturing simulation tools, which enable to view onto industrial line / robotic cells via Virtual Reality glasses. However, the potential of using simulations with VR in manufacturing is not fully uncovered. The main aim of this, industrial robotics targeted research is to enable besides simulation also universal control algorithms through Virtual Reality experience, produced by game engine Unity3D, which can be easily modified for a wide range of industrial equipment. The primary outcome of this work is the development of the synchronisation model of real and virtual industrial robots and experimental testing the developed model in Virtual Reality and shop floor labs.",
            "link": "https://bibliotekanauki.pl/articles/100089",
            "authors": "Kuts, V., Otto, T., T\u00e4hemaa, T., & Bondarenko, Y.",
            "venue": "Journal of Machine Engineering",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Task Status",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 172,
            "slug": "paper_172",
            "title": "Remote Supervision of an Autonomous Surface Vehicle using Virtual relaity",
            "abstract": "We compared three different Graphical User Interfaces (GUI) that we have designed and implemented to enable human supervision of an Autonomous Surface Vehicle (ASV). Special attention has been paid to provide tools for a safe navigation and giving the user a good overall understanding of the surrounding world while keeping the cognitive load at a low level. Our findings indicate that a GUI in 3D, presented either on a screen or in a Virtual Reality (VR) setting provides several benefits compared to a Baseline GUI representing traditional tools.",
            "link": "https://www.sciencedirect.com/science/article/pii/S2405896319304380",
            "authors": "Lager, M., & Topp, E. A.",
            "venue": "IFAC",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Simulated Agents",
                    "parent": "Environmental"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Pose",
                    "parent": "External"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 173,
            "slug": "paper_173",
            "title": "Joint Effects of Depth-Aiding Augmentations and Viewing Positions on the Quality of Experience in Augmented Telepresence",
            "abstract": "Virtual and augmented reality is increasingly prevalent in industrial applications, such as remote control of industrial machinery, due to recent advances in head-mounted display technologies and low-latency communications via 5G. However, the influence of augmentations and camera placement-based viewing positions on operator performance in telepresence systems remains unknown. In this paper, we investigate the joint effects of depth-aiding augmentations and viewing positions on the quality of experience for operators in augmented telepresence systems. A study was conducted with 27 non-expert participants using a real-time augmented telepresence system to perform a remote-controlled navigation and positioning task, with varied depth-aiding augmentations and viewing positions. The resulting quality of experience was analyzed via Likert opinion scales, task performance measurements, and simulator sickness evaluation. Results suggest that reducing the reliance on stereoscopic depth perception via camera placement has a significant benefit to operator performance and quality of experience. Conversely, the depth-aiding augmentations can partly mitigate the negative effects of inferior viewing positions. However the viewing-position based monoscopic and stereoscopic depth cues tend to dominate over cues based on augmentations. There is also a discrepancy between the participants\u2019 subjective opinions on augmentation helpfulness, and its observed effects on positioning task performance.",
            "link": "https://link.springer.com/article/10.1007/s41233-020-0031-7",
            "authors": "Dima, E., Brunnstr\u00f6m, K., Sj\u00f6str\u00f6m, M., Andersson, M., Edlund, J., Johanson, M., & Qureshi, T.",
            "venue": "QUE",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Status Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "External",
                    "parent": "Robot Status Visualizations"
                },
                {
                    "name": "Robot Location",
                    "parent": "External"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Sensed Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 174,
            "slug": "paper_174",
            "title": "StarHopper: A Touch Interface for Remote Object-Centric Drone Navigation",
            "abstract": "Camera drones, a rapidly emerging technology, offer people the ability to remotely inspect an environment with a high degree of mobility and agility. However, manual remote piloting of a drone is prone to errors. In contrast, autopilot systems can require a significant degree of environmental knowledge and are not necessarily designed to support flexible visual inspections. Inspired by camera manipulation techniques in interactive graphics, we designed StarHopper, a novel touch screen interface for efficient object-centric camera drone navigation, in which a user directly specifies the navigation of a drone camera relative to a specified object of interest. The system relies on minimal environmental information and combines both manual and automated control mechanisms to give users the freedom to remotely explore an environment with efficiency and accuracy. A lab study shows that StarHopper offers an efficiency gain of 35.4% over manual piloting, complimented by an overall user preference towards our object-centric navigation system.",
            "link": "https://openreview.net/forum?id=BviYjfnIk",
            "authors": "Li, J., Balakrishnan, R., & Grossman, T.",
            "venue": "GI",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "3D Controllers",
                    "parent": "Control Objects"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor Images & Videos",
                    "parent": "Environment"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 175,
            "slug": "paper_175",
            "title": "A New Mixed-Reality-Based Teleoperation System for Telepresence and Maneuverability Enhancement",
            "abstract": "Virtual reality (VR) is regarded as a useful tool for teleoperation systems and provides operators with immersive visual feedback on the robot and the environment. However, without any haptic feedback or physical constructions, VR-based teleoperation systems normally suffer from poor maneuverability, and operational faults may be caused in some fine movements. In this article, we employ mixed reality (MR), which combines real and virtual worlds, to develop a novel teleoperation system. A new system design and control algorithms are proposed. For the system design, an MR interface is developed based on a virtual environment augmented with real-time data from the task space with the goal of enhancing the operator's visual perception. To allow the operator to be freely decoupled from the control loop and offload the operator's burden, a new interaction proxy is proposed to control the robot. For the control algorithms, two control modes are introduced to improve the long-distance movements and fine movements of the MR-based teleoperation system. In addition, a set of fuzzy-logic-based methods are proposed to regulate the orientation, position, velocity, and force of the robot to enhance the system's maneuverability and address potential operational faults. A barrier Lyapunov function and a backstepping method are leveraged to design the control laws and simultaneously guarantee the system's stability under state constraints. Experiments conducted using a six-degree-of-freedom robotic arm prove the feasibility of the system.",
            "link": "https://ieeexplore.ieee.org/abstract/document/8957294?casa_token=-WFgkmglEGAAAAAA:2rlmUq8OEKZRbkCifn7wI9DYdXSqIn0onZzdMI0D3GMRFOMqLlWhk9mrM5rVd0jqwR-_Ym-g",
            "authors": "Sun, D., Kiselev, A., Liao, Q., Stoyanov, T., & Loutfi, A.",
            "venue": "THMS",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Robot Digital Twins",
                    "parent": "Robots"
                },
                {
                    "name": "Control Objects",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Panels & Buttons",
                    "parent": "Control Objects"
                },
                {
                    "name": "3D Controllers",
                    "parent": "Control Objects"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Environment Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 176,
            "slug": "paper_176",
            "title": "Augmented Reality Interface for Constrained Learning from Demonstration",
            "abstract": "This paper presents a novel augmented reality (AR) interface for the visualization and directed control of robot skill Learning from Demonstration (LfD). This system is designed for use with the Concept-Constrained Learning from Demonstration (CC-LfD) algorithm for general robotic arm manipulation tasks, in which trajectories in an LfD system are subjected to various constraints during the learning process to ensure that the desired skill is learned properly. This system provides an interactive visualization of observed and learned robot trajectories, as well as the active predicate constraints applied for CC-LfD. In this paper, we describe current work on a system for helping users give improved trajectory demonstrations using AR visualizations of constraints, as well as a planned human subjects experiment to evaluate the usefulness of this system. Additionally, we discuss future extensions of this work involving using an AR interface to modify existing trajectory demonstrations.",
            "link": "https://m-luebbers.github.io/pdfs/vamhri19.pdf",
            "authors": "Luebbers, M. B., Brooks, C., Kim, M. J., Szafir, D., & Hayes, B.",
            "venue": "VAM-HRI",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Waypoints",
                    "parent": "Task"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                },
                {
                    "name": "Trajectories",
                    "parent": "Task"
                },
                {
                    "name": "Command Options & Validity",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 177,
            "slug": "paper_177",
            "title": "Knowledge Acquisition for Robots Through Mixed Reality Head-Mounted Displays",
            "abstract": "A robot can clean a room by performing a series of pick-and-place tasks on relevant items. To accomplish this sequence, the robot must know the original poses of the relevant items, how to grasp these items, and the final poses to place each item at. Language and gestures have been shown to successfully enable humans to help a robot acquire this knowledge, but are limited because they do not allow people to express poses precisely. Mixed Reality Head-Mounted Displays (MR-HMDs) have recently gained traction as a means to facilitate human-robot communication because they can visualize 3D graphics on the environment from the perspective of of the user. We hypothesize that MR-HMDs provide an intuitive interface for enabling endusers to help robots acquire grounded knowledge about how to clean up a shared workspace. We propose an MR system that allows a user to communicate to a mobile manipulator what items need to be placed away, how to grab them, where they are located and where they need to be placed. After this information is provided, the robot can autonomously clean a room.",
            "link": "https://www.researchgate.net/profile/Eric-Rosen-3/publication/331563746_Knowledge_Acquisition_for_Robots_Through_Mixed_Reality_Head-Mounted_Displays/links/5c80d97492851c69505c9409/Knowledge-Acquisition-for-Robots-Through-Mixed-Reality-Head-Mounted-Displays.pdf",
            "authors": "Kumar, N., Rosen, E., & Tellex, S.",
            "venue": "VAM-HRI",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Environmental",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Object Digital Twins",
                    "parent": "Environmental"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "User-Defined Spatial Regions",
                    "parent": "Environment"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Spatial Previews",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 178,
            "slug": "paper_178",
            "title": "Toward Allocentric Mixed-Reality Deictic Gesture",
            "abstract": "Research has shown that robots that use physical deictic gestures such as pointing enable more effective and natural interaction. However, it is not yet clear whether these benefits hold true for new forms of deictic gesture that become available in mixed-reality environments. In previous work, we presented a human-subject study suggesting that these benefits may indeed translate in the case of allocentric mixed-reality gestures, in which target referents are picked out in users\u2019 fields of view using annotations such as circles and arrows, especially when those gestures are paired with complex referring expressions. In this paper we provide additional evidence for this hypothesis through a second experiment that addresses potential confounds from our original experiment.",
            "link": "https://www.researchgate.net/profile/Tom-Williams-16/publication/339815028_Toward_Allocentric_Mixed-Reality_Deictic_Gesture/links/5e670d6c92851c7ce0575d41/Toward-Allocentric-Mixed-Reality-Deictic-Gesture.pdf",
            "authors": "Williams, T., Bussing, M., Cabrol, S., & Lau, I.",
            "venue": "VAM-HRI",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Entity",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Entity Locations",
                    "parent": "Entity"
                },
                {
                    "name": "Task",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "Callouts",
                    "parent": "Task"
                }
            ],
            "citations": [],
            "selected": true
        },
        {
            "UID": 179,
            "slug": "paper_179",
            "title": "Referencing between a Head-Mounted Device and Robotic Manipulators",
            "abstract": "Having a precise and robust transformation between the robot coordinate system and the AR-device coordinate system is paramount during human-robot interaction (HRI) based on augmented reality using Head mounted displays (HMD), both for intuitive information display and for the tracking of human motions. Most current solutions in this area rely either on the tracking of visual markers, e.g. QR codes, or on manual referencing, both of which provide unsatisfying results. Meanwhile a plethora of object detection and referencing methods exist in the wider robotic and machine vision communities. The precision of the referencing is likewise almost never measured. Here we would like to address this issue by firstly presenting an overview of currently used referencing methods between robots and HMDs. This is followed by a brief overview of object detection and referencing methods used in the field of robotics. Based on these methods we suggest three classes of referencing algorithms we intend to pursue - semi-automatic, on-shot; automatic, one-shot; and automatic continuous. We describe the general workflows of these three classes as well as describing our proposed algorithms in each of these classes. Finally we present the first experimental results of a semi-automatic referencing algorithm, tested on an industrial KUKA KR-5 manipulator.",
            "link": "https://arxiv.org/abs/1904.02480",
            "authors": "Puljiz, D., Riesterer, K. S., Hein, B., & Kr\u00f6ger, T.",
            "venue": "VAM-HRI",
            "sessions": "",
            "year": 2019,
            "keywords": "",
            "tags": [
                {
                    "name": "Virtual Design Element (VDE)",
                    "parent": null
                },
                {
                    "name": "Virtual Entities",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Robots",
                    "parent": "Virtual Entities"
                },
                {
                    "name": "Visualization Robots",
                    "parent": "Robots"
                },
                {
                    "name": "Robot Comprehension Visualizations",
                    "parent": "Virtual Design Element (VDE)"
                },
                {
                    "name": "Environment",
                    "parent": "Robot Comprehension Visualizations"
                },
                {
                    "name": "External Sensor 3D Data",
                    "parent": "Environment"
                }
            ],
            "citations": [],
            "selected": true
        }
    ],
    "links": [
        {
            "source": 176,
            "target": 53
        },
        {
            "source": 176,
            "target": 103
        },
        {
            "source": 176,
            "target": 102
        },
        {
            "source": 176,
            "target": 71
        },
        {
            "source": 176,
            "target": 101
        },
        {
            "source": 11,
            "target": 7
        },
        {
            "source": 90,
            "target": 57
        },
        {
            "source": 97,
            "target": 44
        },
        {
            "source": 135,
            "target": 62
        },
        {
            "source": 135,
            "target": 52
        },
        {
            "source": 135,
            "target": 36
        },
        {
            "source": 135,
            "target": 62
        },
        {
            "source": 54,
            "target": 44
        },
        {
            "source": 54,
            "target": 36
        },
        {
            "source": 54,
            "target": 32
        },
        {
            "source": 54,
            "target": 44
        },
        {
            "source": 54,
            "target": 36
        },
        {
            "source": 96,
            "target": 65
        },
        {
            "source": 144,
            "target": 53
        },
        {
            "source": 144,
            "target": 48
        },
        {
            "source": 168,
            "target": 108
        },
        {
            "source": 168,
            "target": 108
        },
        {
            "source": 42,
            "target": 41
        },
        {
            "source": 42,
            "target": 41
        },
        {
            "source": 139,
            "target": 19
        },
        {
            "source": 139,
            "target": 63
        },
        {
            "source": 174,
            "target": 36
        },
        {
            "source": 174,
            "target": 73
        },
        {
            "source": 100,
            "target": 52
        },
        {
            "source": 125,
            "target": 62
        },
        {
            "source": 125,
            "target": 57
        },
        {
            "source": 125,
            "target": 122
        },
        {
            "source": 125,
            "target": 70
        },
        {
            "source": 133,
            "target": 62
        },
        {
            "source": 133,
            "target": 94
        },
        {
            "source": 133,
            "target": 131
        },
        {
            "source": 49,
            "target": 43
        },
        {
            "source": 169,
            "target": 62
        },
        {
            "source": 169,
            "target": 57
        },
        {
            "source": 169,
            "target": 95
        },
        {
            "source": 169,
            "target": 62
        },
        {
            "source": 71,
            "target": 62
        },
        {
            "source": 71,
            "target": 57
        },
        {
            "source": 71,
            "target": 7
        },
        {
            "source": 129,
            "target": 65
        },
        {
            "source": 131,
            "target": 94
        },
        {
            "source": 131,
            "target": 63
        },
        {
            "source": 131,
            "target": 70
        },
        {
            "source": 131,
            "target": 53
        },
        {
            "source": 131,
            "target": 44
        },
        {
            "source": 131,
            "target": 103
        },
        {
            "source": 142,
            "target": 17
        },
        {
            "source": 142,
            "target": 53
        },
        {
            "source": 142,
            "target": 46
        },
        {
            "source": 142,
            "target": 39
        },
        {
            "source": 101,
            "target": 62
        },
        {
            "source": 101,
            "target": 57
        },
        {
            "source": 101,
            "target": 17
        },
        {
            "source": 101,
            "target": 35
        },
        {
            "source": 101,
            "target": 36
        },
        {
            "source": 101,
            "target": 7
        },
        {
            "source": 101,
            "target": 13
        },
        {
            "source": 101,
            "target": 58
        },
        {
            "source": 101,
            "target": 36
        },
        {
            "source": 101,
            "target": 58
        },
        {
            "source": 126,
            "target": 55
        },
        {
            "source": 126,
            "target": 68
        },
        {
            "source": 162,
            "target": 108
        },
        {
            "source": 162,
            "target": 108
        },
        {
            "source": 150,
            "target": 122
        },
        {
            "source": 157,
            "target": 37
        },
        {
            "source": 157,
            "target": 44
        },
        {
            "source": 157,
            "target": 29
        },
        {
            "source": 70,
            "target": 63
        },
        {
            "source": 70,
            "target": 35
        },
        {
            "source": 70,
            "target": 44
        },
        {
            "source": 70,
            "target": 7
        },
        {
            "source": 84,
            "target": 35
        },
        {
            "source": 132,
            "target": 44
        },
        {
            "source": 141,
            "target": 130
        },
        {
            "source": 141,
            "target": 125
        },
        {
            "source": 141,
            "target": 88
        },
        {
            "source": 141,
            "target": 101
        },
        {
            "source": 141,
            "target": 101
        },
        {
            "source": 141,
            "target": 130
        },
        {
            "source": 141,
            "target": 125
        },
        {
            "source": 141,
            "target": 88
        },
        {
            "source": 178,
            "target": 62
        },
        {
            "source": 178,
            "target": 83
        },
        {
            "source": 178,
            "target": 122
        },
        {
            "source": 178,
            "target": 82
        },
        {
            "source": 178,
            "target": 62
        },
        {
            "source": 120,
            "target": 69
        },
        {
            "source": 120,
            "target": 55
        },
        {
            "source": 120,
            "target": 67
        },
        {
            "source": 120,
            "target": 108
        },
        {
            "source": 120,
            "target": 55
        },
        {
            "source": 48,
            "target": 26
        },
        {
            "source": 26,
            "target": 19
        },
        {
            "source": 26,
            "target": 18
        },
        {
            "source": 26,
            "target": 7
        },
        {
            "source": 26,
            "target": 7
        },
        {
            "source": 30,
            "target": 18
        },
        {
            "source": 93,
            "target": 38
        },
        {
            "source": 93,
            "target": 52
        },
        {
            "source": 93,
            "target": 23
        },
        {
            "source": 93,
            "target": 35
        },
        {
            "source": 93,
            "target": 7
        },
        {
            "source": 93,
            "target": 34
        },
        {
            "source": 93,
            "target": 21
        },
        {
            "source": 93,
            "target": 35
        },
        {
            "source": 93,
            "target": 7
        },
        {
            "source": 93,
            "target": 21
        },
        {
            "source": 94,
            "target": 63
        },
        {
            "source": 94,
            "target": 44
        },
        {
            "source": 117,
            "target": 21
        }
    ]
}